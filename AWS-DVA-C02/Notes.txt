IAM
----------
IAM = Identity and Access Management
Root account created by default, should't be used or shared
Users and people within your organization and can be grouped
Groups only contains users, not other groups
Users don't have to belong to a group, and user can belong to multiple groups

IAM: Permissions
    Users or groups can be assigned JSON doccuments called policies
    These policies define the permission of the users
    In AWS you apply the least privilege principle: don't give more permission than a user needs

EC2
--------------
EC2 Instance Metadata (IMDS)
    AWS EC2 Instance Metadata (IMDS) is powerful but one of the least known features to developers
    It allows AWS EC2 instances to "learn about themselves" without using an IAM Role for that purpose
    The URL is http://169.254.169.264/latest/meta-data
    You can retrieve the IAM Role name from the metadata, but you CANNOT retrieve the IAM Policy.
    Metadata = Info about the EC2 instance
    Userdata = launch script of the EC2 instance 

IMDSv2 vs. IMDSv1
    IMDSv1 is accessing http://169.254.169.264/latest/meta-data directly

    IMDSv2 is more secure and is done in 2 steps:
        1. Get Session Token (limited validity) - using headers & PUT
            $TOKEN=`curl -X PUT "http://169.254.169.264/latest/api/token"` -H "X-aws-ec2-metadata-token-ttl-seconds: 21600"`
        2. Use Session Token in IMDSv2 calls - using headers
            curl http://169.254.169.264/latest/meta-data/profile -H "X-aws-ec2-metadata-token: $TOKEN"

Manage multiple AWS Accounts by using AWS Profile 
    aws configure --profile my-other-aws-account
    give access key id and secret access key and enter region 

    $ aws s3 ls --profile my-other-aws-account

AWS CLI and MFA
    To use MFA with the CLI, you must create a temporary Session
    To do so you must run the STS GetSessionToken API call

    aws sts get-session-token --serial-number <arn-of-the-mfa-device> --token-code <code-from-token> --duration-seconds 3600
    We get temporary credentials we use these creds

    $aws configure profile mfa
    Enter temporary credentials

    cat ~/.aws/credentials
    Add session token below aws_secret_access_key
    aws_session_token = SDQEWDWADFEDewfe9wefc7wecfw99we7f9we8fc9we79ew7fc9e==.....

AWS SDK Overview
    What if you want to perform actions on AWS directly from your applications code ? (without using the CLI).
    You can use an SDK (Software Development Kit)
    Official SDK's are...
        Java
        .NET
        Node.js
        PHP
        Python (named boto3/botocore)
        Go
        Ruby
        C++
    We have to use the AWS SDK when coding against AWS Sevices such as DynamoDB
    AWS CLI uses Python SDK (boto3)
    If we don't specify a region us-east-1 will be chosen by default

AWS Limits (Quotas)
    API Rate Limits
        DescribeInstances API for EC2 has a limit of 100 calls per seconds
        GetObject on S3 has a limit of 5500 GET per second per prefix
        For Intermittent Errors: implement exponential Backoff
        For Consistent Errors: request an API throttling limit increase
    Service Quotas (Service Limits)
        Running On-Demand Standard Instances: 1152 vCPU
        You can request a service limit increase by opening a ticket
        You can increase a service quota increase by using Service Quotas API
    Exponential Backoff (any AWS service)
        If you get ThrottlingException intermittently, use exponential backoff
        Retry mechanism already included in AWS SDK API calls
        Must implement yourself if using the AWS API as-is or in specific cases
            Must only implement the retries on 5xx server errors and throttling
            Do not implement on 4xx client errors

AWS CLI Credentials Provider Chain
    The CLI will look for credentials in this order

    1. Command line Options - --region, --output and --profile
    2. Environment variables -  AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY and aws_session_token
    3. CLI credentials file - -aws configure ~/.aws/credentials on Linux/macOS & C:\Users\USERNAME\.aws\credentials on Windows
    4. CLI configuration file - -aws configure ~/.aws/config on Linux/macOS & C:\Users\USERNAME\.aws\config on Windows
    5. Container credentials - for ECS tasks
    6. Instance profile credentials - for EC2 Instance Profiles

    AWS SDK Default Credentials Provider Chain
        The Java SDK will look for credentials in this order
        Java System properties - aws.accessKeyID and aws.secretKey
        Environment variables - AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY
        The default credential profiles are ~/.aws/credentials, shared by many SDK
        Amazon ECS Container credentials -  for ECS containers
        Instance profile credentials -  used on EC2 instances
    
    AWS Credentials Scenario
        An application deployed on EC2 instance is using environment variables with credentials from an IAM user to call the Amazon S3 API.
        The IAM user has S3 full access permissions 
        The application only uses one S3 bucket, so according to best practices:
            An IAM Role & EC2 Instance profile was created for the EC2 instance
            The Role was assigned the minimum permissions to access that one S3 bucket
        The IAM Instance Profile was assigned to the EC2 instance, but it still had access to all S3 buckets Why?
            the credentais chain is still giving priorities to the environment variables
    
    AWS Credentials Best practices
        Overall NEVER store AWS CREDENTIALS In your code
        Best practice is for credentials to be inherited from the credentials chain

        If using working within AWS, use IAM roles
            EC2 Instnace roles for EC2 Instances
            ECS Roles for ECS tasks
            Lambda Roles for Lambda functions 
        If working outside AWS, use environment variables / named profiles

Signing AWS API requests
    When you call the AWS HTTP API, you sign the request so that AWS can identify you, using your AWS credentials (access key and secret key)
    Some request to S3 don't need to be signed
    If you use the SDK or CLI, the HTTP requests are signed for you

    You should sign an AWS HTTP request using Signature v4 (SigV4)
 
S3
------------
Amazon S3 is one of the main building blocks of AWS 
Its advertised as "infinitely scaling" storage

Many websites use Amazon S3 as a backbone
Many AWS services use Amazon S3 as an integration as well

Use Case 
    Backup and storage
    Disaster Recovery
    Archive
    Hybrid Cloud storage
    Application Hosting
    Media Hosting
    Data lakes and big data analytics
    Software delivery
    Static websites

    examples:
        Nasdaq stores 7 years of data into S3 Glacier
        Sysco runs analytics on its data and gain business insights
    
Amazon S3 - Buckets 
    Amazon S3 allows people to store objects (files) in "buckets" (directories)
    Buckets must have a globally unique name (across all regions all accounts)
    Buckets are defined at the region level
    S3 looks like a global service but buckets are created in a region
    Naming convention
        No uppercase, No underscore
        3 - 63 characters long
        Not an IP
        Must start with lowercase letter or number
        Must NOT start with the prefix xn--
        Must NOT end with the suffix -s3alias

Amazon S3 - Objects
    Objects (files) have a Key
    The key is the FULL path:
        s3://my-bucket/my_file.txt
        s3://my-bucket/my_folder1/another_folder/my_file.txt
    The key is composed of prefix + object name 
        s3://my-bucket/my_folder1/another_folder/my_file.txt
    There is no concept of "directories" within buckets (although the UI will trick you to think otherwise)
    Just keys with very long names that contain slashes ("/")
    Object values are the content of the body:
        Max Object Size is 5 TB (5000 GB)
        If uploading more than 5GB, must use "multi-part upload"
    Metadata (list of text key / value pairs - system or user metadata)
    Tags (Unicode key / value pair - up to 10) - useful for security / lifecycle
    Version ID (if versioning is enabled)

    Create Bucket -> Select Region -> Create Bucket
    Public URL for S3 objects are not accessible directly from Internet

Amazon S3 Security
    User-Based
        IAM Policies - which API calls should be allowed for a specific user from IAM 
    Resource-Based
        Bucket Policies - bucket wide rules from the S3 console - allows cross account
        Object Access Control List (ACL) - finer grain (can be disabled)
        Bucket Access Control List (ACL) - less common (can be disabled)
    Note: an IAM principal can access an S3 bucket if 
        The user IAM permissions ALLOW it OR the resource policy ALLOWS it AND there's no explicit DENY
    Encryption: encrypt objects in Amazon S3 using encryption keys
    S3 Bucket Policies 
        JSON based policies
            Resources: buckets and objects
            Effect: Allow / Deny
            Actions: Set of API to Allow or Deny
            Principal: The account or user to apply the policy to
        Use S3 bucket for policy to:
            Grant public access to the bucket
            Force objects to be encrypted at upload
            Grant access to another account (Cross Account)

        {
            "Version": "2012-10-17"
            "Statement": [
                "Sid": "PublicRead",
                "Effect": "Allow",
                "Principal": "*",
                "Action": [
                    "s3:GetObject"
                ],
                "Resource": [
                    "arn:aws:s3:::examplebucket/*"
                ]
            ]
        }

        If we have an user on Internet who wants to access our files in S3 bucket we use the S3 Bucket Policy to give public access
        If we have an user within account (IAM user) and user wants to access S3 then we can assign IAM permissions to the user through the policy
        If its an EC2 instance and we want to access from EC2 instance to S3 bucket we need to use IAM roles 
        If we want to allow cross-account access then we use Bucket policy

        Bucket settings for Block Public Access.
            These settings were created to prevent company data leaks 
            If you know your bucket should never be public, leave these on 
            Can be set at account level

        Create Bucket Policy
            Permissions -> Allow Public access -> Create Bucket Policy using Policy Generator

        Amazon S3 - Static Website Hosting
            S3 can host static websites and have them accessible on the Internet
            The website URL will be (depending on region)
                http://bucket-name.s3-website-aws-region.amazonaws.com
                OR
                http://bucket-name.s3-website.aws-region.amazonaws.com
            If you get a 403 Forbidden error, make sure the bucket policy allows public reads!

Hosting a Website on S3
    Buckets -> properties -> Static website hosting -> Host a static website -> get the URL

Amazon S3 - Versioning
----------------------
We can version files in Amazon S3
It is enabled at the bucket level
Same key overwrite will increment the "version": 1,2,3....
It is best practice to version buckets
-  protection against uninteded deletes
-  Easy rollback to previous version
Any file that is not versioned prior to enabling versioning will have version "null"
Suspending versioning does not delete the previous versions.

S3 Replication  
--------------
We create a new bucket for replication of a bucket
For replication to happen and work we need to enable versioning on both source and target bucket.
main Bucket > Management > Replication rule



S3 Standard - general purpose storage of frequently accessed data. Fast access & object replication in multi AZ.
    Low Latency and High throughput
    Content Distribution
    Big Data Analytics
    Dynamic websites 
    Gaming Applications
S3 IA-Infrequent Access - Long-lived, but less frequently accessed data. Slow access, object replication in multi AZ
    Backups
    Disaster Recovery files
    Long term storage
S3 One Zone-IA - is for data that is accessed less frequently, but requires rapid access when needed. Slow access, no object replication.
    Used to store data not frequently accessed in cost efficient way
    Non Critical and easily reproducable data.
S3 Glacier - Low cost Storage class for data Archiving
	Low cost object storage meant for archiving/backup
	price for storage + object retrieval cost
	
S3 Glacier Instant Retrieval - 
    Millisecond retrieval, great for data accessed once a quarter
    Minimum storage duration is 90 days

S3 Glacier Flexible Retrieval - 
    Expedited(1 to 5 mins), Standard(3 to 5 hours), Bulk (5 to 12 hours) - free
    Minimum storage duration is 90 days

S3 Glacier Deep Archive - Lowest cost storage,Long time storage, retrieval time of 12Hrs to 48 Hrs
    public sectors, Financial Services, Healthcare
    which need to store data for 7-10 years for compliance requirements should use this class
    Minimum storage duration is 180 days

S3 Intelligent Tiering - Automatically moves data to most cost effective tier.
    If confused between S3 Standard and S3 Standard IA this can be used.
    If data is not used for 30 days its is moved to IA.
    Small monthly monitoring and auto-tiering fee
    Moves objects automatically between access tiers based on usage
    There are no retrieval charges in S3 Intelligent-Tiering

    Frequent Access tier (automatic): default tier 
    Infrequent Access tier (automatic): objects not accessed for 30 days
    Archive Instant Access tier (automatic): objects not accessed for 90 days
    Archive Access tier (optional): configurable from 90 days to 700+ days
    Deep Archive Access tier (optional): 180 days to 700+ days

By Default whatever we upload in S3 bucket is private.

S3 > Create Unique Bucket name > Create Bucket 

S3 Websites
  S3 Websites can host website and have them accessible on the www
  The website URL will be :
     <bucket-name>.s3-website-<AWS-region>.amazonaws.com
  If you get a 403 (Forbidden) error, make sure the bucket policy allows public reads.

We can host Static Website on S3 bucket.
------------------------------------------
1. Upload to S3 bucket all file structure.
2. Go to Permissions and uncheck Block all public access.
3. Select everything > Action > Make Public.
4. Properties > Scroll to end > Static Website Hosting > edit > enable > specify index.html and error.html > save changes

No Maintenance, just upload data and host it.

We can create lifecycle rules to move data after a certain number of days to a different storage class or delete to save cost.
We can add multiple transitions to move data to different storage class

We have replication rules where we can replicate data from one S3 bucket to other S3 bucket for disaster recovery.

S3 Object Lock and Vault lock
 - S3 Object Lock
     Adopt a WORM (Write Once Read Many) model
     Block an object version deletion for a specified amount of time

 - Glacier Vault Lock
     Adopt a WORM (Write Once Read Many) model
     Lock the policy for future edits (can no longer be changed)
  Helpful for compliance and data retention