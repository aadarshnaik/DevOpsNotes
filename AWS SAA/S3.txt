S3
-----
Amazon S3 allows people to store objects (files) in "buckets" (directories)
Buckets must have a globally unique name (across all regions all accounts)
Buckets are defined at the region level
S3 looks like a global service but buckets are created in a region
Naming convention
    No uppercase, No underscore
    3-63 characters long
    Not an IP
    Must start with lowercase letter or number
    Must NOT start with an prefix xn--
    Must NOT end with the suffix -s3alias

Amazon S3 - objects
---------------------
Objects (files) have a Key
The key is the FULL path:
    - s3://my-bucket/my_file.txt
    - s3://my-bucket/my_folder1/another_folder/my_file.txt
The key is composed of prefix + object name
    - s3://my-bucket/my_folder1/another_folder/my_file.txt
There's no concept of "directories" within buckets (although UI will trick you to think otherwise)
Just keys with very long names that contain slashes ("/")
Object values are the content of the body:
    Max. Object Size is 5TB (5000GB)
    If uploading more than 5GB, must use "multi-part upload"
Metadata (list of text key / value pairs - system or user metadata)
Tags (Unicode key/value pair - up to 10) - useful for security / lifestyle
Version ID (if versioning is enabled)

Amazon S3 - Security
-----------------------

    User-Based
        IAM Policies - which API calls should be allowed for a specific user from IAM
    
    Resource-Based
        Bucket Policies - bucket wide rules from S3 console - allows cross account
        Object Access Control List (ACL) - finer grain (can be disabled)
        Bucket Access Control List (ACL) - less common (can be disabled)
    
    Note: an IAM principle can access an S3 object if 
        The user IAM permissions ALLOW it OR the resource policy ALLOWS it
        AND there's no explicit DENY
    
    Encryption: encrypt object in Amazon S3 using encryption keys

    S3 Bucket policy
    -----------------
        JSON based policies
            Resources: buckets and objects
            Effect: Allow / Deny
            Actions: Set of API to Allow or Deny
            Principal: The account or user to apply the policy to
        
        Use S3 bucket for policy to:
            Grant public access to the bucket
            Force objects to be encrypted at upload
            Grant access to another account (Cross Account)

    {
        "Version": "2012-10-17"
        "Statement": [
            {
                "Sid": "PublicRead",
                "Effect": "Allow",
                "Principal": "*".
                "Action": [
                    "s3:GetObject"
                ],
                "Resource": [
                    "arn:aws:s3:::examplebucket/*"
                ]
            }
        ]
    }
    Hands-On:
        How to make a bucket policy ?
        Permissions > Allow Public Access > edit > Untick Block all public access >
        Bucket Policy > Edit > Policy Generator > Select type > Principal: * > Actions: GetObject >  
        ARN: arn:aws:s3:::examplebucket/* > Add Statement > Generate Policy

Amazon S3 - Versioning
----------------------
We can version files in Amazon S3
It is enabled at the bucket level
Same key overwrite will increment the "version": 1,2,3....
It is best practice to version buckets
-  protection against uninteded deletes
-  Easy rollback to previous version
Any file that is not versioned prior to enabling versioning will have version "null"
Suspending versioning does not delete the previous versions.

Amazon S3 - Replication  (CRR & SRR)
--------------------------------------
Must enable Versioning in source and destination buckets
Cross-Region Replication (CRR)
Same-Region Replication (SRR)
Buckets can be in different AWS accounts
Copying is asynchronous
Must give proper IAM permissions to S3
Use cases:
    CRR - compliance, lower latency access, replication across accounts
    SRR - log aggregation, live replication between production and test accounts

We create a new bucket for replication of a bucket
For replication to happen and work we need to enable versioning on both source and target bucket.
main Bucket > Management > Replication rule

Amazon S3 - Replication (Notes)
--------------------------------
After you enable Replication, only new objects are replicated
Optionally, you can replicate existing Objects using S3 Batch Replication
    Replicates existing objects and objects that failed replication

For DELETE operations
    Can replicate delete markers from source to target (optional setting)
    Deletions with a version ID are not replicated (to avoid malicious deletes)

There is no "chaining" of replication
    If bucket 1 has replication into bucket 2, which has replication into bucket 3
    Then objects created in bucket 1 are not replicated to bucket 3

S3 Storage Classes
--------------------
S3 Durability and Availability
    Durability


S3 Standard - general purpose storage of frequently accessed data. Fast access & object replication in multi AZ.
    Low Latency and High throughput
    Content Distribution
    Big Data Analytics
    Dynamic websites 
    Gaming Applications
S3 IA-Infrequent Access - Long-lived, but less frequently accessed data. Slow access, object replication in multi AZ
    Backups
    Disaster Recovery files
    Long term storage
S3 One Zone-IA - is for data that is accessed less frequently, but requires rapid access when needed. Slow access, no object replication.
    Used to store data not frequently accessed in cost efficient way
    Non Critical and easily reproducable data.
S3 Glacier - Low cost Storage class for data Archiving
	Low cost object storage meant for archiving/backup
	price for storage + object retrieval cost
	
    
    S3 Glacier Instant Retrieval - 
        Millisecond retrieval, great for data accessed once a quarter
        Minimum storage duration is 90 days

    S3 Glacier Flexible Retrieval - 
        Expedited(1 to 5 mins), Standard(3 to 5 hours), Bulk (5 to 12 hours) - free
        Minimum storage duration is 90 days

    S3 Glacier Deep Archive - Lowest cost storage,Long time storage, retrieval time of 12Hrs to 48 Hrs
        public sectors, Financial Services, Healthcare
        which need to store data for 7-10 years for compliance requirements should use this class
        Minimum storage duration is 180 days

    S3 Intelligent Tiering - Automatically moves data to most cost effective tier.
        If confused between S3 Standard and S3 Standard IA this can be used.
        If data is not used for 30 days its is moved to IA.
        Small monthly monitoring and auto-tiering fee
        Moves objects automatically between access tiers based on usage
        There are no retrieval charges in S3 Intelligent-Tiering

        Frequent Access tier (automatic): default tier 
        Infrequent Access tier (automatic): objects not accessed for 30 days
        Archive Instant Access tier (automatic): objects not accessed for 90 days
        Archive Access tier (optional): configurable from 90 days to 700+ days
        Deep Archive Access tier (optional): 180 days to 700+ days

Amazon S3 - Moving between Storage Classes
--------------------------------------------
We can transition objects between storage Classes

For infrequently accessed object, move them to Standard IA
For archive objects that you don't need fast access to, move toGlacier or Glacier Deep Archive
Moving objects can be automated using a Lifecycle Rules

Amazon S3 - Lifecycle Rules
    Transition Actions - configure objects to transition to another storage class
        Move objects to Standard IA class 60 days after creation
        Move to Glacier for archiving after 6 months
    
    Expiration actions - configure objects to expire (delete) after some time
        Access log files can be set to delete after a 365 days
        Can be used to delete old versions of files (if versioning is enabled)
        Can be used to delete incomplete Multi-Part uploads

    Rules can be created for a certain prefix (example:s3://mybucket/mp3/*)
    Rules can be created for certain object Tags (example: Department: Finance)

    - Your application on EC2 creates images thumbnails after profile photos are uploaded
      to Amazon S3. These thumbnails can be easily recreated, and only need to be kept for 
      60 days. The source images should be able to be immediately retrieved for these 60 days, 
      and afterwards, the user can wait up to 6 hours. How to design this Scenario?

      S3 source images can be on Standardm with a lifecycle configuration to transition them to 
      Glacier after 60 days

      S3 thumbnails can be on One-Zone IA, with a lifecycle configuration 
      to expire them (delete them) after 60 days
    
    - A rule in your company states that you should be able to recover your 
      deleted S3 objects immediately for 30 days, although this may happen 
      rarely. After this time, and for up to 365 days, deleted objects should be 
      recoverable within 48 hours 

      Enable S3 Versioning in order to have object versions, so that 
      "deleted objects" are in fact hidden by a "delete marker" and can be recovered

      Transition the "noncurrent versions" of the object to Standard IA
      Transition afterwards the "noncurrent versions" to Glacier Deep Archive

Amazon S3 Analytics - Storage Class Analysis
----------------------------------------------
    Help you decide when to transition objects to the right storage class
    Recommendations for Standard and Standard IA
        Does not work for One-Zone IA or Glacier
    Report is updated daily
    24 to 48 hours to start seeing data Analysis

Hands-On:
    Bucket > Management > Lifecycle Rule 


S3 - Requester Pays
---------------------
    In general, bucket owners pay for all Amazon S3 storage and adata transfer costs associated with
    their bucket

    With Requester Pays buckets, the requester instead of the bucket owner Pays
    the cost of the request and the data download from the bucket

    Helpful when you want to share large datasets with other accounts

    The requester must be authenticated in AWS (cannot be anonymous)

S3 Event Notifications
------------------------
    S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication...
    Object name filtering possible (*.jpg)
    Use case: generate thumbnails of images uploaded to S3
    Can create as many "S3 events" as desired
    S3 event notifications typically deliver events in seconds but can sometimes take a min or longer

    S3 Event Notification with Amazon EventBridge
        From EventBridge we can setup rules and with hep of rules we can send 
        these events to 18 AWS Services as destinations

        Advanced filtering options with JSON rules (metadata, object size, name...)
        Multiple Destinations - ex Step Functions, Kinesis Streams / Firehose...
        EventBridge Capabilities - Archive, Replay Events, Reliable delivery

Hands-On:
    Create a SQS Queue and enhance Access Policy to allow S3 bucket to write into SQS Queue

    Bucket > Properties > Event notifications > Create event Notification > Enter details > 
    select Event types > Destination > select SQL Queue > Save changes
     
S3 - Baseline Performance
---------------------------
Amazon S3 automatically scales to high request rates, latency 100-200 ms
Your application can achieve at least 3500 PUT/COPY/POST/DELETE and 5500 GET/HEAD request per second per prefix in a bucket.

There are no limits to the number of prefix in a bucket
If we spread reads across 4 prefixes evenly, we can achieve 22000 requests per second for GET and HEAD

S3 Performance
---------------
Multi-Part Upload:
    - recommended for files > 100MB
      must use for files > 5GB
    - Can help parallelize uploads (speed up transfers)

S3 Transfer Acceleration
    - Increase transfer speed by transferring file to an AWS edge location 
      which will forward the data to the S3 bucket in the target region
    - Compatible with multi-part upload

S3 Byte-Range Fetches
    Parallelize GETS by requesting specific byte ranges
    Better resilience in case of failures

    If a file is too large we can download it in parts at the same time
    We can also request first few bytes or last few bytes according to needs

S3 Select & Glacier Select 
----------------------------
    Retrieve less data using SQL by performing server-side filtering
    Can filter by rows & columns (simple SQL statements)
    Less network transfer, less CPU cost client-side

S3 Batch operations
--------------------
Perform bulk operations on existing S3 objects with a single requests
example:
    Modify object metadata & Properties
    Copy objects between S3 buckets
    Encrypt un-encrypted objects
    Modify ACLs, Tags
    Restore objects from S3 Glacier
    Invoke Lambda function to perform custom action on each object
A job consists of a list of objects, the action to perform, and optional parameters
S3 Batch Operations manages retries, tracks progress, sends completion notifications, generate reports...
You can use S3 Inventory to get object list and use S3 Select to filter your objects

Amazon S3 Security
--------------------
Amazon S3 - Object Encryption
    You can encrypt objects in S3 buckets using one of 4 methods
    Server-Side Encryption (SSE)
        Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) - Enabled by default
            Encrypts S3 objects using keys handled, managed, and owned by AWS
        Server-Side Encryption with KMS Keys stored in AWS KMS (SSE-KMS)
            Leverage AWS Key Management Service (AWS KMS) to manage encryption keys
        Server-Side Encryption with Customer-Provided Keys (SSE-C)
            When you want to manage your own encryption keys
    Client-Side Encryption
        Use client libraries such as Amazon S3 Client-Side Encryption Library
        Clients must encrypt data themselves before sending to Amazon S3
        Clients must decrypt data themselves when retrieving from Amazon S3
        Customer fully manages the keys and encryption cycle


    Amazon S3 Encryption - SSE-S3
        Encryption using keys handled, managed, and owned by AWS
        Object is encrypted server-side
        Encryption type is AES-256
        Must set header "x-amz-server-side-encryption":"AES256"
        Enabled by default for new buckets & new objects 
    Amazon S3 Encryption - SSE-KMS
        Encryption using keys handled and managed by AWS KMS (Key Management Service)
        KMS advantages: user control + audit key usage using CloudTrail
        Object is encrypted server side
        Must set header "x-amz-server-side-encryption":"aws:kms"
        Limitations
            In you use SSE-KMS, you may be impacted by the KMS limits
            When you upload, it calls the GenerateDataKey KMS API
            When you download, it calls the Decrypt KMS API
            Count towards the KMS quota per second (5500, 10000, 30000 req/s based on region)
            You can request a quota increase using the Service Quotas Console
    Amazon S3 Encryption - SSE-C
        Server-Side Encryption using keys fully managed by the customer outside of AWS
        Amazon S3 does NOT store the encryption key you provide
        HTTPS must be used
        Encryption keys must Provided in HTTP headers, for every HTTP request made

    Amazon S3  - Encryption in transit (SSL/TLS)
        Encryption in flight is also called SSL/TLS
        Amazon S3 exposes two endpoints:
            HTTP Endpoint - not encrypted
            HTTPS Endpoint - encryption in flight
        HTTPS is recommended
        HTTPS is mandatory for SSE-C
        Most clients would use the HTTPS endpoint by default
    
Amazon S3 - Default Encryption vs Bucket Policies
--------------------------------------------------
SSE-S3 encryption is automatically applied to new objects stored in S3 bucket
Optionally, you can "force encryption" using a bucket policy and refuse any API 
                            call to PUT an S3 Object without encryption headers
Eg.,

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Deny",
            "Action": "s3:PutObject",
            "Principal": "*",
            "Resource": "arn:aws:s3:::my-bucket/*",
            "Condition": {
                "StringNotEquals": {
                    "s3:x-amz-server-side-encryption": "aws:kms"
                }
            }
            
        }
    ]
}

Bucket Policies are evaluated before "Default Encryption"

S3 - CORS
---------------
Cross-Origin Resource Sharing (CORS)
Origin = scheme (protocol) + host (domain) + port
    example: https://www.example.com (implied port is 443 for HTTPS, 80 for HTTP)
Web Browser based mechanism to allow requests to ther origins while visiting the main origin
Same origin: http://example.com/app1 & http://example.com/app2
Different origins: http://www.example.com & http://other.example.com
The requests won't be fulfilled unless the other origin allows for the requests, using CORS Headers

Hands-On:
    If we have a image or html-file in different s3 bucket and we have to access it from first page, 
    we have to enable CORS for object in second s3 bucket. Other bucket should also have public access and 
    should be enabled as website. 
    Second-Bucket > Permissions > Cross-origin resource sharing 
    [
        {
            "AllowedHeaders": [
                "Authorization"
            ],
            "AllowedMethods": [
                "GET"
            ],
            "AllowedOrigins": [
                "<url of the first bucket with http://..... without the slash at the end>"
            ],
            "ExposeHeaders": [],
            "MaxAgeSeconds": 3000
        }
    ]


Amazon S3 - MFA Delete
------------------------
    - MFA (Multi-Factor Authentication) - force users to generate a code on a 
      device (usually a mobile phone or hardware) before doing important operations on S3
    - MFA will be required to:
        Permanently delete an object version
        Suspend Versioning on the bucket
    - MFA won't be required to:
        Enable Versioning
        List deleted versions
    To use MFA Delete, Versioning must be enabled on the bucket
    Only the bucket owner (root account) can enable/disable MFA Delete

    Hands-On: 
        Select Bucket > Properties > Edit Bucket Versioning
        We can enable MFA Delete using aws-cli
        Pre-req:
            Under IAM we have setup MFA for root account

S3 Access Logs
---------------
For audit purpose, you may want to log all access to S3 buckets
Any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket
That data can be analyzed using data analysis tools...
The target logging bucket must be in the same AWS region
Warning
    Do not set your logging bucket to be the monitored bucket
    It will create a logging loop, and your bucket will grow exponentially
Bucket > Properties > Server access logging > enable > choose target logs bucket 


Amazon S3 - Pre-Signed URLs
-----------------------------
Generate pre-signed URLs using the S3 Console, AWS CLI or SDK
URL Expiration
    S3 Console - 1 min up to 720 mins (12 hours)
    AWS CLI - configure expiration with -expires-in parameter in seconds (default 3600 secs, max. 604800 secs ~ 168 hours)
Users given a pre-signed URL inherit the permissions of the user that generated the URL for GET/PUT
Examples:
    Allow only logged-in users to download a premium video from S3 bucket
    Allow an ever-changing list of users to download files by generating URLs dynamically
    Allow temporarily a user to upload a file to a precise location in your S3 bucket

    Hands-On:
        Select Bucket > Select object > Object actions > Share a presigned URL > Specify time > Create presigned url


S3 Glacier Vault Lock
-----------------------
Adopt a WORM (Write Once Read Many) model
Create a Vault Lock Policy
Lock the policy for future edits (can no longer be changed or deleted)
Helpful for compliance and data retention

S3 Object Lock (versioning must be enabled)
    Adopt a WORM (Write Once Read Many) model
    Block an object version deletion for a specific ampount of time
    Retention mode - compliance:
        Object versions can't be overwritten or deleted by any user, including the root user
        Objects retention modes can't be changed, and retention periods can't be shortened
    Retention mode - Governance:
        Most users can't overwrite or delete an object version or alter its lock settings
        Some users have special permissions to change the retention or delete the object
    Retention periods  
        protect the object for a fixed period, it can be extended
    Legal Hold:
        protect the object indefinitely, independent from retention period
        can be freely placed and removed using s3:PutObjectLegalHold IAM permission


S3 - Access Point
-------------------
There are Finance, Sales and Analytics people who will access the same data
We can give access by bucket policy but if we have more user groups Bucket policy can get complicated
So we create an Access Point for Finance group connected to Finance Data
Access point for Sale group connected to sales data
Analytics point for analytics group connected to Finance and Sales data

We will attach a Policy to grant r/w access to a specific /finance or /sales prefix
For Analytics group Policy to grant read access to all the bucket


S3 Object Lambda
------------------
Use AWS Lambda Functions to change the object before it is retrieved by the caller application
Only one S3 bucket is needed, on top of which we create S3 Access Point and S3 Object Lambda Access Points.

An Analytics application may need only deleted data from object 
We create a S3 Access Point on top of S3 Bucket and its connected to
a Lambda function. Lambda Function will redact the data as it is being 
retrieved from the s3 bucket and on top of this Lambda function we will 
create a S3 Object Lambda Access point and from here our Analytics application will
access our S3 bucket.

Use Cases:
    Redacting personally identifiable information for analytics or non-production environments
    Coverting across data formats, such as converting XML to JSON.
    Resizing and watermarking images on the fly using caller-specific details, such as the user who requestsed the object.