Virtualization
--------------------
Virtualization is the primary technology that supports cloud computing
A service that results from virtualization, which is software that modifies hardware, is referred to as cloud computing. 
Virtualization is necessary for cloud computing to exist

Hypervisors

Type 1 hypervisor
    Bare metal - A computer which only has necessary hardware component and does not have any software present with them 

    example of type 1:- kvm, xen, microsoft, hyper-v, xen - hvm, pv

Type 2 --hosted
    In this type of hypervisor, there will be a host hardware and os to this only you will mount the guest os and arch that you will set up will running like an application
    example virtualbox, vmware

XEN hypervisor - HVM & PV
HVM (Hardware Virtual Machine) - No additional modifications can be done. 
    HVM guests are fully virtualized. It means that the VMs running on top of their hypervisors are not aware that they are sharing processing time with 
    other clients on the same hardware. The host should have the capability to emulate underlying hardware for each of its guest machines. 
    This virtualization type provides the ability to run an operating system directly on top of a virtual machine without any modification â€” as if it were run 
    on the bare-metal hardware. The advantage of this is that HVMs can use hardware extensions which provide very fast access to underlying hardware on the host system.
PV (Paravirtualization) - Software modification is allowed in PV
    Paravirtualization is a virtualization technique that involves modifying the guest operating system (OS) to be aware of the virtualization layer. 
    Unlike full virtualization, where the guest OS runs without modifications, paravirtualization requires some changes to the guest OS kernel to interact 
    efficiently with the virtualization layer, often referred to as the hypervisor.

    In paravirtualization, the guest OS kernel is modified to replace certain privileged instructions with hypercalls. 
    Hypercalls are special calls made by the guest OS to the hypervisor to perform tasks that would otherwise require privileged access.


Xen project is a hypervisor using microkernel design
Was launched in October 2003 by University of Cambridge Computer Laboratory
Leading open source virtualization platform which is powering the biggest players in cloud today
AWS, RackSpace, OpenStack, Verizon Cloud etc use Xen

2002 - Internally Launched
2003 - Amazon Infrastructure is one of their core strength. Idea to market
2004 - Launched publically with SQS
2006 - Relaunched publically with SQS, S3 & EC2
2007 - Launched in Europe

AWS accounts for 47% of the market in 2019 (Microsoft is 2nd with 22%)
Pioneer and Leader of the AWS Cloud Market for the 9th consecutive year
Over 1000000 active users

AWS Cloud use Cases
--------------------
AWS enables to build sophisticated, scalable applications
Applicable to a diverse set of industries
Use cases include
	Enterprise IT, Backup & Storage, Big Data analytics
	Website hosting, Mobile & Social Apps
	Gaming servers

AWS Global Infrastructure
--------------------------
AWS Regions
AWS Availability Zones
AWS Data Centers
AWS Edge Locations / Points of Presence

AWS Regions
------------
AWS has Regions all around the world
Names can be us-east-1, eu-west-3...
A region is a cluster of data centers
Most AWS services are region-scoped

How to choose an AWS Region?
- Compliance with data governance and legal requirements: data never leaves a region without your explicit permission
- Proximity to customers: reduced latency
- Available services within a Region: new services and features aren't available in every Region
- Pricing: pricing varies region to region and is transparent in the service pricing page

AWS Availability Zone
----------------------
Each region has many availability zones (usually 3, min is 2, max is 6). Example:
	ap-southeast-2a
	ap-southeast-2b
	ap-southeast-2c
Each Availability zone is one or more discrete data centers with redundant power, networking, and connectivity
They're seperate from each other, so that they're isolated from disasters.
They're connected with high bandwidth, ultra-low latency networking

AWS Points of Presence (Edge Location)
----------------------------------------
Amazon has 216 Points of Presence (205 Edge Locations & Locations & Regional Caches) in 84 cities across 42 countries
Content is delivered to end users with lower latency









-------------------------------------------------------------------------------------------------------------------------------------------
IAM & AWS CLI
-------------------------------------------------------------------------------------------------------------------------------------------
IAM = Identity and Access Management, Global service
Root account created by default, shouldn't be used or shared
Users are people within your organization, and can be grouped
Groups only contain users, not other groups
Users don't have to belong to a group, and user can belong to multiple groups

IAM: Permissions
-----------------
Users or Groups can be assigned JSON documents called policies
These policies define the permissions of the users
In AWS you apply the least privilege princlipe: don't give more permissions than a user needs

IAM Policy Inheritance
-----------------------
We can attach policy at IAM Group level so every member of group will all get access
Inline Policy is only attached to user

IAM Policy Strucure
--------------------
{
    "Version": "2012-10-17",
    "Id": ""S3-Account-Permissions",
    "Statement": [
        {
            "Sid": "1",
            "Effect": "Allow",
            "Principal": {
                "AWS": ["arn:aws:iam::123456789012:root"]
            },
            "Action": [
                "s3:GetObject",
                "s3:PutObject"
            ],
            "Resource": ["arn:aws:s3:::mybucket/*"]
        }
    ]
}
Password Policy
----------------
Account Settings -> Change Password Policy

Generate Access Keys
-----------------------
We always use IAM user to geerate access Keys
In IAM --> Access Management --> Users --> Security Credentials --> Access Keys --> Create Access Keys
To configure AWS cli 
In command line ~> aws configure
Enter AWS Access Key ID, Secret Access key, Region 

IAM Roles and Services
------------------------
Some AWS service will need to perform action on your behalf
To do so we will assign permissions to AWS services with IAM Roles

IAM Credentials Report
------------------------
A report that lists all your account user and the status of their various Credentials

Setup MFA
----------
Click Account Name > My Security Credentials > MFA > Activate MFA 

IAM Access Advisor
-------------------
Access Advisor shows the service permissions granted to a user and when those services were last accessed. You can use this information to revise your policies.

IAM policies
-------------
JSON Doccuments that define a set of permissions for making requests to AWS Services, and can be used by IAM Users, User Groups and IAM Roles


AWS CLI
---------
To access AWS, we have 3 options:
    AWS Management Console (protected by password + MFA)
    AWS Command Line Interface (CLI): protected by access keys
    AWS Software Developer Kit (SDK) - for code: protected by access keys
Access Keys are generated through the AWS Console

AWS SDK
--------
AWS Software Development Kit
Language Specific API's
enables to access aand manage AWS services programmatically
Embedded within your application
Supports
    - SDK's (JavaScript, Python, PHP,.NET, Ruby, Java, Go, Node.js, C++)
    - Mobile SDKs (Android, iOS,...)
    - IoT Device SDKs (Embedded C, Arduino)
We have to use the AWS SDK when coding against AWS Services such as DynamoDB
AWS CLI uses the Python SDK (boto3)

Once we generate the Access Key ID and Secret Access Key
we execute $aws configure       to setup
Enter access keys, secret access, region name etc..
we can now run aws commands 

AWS CloudShell is a terminal in CloudShell

IAM Roles
----------
Some AWS service will need to perform actions on your behalf
To do so, we will assign permissions to AWS services with IAM Roles
Common roles:
    EC2 Instance Roles 
    Lambda Function Roles
    Roles for CloudFormation

IAM > Roles > AWS service > Select Service (EC2) > Assign Policies/Permissions > Name, Descriptuin etc > Create Role

IAM Security Tools
--------------------
IAM Credentials Report (account-level)
    A report that lists all your account's users and the status of their various Credentials
IAM Access Advisor (user-level)
    Access advisor shows the service permissions granted to a user and when those services were last accessed
    This information can be used to revise policies


IAM Guidelines and Best Practices
-----------------------------------
Don't use the root account except for AWS Account setup
One Physical user = One AWS user
Assign users to groups and assign permissions to groups
Create a strong password policy
Use and enforce the use of MFA
Create and use Roles for giving permissions to AWS services
Use Access keys for Programmatic Access (CLI/SDK)
Audit permissions to your account with IAM Credentials Report
Never share IAM users and Access Keys

-----------------------------------------------------------------------------------------


To test the Policies we make we can test it using AWS Policy Simulator


AWS EC2 Instance Metadata
---------------------------
AWS EC2 Instance Metadata is powerful but one of the least known features to developers
It allows AWS EC2 instances to "learn about themselves" without using an IAM Role for that purpose
The URL is http://169.254.169.254/latest/meta-data
You can retrieve the IAM Role name from the metadata, but you CANNOT retrieve the IAM Policy.
Metadata = Info about the EC2 instance
Userdata = launch script of the EC2 instance

In ec2 Instance 
    curl http://169.254.169.254/latest/meta-data/iam/security-credentials/DemoRoleEC2






-------------------------------------------------------------------------------------------------------------------------------------------
EC2 
-------------------------------------------------------------------------------------------------------------------------------------------

AWS Budgets
-------------
To get access to Billing Dashboard for IAM User from Root Account
    IAM User and Role Access to Billing Information > edit > Activate IAM Access
From IAM User Account with Admin access
This will send email for any spend Incurred
    My Account > My Billing Dashboard > Budgets > Zero Spend Budgets > enter email > Create Budget 
    My Account > My Billing Dashboard > Budgets > Monthly Cost Budgets > Enter Budget Amount > enter email > Create Budget 


EC2 Basics
------------
EC2 is one of the most popular of AWS offering
EC2 = Elastic Compute Cloud = Infrastructure as a Service
It mainly consists in the capability of:
    Renting virtual Machines (EC2)
    Storing data on virtual drives (EBS)
    Distributing load across machines (ELB)
    Scaling the services using an auto-scaling group (ASG)
Knowing EC2 is fundamental to understand how the Cloud works

EC2 sizing & configuration options
------------------------------------
Operating System (OS): Linux, Windows or Mac OS
How muct compute power & cores (CPU)
How much random-access memory (RAM)
How much storage space:
    Network-attached (EBS & EFS)
    hardware (EC2 Instance Store)
Network card: speed of the card, Public IP address
Firewall rules: security group
Bootstrap script (configure at first launch): EC2 User data
EC2 User data  
    It is possible to bootstrap our instances using an EC2 User data script.
    bootstrapping means launching commaands when machine starts
    That script is only run once at the instaance first start
    EC2 user data is used to automate boot tasks such as:
        Installing updates
        Installing software
        Downloading common files from the internet
        Anything you can think of 
    The EC2 User Data Script runs with the root user

EC2 Instance types
--------------------
General purpose  -- t2.micro , m5.large etc
Compute Optimized -- Batch processing, Scientific Modelling, Computer intensive applications -- c6g.large , c5.large etc
Memory Optimized -- Analytics, In-Memory Databases -- r5.large , x1.16xlarge etc
Acclerated Computing Instance -- Deep Learning, Machine Learning, Seismic Analysis, High floating point workloads -- p3.2xlarge , p2.xlarge etc
Storage Optimized -- NoSQL Database, Data Warehousing, High disk performance workloads etc --   i3.large , d2.xlarge etc

General Purpose categorised into M typ and T type. M type is steady traffic, T type is burstable instance. 

Security Groups
----------------
Security Groups are the fundamental of network security in AWS
They control how traffic is allowed into or out of our EC2 instances
SG only contain allow rules
SG rules can reference by IP or by security group

Security Groups - Good to Know
-------------------------------
Can be attached to multiple instances
Locked down to a region/VPC combination
Does live "outside" the EC2 - if the traffic is blocked the EC2 instance won't see It
Its good to maintain one seperate security group for SSH access
If your application is not accessible (time out), then its security group issue
If your application gives a "connection refused" error, then its an applocation error or its not launched
All inbound traffic is blocked by default
All outbound traffic is authorised by default

Classic Ports to Know
-----------------------
22 = SSH (Secure Shell) - log into a Linux instance
21 = FTP (File Transfer Protocol) - upload files into a file share
22 = SFTP (Secure File Transfer Protocol) - upload files using SSH
80 = HTTP - access unsecured websites
443 = HTTPS - access secured websites
3389 = RDP (Remote Desktop Protocol) - log into a Windows instance

SSH 
-----
How to SSH into EC2 Instance Linux/Mac OS X
- SSH is one of the most important function. It allows you to control a 
  remote machine, all using command line
   
SSH Troubleshooting
--------------------
This is a security group issue. Any timeout (not just ssh) is related to security Groups
or a firewall. Ensure your security group looks like this and correctly
assigned to your EC2 instance.

EC2 Instance connection
-------------------------
Its a browser based ssh session into an instance 

EC2 Instance Roles
--------------------
It is very unsafe to do aws configure in an EC2 Instance as it could be accessed by everyone
We use IAM Roles instead.
For any instance > Actions > Security > Modify IAM Role
we can add our roles with appropiate permissions to do a task

EC2 Instances Purchasing options
---------------------------------
On-Demand Instances - short workload, predictable pricing, pay by second
Reserved (1 & 3 years)
    Reserved Instances - long workloads
    Convertible Reserved Instances - long workloads with flexible instances
Savings Plans (1 & 3 years) - commitment to an amount of usage, long workload
Spot Instances - short workloads, cheap, can lose instances
Dedicated Hosts - book an entire physical server; control instance placement
Dedicated Instances - no other customers will share your hardware
Capacity Reservations - reserve capacity in a specific AZ for any duration

EC2 On Demand
---------------
Pay for what you use:
    Linux or Windows - billing per second, after the first minute
    All other operating systems - billing per hour
Has the highest cost but no upfront payment
No long-term commitment
Recommended for sshort-term and un-interrupted workloads, where
you can't predict how the application will behave

EC2 Reserved Instances
------------------------
Upto 72% discount compared to On-Demand
You reserve a specific instance attributes (Instance Type, Region, Tenancy, OS)
Reservation Period - 1 year (+discount) or 3 years (+++discount)
Payment Options - No Upfront(+), Partial Upfront(++), All Upfront(+++)
Reserved Instance's Scope - Regional or Zonal (reserve capacity in an AZ)
Recommended for steady-state usage applications (think database)
You can buy and sell in the Reserved Instance Marketplace
Convertible Reserved instance
    Can change the EC2 instance type, instance family, OS, scope and Tenancy
    Up to 66% discount

EC2 Savings Plans
------------------
Get a discount based on long term usage (upto 72% - same as RIs)
Commit to a certain type of usage ($10/hour for 1 or 3 years)
Usage beyond EC2 Savings Plans is billed at the On-Demand price
Locked to a specific instance family & AWS region (eg., M5 in us-east-1)
Flexible across:
    Instance Size (e.g., m5.xlarge, m5.2xlarge)
    OS (e.g., Linux, Windows)
    Tenancy (Host, Dedicated, Default)

EC2 Spot Instances
--------------------
Can get a discount of upto 90% compared to On-Demand
Instances that you can "lose at any point of time if your max price is less than the current spot price"
The MOST cost-efficient instances in AWS

Useful for workloads that are resilient to failure
    Batch jobs
    Data Analysis
    Image processing
    Any distributed workloads
    Workloads with a flexible start and end time
Not suitable for critical jobs or databases

EC2 Dedicated Hosts
--------------------
A physical server with EC2 instance capacity fully dedicated to your use
Allows you to address compliance requirements and use your existing server-bound software 
licenses (per-socket, per-core, pe-VM software licenses)
Purchasing Options:
    On-demand - pay per second for active Dedicated Host.
    Reserved - 1 or 3 years (No Upfront, Partial Upfront, All Upfront)
The most expensive option
Useful for software that have complicated licensing model (BYOL - Bring Your Own License)
Or for companies that have strong regulatory or compliaance needs


EC2 Dedicated Instances
------------------------
Instances run on hardware that's dedicated to you
May share hardware with other instances in same Account
No control over instance placement (can move hardware after Stop/Start)

EC2 Capacity Reservations
--------------------------
Reserve On-Demand instances capacity in a specific AZ for any duration
You always have access to EC2 capacity when you need it
No time commitment (create/cancel anytime), no billing discounts
Combine with Regional Reserved Instances and Savings Plans to benefit from billing discounts
You're charged at On-Demand rate whether you run instaances or not
Suitable for short-term, uninterrupted workloads that needs to be in a specific AZ


Which Purchasing option right for me ? [ RESORT ANALOGY ]
------------------------------------------------------------
On demand: coming and staying in resort whenever we like, we pay the full price
Reserved: like planning ahed and if we plan to stay for a long time, we may get good discount
Savings Plans: pay a certain amount per hour for certain period and stay in any room type (eg., King, Suite, Sea View,....)
Spot instances: the hotel allows people to bid for the empty rooms and the highest bidder keeps the rooms. 
                You can get kicked out any time.
Dedicated Hosts: We book an entire building for resort
Capacity Reservations: You book a room for a period with full price even you don't stay in it


EC2 Spot Instance Requests
---------------------------
Can get a discount of upto 90% compared to On-demand

Define max spot price and get the instance while current spot price < max
    The hourly spot price veries based on offer and capacity
    If the current spot proce > your max price you can choose to stop or terminate your instance with a 2 minutes grace period
Other strategy: Spot Block
    "block" spot instance during a spefified time frame (1 to 6 hours) without interruptions
    In rare situations, the instance may be reclained
Used for Batch Jobs, data analysis, or workloads that are resilient to failures
Not great for critical jobs or databases 

There are 2 types of Spot Instance Requests
    One-time - As soon as Spot request is fullfilled, instances will be launched and then spot request will go away
    Persistant - Till the spot request is valid we want the instances to be up. If instances go down, the spot request 
                 will go into action and once its validated it'll restart instances.

To remove spot instances. we can only cancel Spot Instance requests that are open, active, or disabled
Cancelling a Spot request does not terminate instances
We must first cancel a Spot request, and then terminate the associated Spot Instances

    Spot Fleets
    ------------
    Spot Fleets = set of Spot Instances + (optional) On Demand Instances
    The spot fleet will try to meet the target capacity with price constraints
        Define possible launch pools: instance type (m5.large), OS, Availability Zone
        Can have multiple launch pools, so that the fleet can choose
        Spot fleet stops launching instances when reaching capacity or max cost
    Strategies to allocate Spot Instances:
        lowestPrice: from the pool with lowest price (cost optimization, short workload)
        diversified: distributed across all pools (great for availability, long workloads)
        capacityOptimized: pool with the optimal capacity for the number of instances
    Spot Fleets allow us to automatically request Spot Instances with the lowest price
    

Private vs Public IP (IPv4)
-----------------------------

Networking has two sorts of IPs. IPv4 and IPv6
    IPv4: 1.160.10.240
    IPv6: 3ffe:1900:4545:3:200:f8ff:fe21:67cf

IPv4 is still the most common format used online
IPv6 is newer and solves problems for the Internet of Things (IoT).
IPv4 allows for 3.7 billion different addresses in the public space
IPv4: [0-255].[0-255].[0-255].[0-255].

    Fundamental Differences
    ------------------------
    Public IP:
        Public IP means the machine can be identified on the internet (WWW)
        Must be unique across the whole web (not two machines can have the same public IP).
        Can be geolocated easily

    Private IP:
        Private IP means the machine can only be identified on a private network only
        The IP must be unique across the private network
        BUT two different private networks (two companies) can have the same IPs.
        Machines connect to WWW using a NAT + internet gateway (a proxy)
        Only a specified range of IPs can be used as private IP

    Elastic IPs
        When we stop and start an EC2 instance, it can change its public IP.
        If you need to have a fixed public IP for your instance , you need an Elastic IP
        An Elastic IP is a public IPv4 IP you own as long as you don't delete it
        You can attach it to one instance at a time

        With an Elastic IP address, you can mask the failure of an instance or 
        software by rapidly remapping the address to another instance in your account
        
        We can only have 5 Elastic IP in your account (you can ask AWS to increase that).

        Overall we must try to avoid using Elastic IP:
            They often reflect poor architectural decisions
            Instead, use a random public IP and register a DNS name to it
            We can use Load Balancer and don't use a public IP

    In AWS EC2
        By default, EC2 machine comes with:
            A private IP for the internal AWS Network
            A public IP, for the WWW.
        When we are doing SSH into our EC2 machines:
            We can't use a private IP, because we are not in the same network
            We can only use the public IP.
        If EC2 machine is stopped and then started, the public IP can change



EC2 Placement Groups
---------------------
Sometimes we want control over the EC2 Instance placement strategy
That Strategy can be defined using placement Groups
When we create a placement group, we specify one of the following strategies 
for the group:
    Cluster - clusters instances into a low-latency group in a single Availability Zone
    Spread - spreads instances across underlying hardware (max 7 instances per group per AZ) - critical applications
    Partition - spreads instances across many different partitions (which rely on different sets of racks) within
                an AZ. Scales to 100s of EC2 instances per group (Hadoop, Cassandra, Kafka)


    Cluster: 
        Pros: Great network (10Gbps bandwidth between instances)
        Cons: If the rack fails, all instances fails at the same time
        Use Case:
            Big Data job that needs to complete fast
            Application that needs extremely low latency and high network throughput

    Spread: All EC2 will be located on different hardware  
        Pros: Can span across Availability Zones (AZ)
              Reduced risk is simultaneous failure
              EC2 Instances are on different physical hardware
        Cons: Limited to 7 instances per AZ per placement group
        Use case:
            Application that needs to maximize high availability
            Critical Applications where each instance must be isolated from each other

    Partition:
        Upto 7 partitions per AZ
        Can span across multiple AZs in the same region
        Up to 100s of EC2 instances
        The instances in a partition do not share racks with the instances in the other partitions
        A partition failure can affect many EC2 but won't affect other partitions
        EC2 instances get access to the partition information as metadata
        Use cases:
            HDFS, HBase, Cassandra, Kafka


    Hands-On:
        Under Network and Security > Placement Groups > Create Placement Group > 
        Select placement strategy > Create Group
        While launching EC2 Instance > Advanced Details > Placement Group name > Select appropiate group




Elastic Network Interface (ENI)
--------------------------------
Logical component in a VPC that represents a virtual network card
The ENI can have the following attributes:
    Primary private IPv4, one or more secondary IPv4
    One Elastic IP (IPv4) per private IPv4
    One IP Public IPv4
    One or more security Groups
    A MAC address
You can create ENI independently and attach them on the fly (move them) on EC2 instances for failover
Bound to a specific availability zone (AZ)
    
    Hands-On:
        - We create 2 EC2 Instances which have 2 ENI's. We can see the 
          ENI's in Network interfaces under Instances
        - We create a new Network Interface
          Give Description
          Choose Subnet same as EC2 Instances
          We can choose a IPv4 or auto-assign 
          Attach a SG
          Create a new ENI
          Select new ENI > Action > Attach > Choose Instance > Attach
          
    Now the new ENI is attached to EC2 Instance. This new ENI we have 
    control over, so we could move it from one EC2 Instance to another.
    We can do a quick and easy network failover between the two instances by
    moving an ENI.

    To connect it to another Instance in case of network failover 
        Select ENI > Actions > Detach > Detach/Force Detach
        Then we can choose other instance

By creating our own ENI's we have more control over IPv4 and more control over
Networking and that could be helpful in some type of advance use cases



EC2 Hibernate
---------------
We know we can stop, terminate instances
    Stop - the data on disk (EBS) is kept intact in the next start
    Terminate - any EBS volumes (root) also set-up to be destroyed is lost

On start, the following happens:
    First start: the OS boots & the EC2 User Data script is run
    Following starts: the OS boots up
    Then your application starts, caches get warmed up, and that can take time!

Introducing EC2 Hibernate:
    - The in-memory (RAM) state is preserved
    - The instance boot is much faster! (the OS is not stopped/restarted)
    - Under the hood: the RAM state is written to a file in the root EBS volume
    - The root EBS volume must be encrypted

Use Cases: 
    Long-running processing
    Saving the RAM state
    Services that take time to initialize

Supported Instance Families - C3, C4, C4, I3, M3, M4, R3, R4, T2, T3, ...
Instance RAM Size - must be less than 150 GB.
Instance Size - not supported for bare metal instances.
AMI - Amazon Linux 2, Linux AMI, Ubuntu, RHEL, CentOS & Windows...
Root Volume -  must be EBS, encrypted, not instance store, and large
Available for On-Demand, Reserved and Spot Instances
An instance can NOT be hibernated more than 60 days




-------------------------------------------------------------------------------------------------------------------------------------------
STORAGE
-------------------------------------------------------------------------------------------------------------------------------------------

EBS Volumes
-------------
What's an EBS Volume?

An EBS (Elastic Block Store) Volume is a network drive you can attach to your
instances while they run.
It allows your instances to persist data, even after their termination
They are bound to a specific availability zone
Free tier: 30 GB of free EBS storage of type General Purpose (SSD) or Magnetic per month

Its a network drive (i.e. not a physical drive)
    It uses the network to communicate the instance, which means there might be a bit of latency.
    It can be detached from an EC2 instance and attached to another one quickly

Its locked to an Availability Zone (AZ)
    An EBS Volume in us-east-1 a cannot be attached to us-east-1b
    To move a volume across, you first need to snapshot It

Have a provisioned capacity (size in GBs, and IOPS)
    You get billed for all the provisioned capacity
    You can increase the capacity of the drive over time

EBS - Delete on Termination attribute

Controls the EBS behaviour when an EC2 instance terminates
    By default, the root EBS volume is deleted (attribute enabled)
    By default, any other attached EBS volume is not deleted (attribute disabled)
This can be controlled by the AWS console/AWS CLI
Use case: preserve root volume when instance is terminated

EBS Volumes Types
------------------
EBS Volumes come in 6 Types
    gp2 / gp3 (SSD): General purpose SSD volume that balances price and performance for a wide variety of workloads
    io1 / io2 (SSD): Highest-performance SSD volume for mission-critical low-latency or high-throughput workloads
    st1 (HDD): Low cost HDD volume designed for frequently accessed, throughput-intensive workloads
    sc 1 (HDD): Lowest cost HDD volume designed for less frequently accessed workloads

EBS Volumes are characterized in Size | Throughput | IOPS (I/O Ops Per Sec)
Only gp2/gp3 and io1/io2 can be used as boot volumes

    EBS Volumes Types Use cases 
    ----------------------------
        General Purpose SSD
            Cost effective storage, low latency
            System boot volumes, Virtual desktops, Development and test environments
            1 GiB - 16 TiB
            gp3:
                Baseline of 3000 IOPS and throughput of 125 MiB/s
                Can increase IOPS up to 16000 and throughput up to 1000 MiB/s independently
            gp2:
                Small gp2 volumes can burst IOPS to 3000
                Size of the volume and IOPS are linked, max IOPS is 16000
                3 IOPS per GB, means at 5334 GB we are at the max IOPS
        
        Provisioned IOPS (PIOPS) SSD
            Critical Business applications with sustained IOPS performance
            Or applications that need more than 16000 IOPS
            Great for databases workloads (sensitive to storage perf and consistency)
            io1/io2 (4 GiB - 16 TiB):
                Max PIOPS: 64000 for Nitro EC2 instances and 32000 for other
                Can increase PIOPS independently from storage size
                io2 have more durability and more IOPS per GiB (at the same price as io1)
            io2 Block Express (4 GiB - 64 TiB):
                Sub-millisecond latency
                Max PIOPS: 256000 with an IOPS:GiB ratio of 1000:1
            Supports EBS Multi-attach

        Hard Disk Drives (HDD)
            Cannot be a boot volume
            125 MiB to 16 TiB
            Throughput Optimized HDD (st1)
                Big Data, Data Warehouses, Long Processing
                Max throughput 500 MiB/s - max IOPS 500
            Cold HDD (sc1):
                For data is infrequently accessed
                Scenarios where lowest cost is important
                Max throughput 250 MiB/s - max IOPS 250
    
    EBS Multi-Attach - io1/io2 family
    -----------------------------------
    Attach the same EBS volume to multiple EC2 instances in th same AZ
    Each instance has full read and write permissions to the high-performance volume
    Use case:
        Achieve higher application availability in clustered Linux applications (ex: Teradata)
        Applications must manage concurrent write operations 
    Upto 16 EC2 Instances at a time
    Must use a file system thats's cluster-aware (not XFS, EX4, etc...)


EBS Encryption
---------------
When you create an encrypted EBS volume, you get the foloowing:
    Data at rest is encrypted inside the volume
    All the data in flight moving between the instances and the volume is encrypted
    All snapshots are encrypted
    All volumes created from the snapshot
Encryption and decryption are handled transparently (you have nothing to do)
Encryption has a minimal impact on latency
EBS Encryption leverages keys from KMS (AES-256)
Copying an unencrypted snapshot allows encryption
Snapshots of encrypted volumes are encrypted

How to encrypt an unencrypted EBS volume
    Create an EBS snapshot of the volume
    Encrypt the EBS snapshot ( using copy )
    Create new ebs volume from the snapshot ( the volume will also be encrypted )
    Now you can attach the encrypted volume to the original instance

Creata a snapshot > Go to shanpshot page > actions > copy snapshot > enable encryption > copy snapshot > 
select copied snapshot > actions > create volume from snapshot > Create Volume

EBS Snapshots
--------------
Make a backup (snapshot) of your EBS volume at a point in time
Not necessary to detach volume to do snapshot, but recommended
Can copy snapshots across AZ or Region

    EBS Snapshot Features

        EBS Snapshot Archive
            Move a Snapshot to an "archive tier" that is 75% cheaper
            Takes within 24 to 72 hours for restoring the archive
        
        Recycle Bin for EBS Snapshots
            Setup rules to retain deleted snapshots so you can recover them after an accidental deletion
            Specify retention (from 1 day to 1 year)

        Fast Snapshot Restore (FSR)
            Force full initialization of snapshot to have no latency on the first use
 
Hands-On:
    Select Snapshot > Actions > Create snapshot 
    Elastic Block Store(Menu) > Snapshots > select > rightclickCopy > select destination region > Copy snapshot

    Select Snapshot > Actions > Create volume from snapshot > select AZ > Create Volume

    Using Recycle Bin we can recover snapshots



AMI
------
AMI = Amazon Machine Image
AMI are a customization of the EC2 instance
    You add your own software, configuration, operating system, monitoring ...
    Faster boot/configuration time because all your software is pre-packed
AMI are built for a specific region (and can be copied across regions)
You can launch EC2 instances from:
    A Public AMI: AWS provided
    Your own AMI: you make and maintain them yourself
    An AWS Marketplace AMI: an AMI someone else made (and potentially sells)

    AMI Process (from an EC2 instance)
        Start an EC2 instance and customize it
        Stop the instance (for data integrity)
        Build an AMI - this will also create EBS snapshots
        Launch instances from other AMIs

    Hands-On:
        Create a EC2 Instnace and Customize it > Actions/Rightclick 
        > Image and template > Create image > Enter details > Create Image
Actual content of the AMI / Storage space of the AMI is saved as a snapshot in S3
Even if we delete the image from AMI section, we can go to snapshot and create another image 
    From AMI Actions > Copy AMI we can copy an AMI to different region. 
    It will take space in S3 in another region as well for AMI and data transfer charges from one region to another


EC2 Instance Store
--------------------
EBS volumes are network drives with good but 'limited' performance
If you need a high-performance hardware disk, use EC2 Instance Store
Better I/O performance
EC2 Instnace Store lose their storage if they're stopped 
Good for buffer / cache / scratch data / temporary content
Risk of data loss if hardware fails
Backups and Replication are your responsibility


Amazon - EFS (Elastic File System)
----------------------------------- 
Managed NFS (network file system) that can be mounted on many EC2
EFS works with EC2 instances in multi-AZ
Highly available, scalable, expensive (3x gp2), pay per use

Use-Cases:
    content management, web serving, data sharing, Wordpress
    Uses NFSv4.1 protocol
    Uses security group to control access to EFS
    Compatible with Linux based AMI (not Windows)
    Encryption at rest using KMS
    POSIX file system (~Linux) that has a standard file API
    File system scales automatically, pay-per-use, no capacity planning

EFS Scale:
    1000s of concurrent NFS clients, 10GB + /s throughput
    Grow to Petabyte-scale network file system, automatically

Performance mode (set at EFS creation time)
    General purpose (default): latency-sensitive use case (web server, CMS, etc...)
    Max I/O - higher latency, throughput, highly parallel (big data, media Processing)

Throughput mode
    Bursting (1TB=50MiB/s + bursty of upto 100MiB/s)
    Provisioned: set your throughput regardless of storage size, ex:1GiB/s or 1 TB storage

EFS - Storage Class
    Storage Tiers (lifecycle management feature - move file after N days)
        Standard: for frequently accessed files
        Infrequent access (EFS-IA): cost to retrieve files, lower price to Store, Enable EFS-IA with a Lifecycle Policy
    Availability and durability
        Standard: Multi-AZ, great for prod 
        One Zone: One AZ, great for dev, backup enabled by default, compatible with IA (EFS One Zone-IA)
    Over 90% in cost savings

EBS vs EFS 
------------
EBS Volumes 
        can be attached to only one instance at a time 
        are locked at the Availability Zone level
        gp2LIO increases if the disk size increases
        io1: can increase IO independently
    To migrate an EBS volume across AZ
        Take a snapshot
        Restore the snapshot to another AZ
        EBS backups use IO and you shouldn't run them while your application is handling a lot of traffic
    Root EBS Volumes of instances get terminated by default if the EC2 instance get terminated

EFS Volumes 
    Mounting 100s of instances across AZ
    EFS share website files (WordPress)
    Only for Linux Instances (POSIX)
    EFS has a higher price point than EBS
    Can leverage EFS-IA for cost savings


AWS Snow Family
-----------------
Highly-secure, portable devices to collect and process data at the edge, and migrate data into and out of AWS

Data Migration - Snowcone, Snowball Edge, Snowmobile

Edge Computing - Snowcone, Snowball Edge

Data Migration with AWS Snow Family
	Time it takes to transfer data, it may take a lot of time so we use AWS snow family
	
	Challenges with normal Data transfer
		Limited connectivity
		Limited Bandwidth
		High network cost
		Shared bandwidth (can't maximize the line)
		Connection stability
	AWS Snow family: Offline devices to perform data migrations
	If it takes more then a week to transfer over the network, use Snowball devices!

Snowcone and Snowcone SSD
	Small, portable computing, anywhere, rugged and secure, withstands harsh environments
	Light (4.5 pounds, 2.1 kg)
	Device used for edge computing, storage and data transfer
	Snowcone - 8TB of HDD Storage
	Snowcone SSD - 14TB of SSD Storage
	Use Snowcone where Snowball does not fit (space-constrained enviironment)
	Must provide your own battery / cables
	Can be sent back to AWS offline, or connect it to internet and use AWS DataSync to send data


Snowball Edge (for data transfers)
	Physical data transport solution: move TBs or PBs of data in or out of AWS
	Alternative to moving data over the network (and paying network fees)
	Pay per data transfer job
	Provide block storage and Amazon-s3-compatible object storage
	Snowball Edge Storage Optimized
		80TB of HDD Capacity for block volume and S3 compatible object storage
	Snowball Edge Compute Optimized
		42TB of HDD Capacity for block volume and S3 compatible object storage
	Use cases: large data cloud migrations, DC decommission, disaster recovery


AWS Snowmobile
	Transfers exabytes of data (1EB = 1000PB = 1000000TBs)
	Each Snowmobile has 100PB of capacity 
	High Security: temperature controlled, GPS, 24/7 video surveillance
	Better than Snowball if you transfer more than 10PB


Snow Family - Usage Process
	1. Request Snowball devices from the AWS console for delivery
	2. Install the snowball client / AWS OpsHub on your servers
	3. Connect the snowball to your servers and copy files using the client
	4. Ship back the device when you're done (goes to the right AWS facility)
	5. Data will be loaded into an S3 bucket
	6. Snowball is completely wiped

Edge Computing using Snow Family
	Process data while its being created on an edge location
		A truck on the road, a ship on the sea, a mining station underground
	These Locations may have
		Limited/no internet access
		Limited/no easy access to computing power
	We setup a Snowball Edge / Snowcone device to do edge computing
	Use cases of Edge Computing:
		Preprocess data
		Machine learning at the edge
		Transcoding media streams
	Eventually (if need be) we can ship back the device to AWS (for transferring data for example)
Snow Family - Edge Computing
	Snowcone and Snowcone SSD (smaller)
		2 CPUs, 4 GB of memory, wired or wireless access
		USB-C power using a cord or the optional battery
	Snowball Edge - Compute Optimized
		52 vCPUs, 208 GiB of RAM
		Optional GPU (useful for video processing or machine learning)
		42 TB usable storage
	Snowball Edge - Storage Optimized
		Upto 40 vCPUs, 80 GiB of RAM
		Object storage clustering available
	All: Can run EC2 Instances & AWS Lambda functions (using AWS IoT Greengrass)
	Long-term deployment options: 1 and 3 years discounted pricing

AWS OpsHub
	Historically, to use Snow Family devices, you needed a CLI (Command Line Interface tool)
	Today you can use AWS OpsHub (a software you install on your computer/laptop to 
	manage your Snow Family Device)
		Unlocking and configuring single or clustered devices
		Transferring files
		Launching and managing instances running on Snow Family Devices
		Monitor device metrics (storage capacity, active instances on your device)
		Launch compatible AWS services on your devices (ex: Amazon EC2 instances, AWS DataSync, Network File System (NFS)




Amazon FSx - Overview
----------------------
Launch 3rd party high-performance file systems on AWS
Fully managed service
	- FSx for Lusture
	- FSx for Windows File Server
	- FSx for NetApp ONTAP
	- FSx for OpenZFS

Amazon FSx for Windows (File Server)
	FSx for Windows is a fully managed Windows file system share drive
	Supports SMB protocol & Windows NTFS
	Microsoft Active Directory integration, ACLs, user quotas
	Can be mounted on Linux EC2 instances
	Supports Microsoft's Distributed File System (DFS) Namespaces (group files across multiple FS)

	Scale up to 10s of GB/s, millions of IOPS, 100s PB of data
	Storage Options:
		SSD - latency sensitive workloads (databases, media processing, data analytics)
		HDD - broad spectrum of workloads (home directory, CMS, ...)
	Can be accessed from your on-premises infrastructure (VPN or Direct Connect)
	Can be configured to be Multi-AZ (high availability)
	Data is backed-up daily to S3

Amazon FSx for Lusture 
	Lusture is a type of parallel distributed file system, for large-scale computing
	The name Lusture is derived from "Linux" and "cluster"
	Machine Learning, High Performance Computing (HPC)
	Video Processing, Financial Modeling, Electronic Design Automation
	Scales upto 100s of GB/, millions of IOPS, sub-ms latency
	Storage Options:
		SSD - low latency, IOPS intensive workloads, small and random file operations
		HDD - throughput-intensive workloads, large & sequential file operations
	Seamless integration with S3
		Can "read S3" as a file system (through FSx)
		Can write the output of the computations back to S3 (through FSx)
	Can be used from on-premises servers (VPN or Direct Connect)

	FSx File system Deployment Options
		Scratch File System
			Temporary Storage
			Data is not replicated (doesn't persist if the file server fails)
			High burst (6x faster, 200MBps per TB)
			Usage: short-term processing, optimize costs
		Persistent File System
			Long-term storage
			Data is replicated with same AZ
			Replace failed files within minutes
			Usage: Long-term processing, sensitive data
Amazon FSx for NetApp ONTAP
	Managed NetApp ONTAP on AWS
	File System compatible with NFS, SMB, iSCSI protocol
	Move workloads running on ONTAP or NAS to AWS
	Works with:
		Linux
		Windows
		MacOS
		VMware Cloud on AWS 
		Amazon Workspaces & AppStream 2.0
		Amazon EC2, ECS and EKS 
	Storage shrinks or grows automatically
	Snapshots, replication, low-cost, compression and data de-duplication
	Point-in-time instantaneous cloning (helpful for testing new workloads)

Amazon FSx for OpenZFS
	Managed OpenZFS file system on AWS
	File System compatible with NFS (v3, v4, v4.1, v4.2)
	Move workloads running on ZFS to AWS
	Works with:
		Linux
		Windows
		MacOS
		VMware Cloud on AWS
		Amazon Workspaces & AppStream 2.0
		Amazon EC2, ECS and EKS
	Up to 1000000 IOPS with <0.5ms latency
	Snapshots, compression and low-cost
	Point-in-time instantaneous cloning (helpful for testing new workloads)


Storage Gateway
-------------------
Hybrid Cloud for Storage
	AWS is pushing for "hybrid cloud"
		Part of your infrastructure is on the cloud
		Part of your infrastructure is on-premises
	This can be due to
		Long cloud migrations
		Security requirements
		Compliance requirements
		IT strategy
	S3 is a proprietary storage technology (unlike EFS / NFS), so how do you expose the S3 data on-premises
	The Bridge between S3 and on-premises is going to be AWS Storage Gateway
	
	AWS Storage Gateway
		Bridge between on-premises data and cloud data
		Use-cases:
			data recovery
			backup & restore
			tiered storage
			on-premises cache & low-latency files access
		Types of Storage Gateway:
			S3 File Gateway
			FSx File Gateway
			Volume Gateway
			Tape Gateway

S3 File Gateway
	We want to connect S3 bucket to an on premises application server but we want to use a standard network file system. 
	So we create an S3 file gateway which is going to allow our application server to use the NFS or SMB protocol and by using this protocol
	behind the scenes the S3 file gateway is going to translate those request into https request for AWS S3 bucket 

	Configured S3 buckets are accessible using the NFS and SMB protocol
	Most recently used data is cached in the file gateway
	Supports S3 Standard, S3 Standard IA, S3 One Zone A, S3 Intelligent Tiering
	Transition to S3 Glacier using a Lifecycle Policy
	Bucket access using IAM roles for each File Gateway
	SMB Protocol has integration with Active Directory (AD) for user authentication 

FSx File Gateway
	Native access to Amazon FSx for Windows File Server
	Local cache for frequently accessed data
	Windows native compatibaility (SMB, NTFS, Active Directory...)
	Useful for group file shares and home directories 

Volume Gateway
	Block storage using iSCSI protocal backed by S3
	Backed by EBS snapshots which can help restore on-premises volumes!
	Cached volumes: low latency access to most recent data
	Stored volumes: entire dataset is on premise, scheduled backups to S3
	
Tape Gateway
	Some companies have backup processes using physical tapes
	With Tape Gateway, companies us the same processes but, in the cloud
	Virtual Tape Library (VTL) backed by Amazon S3 and Glacier
	Backup data using existing tape-based processes (and iSCSI interface)
	Works with lending backup software vendors

Storage Gateway - Hardware appliance
	Using Storage Gateway means you need on-premises virtualization
	Othewise you can use a Storage Gateway Hardware Appliance
	You can buy it on amazon.com
	Works with File Gateway, Volume Gateway, Tape Gateway
	Has the required CPU, memory, network, SSD cache resources
	Helpful for daily NFS backups in samll data centers

AWS Storage Gateway
	Bridge between on-premises data and cloud data
	Use cases:
	    - disaster recovery
	    - backup & restore
	    - tiered storage
	    - on-premises cache & low-latency file access


AWS Transfer Family - Overview
------------------------------------
A fully managed service for file transfers into and out of Amazon S3 or Amazon EFS using the FTP protocol
Supported Protocols
	AWS Transfer for FTP (File Transfer Protocol(FTP))
	AWS Transfer for FTPS (File Transfer Protocol over SSL (FTPS))
	AWS Transfer for SFTP (Secure File Transfer Protocol (SFTP))
Managed infrastructure, Scalable, Reliable, Highly Available (multi-AZ)
Pay per provisioned endpoint per hour + data transfers in GB
Store and manage users credentials within the service
Integrate with existing authentication systems (Microsoft Active Directory, LDAP, Okta, Amazon Cognito, custom)
Usage: sharing files, public datasets, CRM, ERP, ..	

AWS DataSync - Overview
----------------------------
AWS DataSync - NFS/SMB to AWS (S3, EFS, FSx...)
Move large amount of data to and from
	On-premises/other cloud to AWS (NFS. SMB, HDFS, S3 API...) - needs agent
	AWS to AWS (different storage services) - no agent needed
Can synchronize to:
	Amazon S3 (any storage classes - including Glacier)
	Amazon EFS
	Amazon FSx (Windows, Lusture, NetApp, OpenZFS...)
Replication tasks can be scheduled hourly, daily, weekly
File permissions and metadata are preserved (NFS POSIX, SMB...)
One agent task can use 10 Gbps, can setup a bandwidth limit

Storage Comparision
---------------------
S3: Object Storage
S3 Glacier: Object Archival
EBS volumes: Network storage for one EC2 instance at a time
Instance Storage: Physical storage for your EC2 instance (high IOPS)
EFS: Network File System for Linux instances, POSIX filesystem
FSx for Windows: Network File System for Windows servers
FSx for Lusture: High Performance Computing Linux file system
FSx for NetApp ONTAP: High OS Compatibility
FSx for OpenZFS: Managed ZFS file system
Storage Gateway: S3 and FSx File Gateway, Volume Gateway (cache & stored), Tape Gateway
Transfer Family: FTP, FTPS, SFTP inerface on top of Amazon S3 or Amazon EFS
DataSync: Schedule data sync from on-premises to AWS, or AWS to AWS
Snowcone / Snowball / Snowmobile: to move large amount of data to the cloud, physically
Database: for specific workloads, usually with indexing and querying






-------------------------------------------------------------------------------------------------------------------------------------------
ELB & ASG
-------------------------------------------------------------------------------------------------------------------------------------------

Scalability & High Availability
---------------------------------
Scalability means that an application/system can handle greater loads by adapting
There are two kinds of Scalability:
    Vertical Scalability
    Horizontal Scalability (=elasticity)
Scalability is linked but different to High Availability

    Vertical Scalability
        Vertical Scalability means increasing the size of instance
        For example, your application runs on a t2.micro
        Scaling that application vertically means running on a t2.large
        Vertical scalability is vert common for non distributed systems. such as a database
        RDS, ElastiCache are services that can scale vertically.
        There's usually a limit to how much you can vertically scale (hardware limit)
    
    Horizontal Scalability
        Horizontal Scalability means increasing the number of instances/systems for your application
        Horizontal scaling implies distributed systems
        This is very common for web applications/modern applications
        It's easy to horizontally scale thanks to cloud offerings such as Amazon EC2


High Availability
    High Availability usually goes hand in hand with horizontal scaling
    High availability means running your application/system in at least 2 data centers (== Availability Zones)
    The goal of high availability is to servive a data center loss
    The high availability can be passive (for RDS Multi AZ for example)
    The high availability can be active (for horizontal scaling)

Load Balancing
-----------------
Load Balances are servers that forward traffic to multiple servers (e.g./ EC2 instances) downstream
Load Balancers spread across multiple downstream instances

Why use a load balancer ?
    Spread laod across multiple downstream instances
    Expose a single point of access (DNS) to your application
    Seamlessly handle failures of downstream instances
    Do regular health checks to your instances
    Provide SSL termination (HTTPS) for your websites
    Enforce stickiness with cookies
    High availability across Zones
    Separate public trafffic from private traffic

Why use an Elastic Load Balancer?
    AWS gurantees that it will be working
    AWS takes care of upgrades, maintenance, high availability
    AWS provides only a few configuration knobs

It costs less to setup your own load balancer but it will be a lot more effort on your end

It is integrated with many AWS offerings/services
    EC2, EC2 Auto Scaling Groups, Amazon ECS
    AWS Certificate Manager (ACM), CloudWatch
    Route 53, AWS WAF, AWS Global Accelerator

Health Checks
    Health Checks are crucial for Load Balancers
    They enable the load balancer to know if instances it forwards traffic to are available to reply to requests
    The health check is done on a port and a route (/health is common)
    If the response is not 200 (OK), then the instance is unhealthy

Types of Load Balancers
-------------------------
- Classic Load Balancer (v1 - old generation) - 2009 - CLB
    HTTP, HTTPS, TCP, SSL (SecureTCP)
- Application Load Balancer (v2 - new generation) - 2016 - ALB
    HTTP, HTTPS, WebSocket
- Network Load Balancer (v2 - new generation) - 2017 - NLB
    TCP, TLS (secure TCP), UDP
- Gateway Load Balancer - 2020 - GWLB
    Operated at layer 3 (Network layer) - IP Protocol

Overall, it is recommended to use the newer generation load balancers as they provide more features
Some load balancers can be setup as internal(private) or external(public) ELBs

    Application Load Balancer (v2)
    --------------------------------
        Application load balancers is Layer 7 (HTTP)
        Load balancing to multiple HTTP applications across machines (target groups)
        Support for HTTP/2 and WebSocket
        Support redirects (from HTTP to HTTPS for example)
        Routing tables to different target groups:
            Routing based in path in URL (example.com/users)
            Routing based on hostname in URL (one.example.com & other.example.com)
            Routing based on Query String, Headers (example.com/users?id=123&order=false)
        ALB are a great fit for micro services & container-based application
        Has a port mapping feature to redirect to a dynamic port in ECS
        In comparision, we'd need multiple Classic Load Balancer per application

        Target Groups
            EC2 instances (can be managed by an Auto Scaling Group) - HTTP
            ECS tasks (managed by ECS itself) - HTTP
            Lambda functions - HTTP request is translated into a JSON event
            IP Addresses - must be private IPs
            ALB can route to multiple target groups
            Health checks are at the target group level
        
        Good To Know
            Fixed hostname (XXX.region.elb.amazonaws.com)
            The application servers don't see the IP of the client directly
                The true IP of the client is inserted in the header X-Forwarded-For
                We can also get Port (X-Forwarded-Port) and proto (X-Forwarded-Proto)
    
    
    Hands-On:
        Launch 2 instances with a same security-group for both (launch-wizard-1)
        Load Balancer > Application Load Balancer > Enter details > Network mapping -all 
        > Create a new SG and allow HTTP traffic > Choose new SG > Create Target Group and add the instances > 
        chooose new TG in Listners and routing section > Create Load Balancer

        From launch-wizard-1 sg we remove inbound rules with incomming HTTP requests from 0.0.0.0/0
        and add inbound rule accepting tarffic from load-balancer SG only.

        Load Balancer > Listners > ClickOn ALB > Rules > Manage Rules > We can add custom rules


    Network Load Balancer(v2)
    --------------------------
        Network Load Balancers (Layer 4) allow to:
            Forward TCP and UDP traffic to your instances
            Handle millions of request per seconds
            Less latency ~ 100ms (vs 400ms for ALB )
        NLB has one static IP per AZ, and supports assigning Elastic IP
        (helpful for whitelisting specific IP)
        NLB are used for extreme performance, TCP or UDP traffic
        Not included in the AWS free tier
        NLB Target Groups:
            EC2 instances
            IP Addresses - must be private IPs
            Application Load Balancer
            Health Checks support the TCP, HTTP and HTTPS Protocols
        
    Hands-On:
        Create a Load Balancer > Network Load Balancer > Network Mapping select all > 
        Create a target group and add all instances > chooose new TG in Listners and routing section >
        In the Security Group of the instances we need to add inbound rule for Http access from 0.0.0.0/0
        
        For NLB we don't define Security group for it. The Security group of EC2 defines access.


    Gateway Load Balancer:
    -----------------------
        Deploy, scale, and manage a fleet of 3rd party network virtual appliances in AWS
        Example: Firewalls, Intrusion Detection and Prevention Systems, Deep Packet Inspection Systems, 
        payload manipulation, ...       

        Operated at Layer 3 (Network Layer) - IP Packets
        Combines the following functions:
            Transparent Network Gateway - single entry/exit for all traffic
            Load Balancer - distributes traffic to your virtual appliances
        Uses the GENEVE protocol on port 6081

        Gateway Load Balancer target Groups can be EC2 Instances, IP Addresses - must be private IPs

Sticky Sessions (Session Affinity)
------------------------------------
It is possible to implement stickiness so that the same client is always redirected to the same instance behind the load balancer
This works for Classic Load Balancers & Application Load Balanvers
The "cookie" is used for stickiness has an expiration date we control
Use case: Make seure the user doesn't loose his session data
Enabling stickiness may bring imbalance to the load over the backend EC2 instances

    Sticky Sessions - Cookie Names  
        Application-based cookies
            Custom cookies
                Generated by the target
                Can include any custom attributes required by the application
                Cookie name must be specified individually for each target group
                Don't use AWSALB, AWSALBAPP, or AWSALBTG (reserved for use by the ELB)
            Application cookie
                Generated by the load balancer
                Cookie name is AWSALBAPP
        Duration-based Cookies
            Cookie generated by the load balancer
            Cookie name is AWSALB for ALB, AWSELB for CLB
        
        To enable cookie:
            Target Group > Actions > Edit attributes > Stickiness > Select type of Cookie > Save Changes

Cross-Zone Load Balancing
---------------------------
With Cross Zone Load Balancing:
    each load balancer instance even if in differnet AZ distributes traffic 
    evenly across all registered instances in all AZ.
Without Cross Zone Load Balancing:
    Requests are distributed to the instances of the node of the Elastic Load Balancer. Th ALB is
    going to send the traffic to the instances in its own AZ


SSL/TLS - Basics
------------------
  - An SSL Certificate allows traffic between your clients and your load balancer to be 
    encrypted in transit (in-flight encryption)
  - SSL refers to Secure Sockets Layer, used to encrypt connections
  - TLS refers to Transport Layer Security, which is a newer version
  - Nowadays, TLS Certificates are mainly used, but people still refer as SSL
  - Public SSL certificates are issued by Certificate Authorities (CA)
  - Comodo, Symantec, GoDaddy, GlobalSign, Digicert, Letsencrypt, etc...
  - SSL certificates have an expiration date and must be renewed

    Load Balancer - SSL Certificates
        The load balancer uses an X.509 certificate (SSL/TLS server certificate)
        You can manage certificates using ACM (AWS Certificate Manager)
        You can create upload your own certificates alternatively
        HTTPS listener:
            You must specify a default certificate
            You can add an optional list of certs to support multiple domains
            Clients can use SNI (Server Name Indication) to specify the hostname they reach
            Ability to specify a security policy to support older versions of SSL/TLS (legacy clients)
        
        SSL - Server Name Indication (SNI)
            SNI solves the problem of loading multiple SSL certificates onto one web server (to serve multiple websites)
            It's a "newer" protocol, and requires the client to indicate the hostname of the target server in the initial SSL handshake
            The server will then find the correct certificate, or return the default one
            Note:
                Only works for ALB & NLB (newer generation), CloudFront
                Does not work for CLB (older gen)
    
    Elastic Load Balancers - SSL Certificates

        Classic Load Balancer (v1)
            Support only one SSL certificate
            Must use multiple CLB for multiple hostname with multiple SSL certificates
        Application Load Balancer (v2)
            Supports multiple listeners with multiple SSL certificates
            Uses Server Name Indication (SNI) to make it work
        Network Load Balancer (v2)
            Supports multiple listeners with multiple SSL certificates
            Uses Server Name Indication (SNI) to make it work

How to enable SSL Certificates on both ALB and NLB ?

    EC2 > Load balancers > Select ALB > Add Listener > Add port > Add TG >  Select security policy > 

Connection Draining
--------------------
Feature naming
    Connection Draining - for CLB
    Deregistration Delay - for ALB & NLB

Time to complete "in-flight requests" while the instance is de-registering or unhealthy
Stop sending new requests to the EC2 instance which is de-registering
Between 1 to 3600 seconds (default: 300 seconds)
Can be disabled (set value to 0)
Set to a low value if your requests are short



Auto Scaling Groups (ASG)
--------------------------
In real-life, the ;load on your websites are application can Change
In the cloud, you can create and get rid of servers very quickly
The goal of an Auto Scaling Group (ASG) is to:
    Scale out (add EC2 instances) to match an increased load
    Scale in (remove EC2 instances) to match a decreased load
    Ensure we have a minimum and a maximum number of EC2 instances running
    Automatically register new instances to a load balancer
    Re-create an EC2 instance in case a previous one is terminated (ex: if unhealthy)
ASG are free (you only pay for the underlying EC2 instances)

A Launch Template (older "Launch Configurations" are deprecated)
    AMI + Instance Type
    EC2 User Data 
    EBS Volumes
    Security Groups
    SSH Key Pair
    IAM Roles for your EC2 Instances
    Network + Subnets Information
    Load Balancer Information
Min Size / Max Size / Initial Capacity
Scaling Policies

Auto Scaling - CloudWatch Alarms & Scaling
    It is possible to scale an ASG based on CloudWatch Alarms
    An alarm monitors a metric (such as Average CPU, or a custom metric)
    Metrics such as Average CPU are computed for the overall ASG instances
    Based on the alarm:
        We can scale-out policies (increase the number of instances)
        We can scale-in policies (decrease the number of instances)

Auto Scaling Groups - Dynamic Scaling Policies
    Target Tracking Scaling
        Most simple and easy to set-up
        Example: I want the average ASG CPU to stay at around 40%
    Simple / Step Scaling
        When a CloudWatch alarm is triggered (example CPU > 70%), then add 2 units
        When a CloudWatch alarm is triggred (example CPU < 30%), then remove 1
    Scheduled Actions
        Anticipate a scaling based on known usage patterns
        Example: increase the min capacity to 10 to 5 pm on Fridays 
    Prdeicitive Scaling
        Continuously forecast and schedule scaling ahed
    
    Good metrics ro scale on:
        CPUUtilization: Average CPU utilization across your instances
        RequestCountPerTarget: To make sure the number of requests per EC2 instances is stable
        Average Network In/Out (if you're application is network bound)
        Any custom metric (that you push using CloudWatch)
    
    Auto Scaling Groups - Scaling Cooldowns
        After a scaling activity happens, you are in the cooldown period (default 300 seconds)
        During the cooldown period, the ASG will not launch or terminate additional instances (to allow for metrics to stabilize)
        Advice : Use a ready-to-use AMI to reduce configuration time in order to be serving requests fasters and reduce the cooldown period






-------------------------------------------------------------------------------------------------------------------------------------------
DATABASES
-------------------------------------------------------------------------------------------------------------------------------------------

Amazon RDS Overview
--------------------
RDS Stands for Relational Database Service
It's a managed DB service for DB use SQL as a query language
It allows you to create databases in the cloud that are managed by AWS
    Postgres
    MySQL
    MariaDB
    Oracle
    Microsoft SQL Server
    Aurora (AWS Proprietary database)

Advantage over using RDS versus deploying DB on EC2
    RDS is a managed service:
        Automated provisioning, OS patching
        Continuous backups and restore to specific timestamp (Point in Time Restore)!
        Monitoring dashboards
        Read replicas for improved read performance
        Multi AZ setup for DR (Disaster Recovery)
        Maintenance windows for upgrades
        Scaling capability (vertical and horizontal)
        Storage backed by EBS (gp2 or io1)
    Can't SSH into your instances

RDS - Storage Auto Scaling
    Helps you increase storage on your RDS DB instance dynamically
    When RDS detects you are running out of free database storage, it scales automatically
    Avoid manually scaling your database storage
    You have to set Maximum Storage Threshold (maximum limit for DB storage)
    Automatically modify storage if:
        Free storage is less than 10% of allocated storage
        Low-storage lasts at least 5 minutes
        6 hours have passed since last modification
    Useful for applications with unpredictable workloads
    Supports all RDS database engines (MariaDB, MYSQL, PostgreSQL, SQL Server, Oracle)


RDS Read Replicas for read scalability
    If we want more reads and write in our database and our only db can't scales enough, we need Read Replicas
        Up to 5 Read Replicas
        Within AZ, Cross AZ or Cross Region
        Replication is ASYNC, so reads are eventually consistent
        Replicas can be promoted to their own DB
        Application must update the connection string to leverage read replicas
    Use Cases:
        You have a production database that is taking on normal load
        You want to run a reporting application to run some analytics
        You create a Read Replica to run the new workload there
        The production application is unaffected
        Read replicas are used for SELECT (=read) only kind of statements (not INSERT, UPDATE, DELETE)

        RDS Read Replicas - Network Cost
            In AWS there's a network cost when data goes from one AZ to another
            For RDS Read Replicas within the same region, you don't pay that fee
        
    RDS Multi AZ (Disaster Recovery)
        SYNC Replication
        One DNS name - automatic app failover to standby
        Increase availability
        Failover in case of loss of AZ, loss of network, instance or storage failure
        No manual intervention in apps
        Not using for scaling 
        The Read Replicas can be setup as Multi AZ for Disaster Recovery (DR)
    
    RDS - From Single-AZ to Multi-AZ
        Zero downtime operation (no need to stop the DB)
        Just click on "modify" for the database
        The following happens internally:
            A snapshot is taken
            A new DB is restored from the snapshot in a new AZ
            Synchronization is established between the two databases
    
    Hands-On:
        Databses > Create Database > Select engine > credentials > Choose Inatance Configurations >
        choose storage > enable Auto scaling if needed 
    
    RDS Custom
        Managed Oracle and Microsoft SQL Server Database with OS and database customization
        RDS: Automates setup, operation, and scaling of database in AWS
        Custom: access to the underlying database and OS so you can 
            Configure settings
            Install patches
            Enable native features
            Access the underlying EC2 Instance using SSH or SSM Session Manager
        De-activate Automation Mode to perform your customization, better to take a DB snapshot before
        RDS vs. RDS Custom
            RDS: entire database and the OS to be managed by AWS
            RDS Custom: full admin access to the underlying OS and the database



Amazon Aurora
----------------
Aurora is a proprietary technology from AWS (not open sourced)
Postgres and MySQL are both supported as Aurora DB (that means your drivers will work as if Postgres or MySQL database)
Aurora is "AWS cloud optimized" and claims 5x performance improvement over MySQL on RDS, over 3x the performance of Postgres on RDS
Aurora storage automatically grows in increments of 10GB, up to 128 TB.
Aurora can have 15 replicas while MySQL has 5, and replication process is faster (sub 10ms replica lag)
Failover in Aurora is instantaneous. It's HA native.
Aurora costs more than RDS (20% more) - but is more efficient

Aurora High Availability and Read Scaling
    6 copies of your data across 3 AZ:
        4 copiees out of 6 needed for writes
        3 copiees out of 6 need for reads
        Self healing with peer-to-peer replication
        Storage is striped across 100s of volumes
    One Aurora Instance that writes (master)
    Automated failover for master in less than 30 seconds
    Master + up to 15 Aurora Read Replicas serve reads
    Support for Cross Region replication

Aurora DB Cluster
    Master is the only thing that will write to the storage
    Client talks to the "Writer-End-Point" that is always pointing to master
    There is Read-replicas and we can enable auto-Scaling on it.
    There is "Reader-End-Point" that helps with connection load balancing and it connects automatically to all the read-replicas

    Features of Aurora  
        Automatic failover
        Backup and Recovery
        Isolation and security
        Industry compliance
        Push-button scaling
        Automated Patching with Zero Downtime
        Advanced Monitoring
        Routine Maintenance
        Backtrack: restore data at any point of time without using backups

Aurora Replicas - Auto Scaling
    If there are many Requests on Reader-Endpoint the Aurora databases will have 
    increased CPU Usage. In this case we can setup Replica Autoscaling. The Reader-Endpoint is going to be 
    extended to new Aurora Replicas created because of Autoscaling.

Aurora - Custom Endpoints
    There are Aurora databases of differnet sizes (i.e., db.r3.large and db.r5.2xlarge)
    We will define a Custom Endpoint on the db.r5.2xlarge. Basically we define a subset of Aurora Instances as a Custom Endpoint
    As these instances are powerful so they would be better to run Analytical queries on these specific replicas
    The Reader Endpoint is generally not used after defining Custom Endpoints
    We setup many custom Endpoint for many different kinds of workloads allowing us to query only a subset of aurora replicas

Aurora Serverless
    Automated database instantiation and auto-scaling based on actual Usage
    Good for infrequent, intermittent or unpredictable workloads
    No capacity planning needed
    Pay per second, can be more cost-effective

Aurora Multi-Master
    In case we want immediate failover for writer node (High Availability) -
    Every node does R/W 

Global Aurora
    Aurora Cross Region Read Replicas:
        Useful for disaster Recovery   
        Simple to put in place
    Aurora Global Database (recommended):
        1 Primary Region (read/write)
        Up to 5 secondary (read-only) regions, replication lag is less than 1 second
        Up to 16 Read Replicas per secondary region
        Helps in decreasing latency
        Promoting another region (for disaster recovery) has an RTO of < 1 minute
        **Typically cross-region replication takes less than 1 second**

Aurora Machine Learning
    Enables you to add ML-based predictions to your applications via SQL
    Simple, optimized and secure integration between Aurora and AWS ML services
    Supported services  
        Amazon SageMaker (usw with ML model)
        Amazon Comprehend (for sentiment analysis)
    You don't need to have ML experience
    Use cases: fraud detection, ads targeting, sentiment analysis, product recommendations


RDS backups 
    Automated backups:
        Daily full backup of the database (during the backup window)
        Transaction Logs are backed-up by RDS every 5 minutes
        => ability to restore to any point in time (from oldest backup to 5 minutes ago)
        1 to 35 days of retention, set 0 to disable automated backups
    Manual DB Snapshots
        Manually triggered by the user
        Retention of backup for as long as you want
    
    Trick: In a stopped RDS database, you will still pay for storage. If you plan on stopping it 
    for a long time, you should snapshot & restore instead

Aurora Backups
    Automated Backups   
        1 to 35 days (cannot be disabled)
        point-in-time recovery in the timeframe
    Manual DB Snapshots
        Manually triggered by the user
        Retention of backup for as long as you want
    
RDS & Aurora Restore options
    Restoring a RDS/Aurora backup or a snapshot created a new database
    Restoring MySQL RDS database from S3
        Create a backup of your on-premises database
        Store it on Amazon S3 (object storage)
        Restore the backup file onto a new RDS instance running MySQL
    Restoring MySQL Aurora cluster from S3
        Create a backup on your on-premises database using Percona XtraBackup
        Store the backup file on Amazon S3
        Restore the backup file onto a new Aurora cluster running MySQL


Aurora Database Cloning
    Create a new Aurora DB Cluster from an existing one
    Faster than snapshot & restore
    The new DB cluster uses the same cluster volume and data as the original but will change when data updates are made
    Very fast & cost-effective
    Useful to creata a "staging" database from a "production" database without impacting the production database


RDS & Aurora security
-----------------------

At-rest encryption
    Database master & replicas encryption using AWS KMS - must be defined as launch time
    If the master is not encrypted, the read replicas cannot be encrypted
    To encrypt an un-encrypted database, go through DB snapshot and restore as encrypted

In-flight encryption: TLS-ready by default, use the AWS TLS root certificates client-side

IAM Authentication: IAM roles to connect to your database (instead of username/pw)

Security Groups: Control Network Access to your RDS/Aurora DB

No SSH available except on RDS Custom

Audit Logs can be enabled and sent to CloudWatch Logs for longer retention


Amazon RDS Proxy
------------------

- Fully managed database proxy for RDS
- Allows apps to pool and share DB Connections established with the database
- Improving database efficiency by reducing the stress on database resources (eg CPU, RAM) and 
  minimize open connections (and timeouts)
- Serverless, autoscaling, high available (multi-AZ)
- Reduced RDS & Aurora failover time by up 66%
- Supports RDS (MySQL, PostgreSQL, MariaDB) and Aurora (MySQL, PostgreSQL)
- No code changes required for most apps
- Enforce IAM Authentication for DB, and securely store credentials in AWS Secrets Manager
- RDS Proxy is never publically accessible (must be accessed from VPC)

Use Case:
    When we use Lambda Functions if they access the DB they will connect and disconnect very fast 
    resulting in open connections and timeouts. So we use RDS Proxy to pool the connections into less
    connection to RDS Instance



Amazon ElastiCache 
--------------------

The same way RDS is to get managed Relational Databases ElastiCache is to get managed Redis or Memcached
Cache are in-memory databases with really high performance, low latency
Helps reduce load off of databases for read intensive workloads
Helps make your application stateless
AWS takes care of OS maintenance / patching, optimizations, setup, configuration, monitoring, failure recovery and backups.
Using ElastiCache involves heavy application code changes

ElastiCache Solution Architecture - DB Cache
    Application queries ElastiCache, if not available, get from RDS and store in ElastiCache
    Helps relieve load in RDS
    Cache must have an invalidation strategy to make sure the most current data is used there

ElastiCache Solution Architecture - User Session Store
    User logs into any of the application
    The application writes the session data into ElastiCache
    The user hits another instance of our application 
    The instance retrieves the data and the user is already logged in

ElastiCache -  Redis vs Memcached
    Redis
        Multi AZ with Auto-Failover
        Read Replicas to scale reads and have high availability
        Data Durability using AOF persistence
        Backup and restore features
    Memcached
        Multi-node for partitioning of data (sharding)
        No high availability (reploication)
        Non persistent
        No backup and restore
        Multi-threaded Architecture

ElastiCache - Cache Security
    All cache in ElastiCache:
        Do not support IAM Authentication
        IAM policies on ElastiCache are only used for AWS API-level security
    Redis AUTH
        You can set a "password/token" when you create a Redis cluster
        This is an extra level of security for your cache (on top of security groups)
        Support SSL in flight encryption
    Memcached
        Supports SASL-based authentication (advanced)

Pattern for ElastiCache
    Lazy Loading: all the read data is cached, data can become stale in cache
    Write Through: Adds or update data in the cache when written to a DB (no stale data)
    Session Store: store temporary session data in the cache (using TTL features)


ElastiCache - Redis Use Case
    Gaming Leaderboards are computationally complex
    Redis Sorted sets gurantee both uniqueness and element ordering
    Each time a new element added, it's ranked in real time, then added in correct order

Amazon Dynamo DB
------------------
    Fully managed, highly available with replication across multiple AZs
    NoSQL databases - not a relational database - with transaction support
    Scales to massive workloads, distributed database
    Millions of requests per seconds, trillions of row, 100s of TB of storage
    Fast and consistent in performance (single-digit millisecond)
    Integrated with IAM for security, authorization and administration
    Low cost and auto-scaling Capabilities
    No maintenance or patching, always available
    Standard & Infrequent Access (IA) Table Class

    DynamoDB - Basics
        DynamoDB is made of tables
        Each table has a Primary Key (must be decided at creation time)
        Each table can have an infinite number of items (rows)
        Each item has attributes (can be added over time)
        Maximum size of an item is 400KB
        Data types supported are
            Scalar Types - String, Number, Binary, Boolean, null
            Document Types - List, Map
            Set Types - String Set, Number Set, Binary Set
        **DynamoDB can rapidly evolve schemas**
    DynamoDB - Read/Write Capacity modes
        Control how you manage your table's capacity (read/write throughput)
        Provisioned Mode (default)
            You specify the number of reads/writes per second
            You need to pplan capacity beforehand
            Pay for provisioned Read Capacity Units (RCU) & Write Capacity Units (WCU)
            Possibility to add auto-scaling mode for RCU & WCU
        On-Demand Mode
            Read/Writes automatically scale up/down with your workloads
            No capacity planning needed
            Pay for what you use, more expensive ($$$)
            Great for unpredicyale workloads, steep sudden spikes

DynamoDB Accelerator (DAX)
    Fully-managed, highly available, seamless in-memory cache for DynamoDB
    Help solve read congestion by caching
    Microseconds latency for cached data
    Doesn't require application logic modification (compatible with existing DynamoDB API)
    5 minutes TTL for cache (default)

DynamoDB - Stream Processing
    Ordered stream of item-level modifications (create/update/delete) in a table
    Use cases:
        React to changes real time (welcome email to users)
        Real-time usage analytics
        Insert into derivatives tables
        Implement cross-region replication
        Invoke AWS Lambda on changes to your DynamoDB table
    DynamoDB Streams
        24 hours retention
        Limited no of consumers
        Process using AWS Lambda Triggers, or DynamoDB Stream Kinesis adapter

DynamoDB Global Tables  
    Make a DynamoDB table accessible with low latency in multiple-regions
    Active-Active replication
    Application can READ and WRITE to the table in any region
    Must enable DynamoDB Streams as a pre-requisite

DynamoDB - Backups for disaster recovery
    Continuous backups using point-in-time recovery (PITR)
        Optionally enabled for last 35 days
        Point-in-time recovery to any time within the backup window
        The recovery process creates a new table
    On-demand backups
        Full backups for long-term retention, untill explicitely deleted
        Doesn't affect performance or latency
        Can be configured and managed in AWS Backup (enables cross-region copy)
        The recovery provess creates a new table

DynamoDB - Integration with Amazon S3
    Export to S3 (must enable PITR)
        Works for any point of time in the last 35 days
        Doesnt affect the read capacity of your table
        Perform data analysis on top of DynamoDB
        Retain snapshots for auditing
        ETL on top of S3 data before importing back into DynamoDB
        Export in DynamoDB JSON or ION format
    Import to S3
        Import CSV, DynamoDB JSON or ION format
        Doesn't consume any write capacity
        Creates a new table
        Import errors are logged in CloudWatch Logs



-------------------------------------------------------------------------------------------------------------------------------------------
CLOUDFRONT & GLOBAL ACCELERATOR
-------------------------------------------------------------------------------------------------------------------------------------------

AWS CloudFront
----------------
Content Delivery Network (CDN)
Improves read performance, content is cached at the edge
Improves users experience
216 Point of Presence globally (edge locations)
DDoS protection (because worldwide), integration with Shield, AWS Web Application Firewall

CloudFront - Origins
    S3 bucket
        For distributing files and caching them at the edge
        Enhanced security with CloudFront Origin Access Control (OAC)
        OAC is replacing Origin Access Identity (OAI)
        CloudFront can be used as an ingress (to upload files to S3)

    Custom Origin (HTTP)
        Application Load Balancer
        EC2 instance
        S3 website (must first enable the bucket as a static S3 website)
        Any HTTP backend you want    

CloudFront vs S3 Cross Region Replication
    CloudFront:
        Global Edge network
        Files are cached for a TTL (maybe a day)
        Great for static content that must be available everywhere
    
    S3 Cross Region Replication:
        Must be setup for each region you want replication to happen
        Files are updated in the real-time
        Read only
        Great for dynamic content that needs to be available at low-latency in few regions


CloudFront and S3 Hands-On:
    MyBucket (private). We can use cloudFront to make these files accessible without making them public
    CloudFront > Origin Domain(select s3) > Origin access control > Create control settings > Default root object - index.html >
    Update the policy created by Origin Access Policy in S3 Bucket 


CloudFront Geographic Restriction
    You can restrict who can access your distribution
        Allowlist: Allow your users to access your your content only if they're in one of the countries on a list of approved countries
        Blocklist: Prevent your users from accessing your content in they're in one of the countries on a list of banned countries
    The "country" is determined using 3rd party Geo-IP database
    Use case: Copyright Laws to control access to content

CloudFront - Pricing
    We can reduce the number of edge locations for cost reduction
    Three price classes:
        Price Class All: all regions - best performance
        Price Class 200: most regions, but excludes the most expensive regions
        Price Class 100: only the least expensive regions

CloudFront - Cache Invalidations
    In case you update the back-end origin, CloudFront doesn't know about it and will only get the refreshed content after TTL has expired
    However we can force an entire or partial cacherefresh (thus bypassing the TTL) by performing a CloudFront Invalidation
    You can invalidate all files (*) or a special path (/images/*)


Global Accelerator
----------------------
    We have deployed an application and have global users who want to access it directly
    They go over the public internet, which can add a lot of latency due to many hops
    We wish to go as fast as possible through AWS network to minimize latency
    Unicast IP
        Unicast IP is one server holds one IP address
    Anycast IP:
        All servers hold the same IP address and the client is routed to the nearest one
    Global Accelerator uses Anycast IP to connect to nearest server

    Global Accelerator Leverage the AWS internal network to route to application
    2 Anycast IP are created for your application
    The Anycast IP sends traffic directly to Edge Locations
    The Edge locations send the traffic to your application

    AWS Global Accelerator
        Works with Elastic IP, EC2 instances, ALB, NLB, public or private
        Consistent Performance
            Intelligent routing to lowest latency and faster regional failover
            No issue with client cache (because IP doesn't change)
            Internal AWS network
        Health Checks
            Global Accelerator performs a health check of your applications
            Helps make your application global (failover less than 1 min for unhealthy)
            Great for disaster recovery(thanks to health checks)
        Security
            only 2 external IP need to be whitelisted
            DDoS protection thanks to AWS Shield
    
    AWS Global Accelerator vs CloudFront
        They both use the AWS global network and its edge locations around the world
        Both services integrate the AWS Shield for DDoS protection
        CloudFront
            Improves performance for both cacheble content (images and videos)
            Dynamic Content (Such as API acceleration and dynamic site delivery)
            Content is served at the edge
        Global Accelerator
            Improves performance for a wide range of applications over TCP or UDP
            Proxying packets at the edge to applications running in one or more AWS Regions
            Good fit for non-HTTP use cases, such as gaming (UDP), IoT(MQTT), or Voice over IP
            Good for HTTP use cases that require static IP addresses
            Good for HTTP use cases that required deterministic, fast regional failover
	Hands-On:
		We setup our EC2 instances in 2 zones
		Global Accelerator > Enter name > Listerner(Port 80, Protocal TCP for HTTP traffic >
		Endpoint groups (Enter the % of traffic we want in a region) > Health Checks > Add endpoint >
		Create accelerator  






-------------------------------------------------------------------------------------------------------------------------------------------
ROUTE 53
-------------------------------------------------------------------------------------------------------------------------------------------

What is DNS?

Domain Name System which translates the human friendly hostnames into the machine IP addresses
www.google.com => 172.217.18.36
DNS is the backbone of the Internet
DNS uses hierarchical naming structure

DNS Terminologies
    Domain Registrar: A, AAAA, CNAME, NS, ...
    DNS Records: A, AAAA, CNAME, NS, ...
    Zone File: contains DNS records
    Name Server: resolves DNS queries (Authoritative or Non-Authoritative)
    Top Level Domain (TLD): .com, .us, .in, .gov, .org, ... 
    Second Level Domain (SLD): amazon.com, google.com, ...

    http://api.www.example.com

    .com -> TLD
    .example -> SLD
    .www -> Sub Domain
    api -> Domain Name
    http: -> protocol


WEB BROWSER => LOCAL DNS SERVER => ROOT DNS SERVER (Managed by ICANN) 
                                => TLD DNS SERVER (Managed by IANA - Branch of ICANN)
                                => SLD DNS SERVER (Managed by Domain Registrar)


Amazon Route 53
-------------------
A highly available, scalable, fully managed and Authoritative DNS
    Authoritative = the customer (you) can update the DNS records
Route 53 is also a Domain Registrar
Ability to check the health of your resources
The only AWS service which provides 100% availability SLA
Why Route 53? 53 is a reference to the traditional DNS port

Route 53 - Records
    How you want to route traffic for a Domain
    Each record contains:
        Domain/subdomain Name - e.g., example.com
        Record Type - e.g., A or AAAA
        Value - e.g., 12.34.56.78
        Routing Policy - how Route 53 responds to queries
        TTL - amount of time the record cached at DNS Resolvers
    Route 53 supports the following DNS record types:
        A / AAAA / CNAME / NS
        CAA / DS / MX / NAPTR / PTR / SOA / TXT / SPF / SRV

Route 53 - Record Types
    A - maps a hostname to IPv4
    AAAA - maps a hostname to IPv6
    CNAME - maps a hostname to another hostname
        The target is a domain name which must have an A or AAAA record
        Can't create a CNAME record for the top node of a DNS namespace (Zone Apex)
    NS - Name Servers for Hosted Zone
        Control how traffic is routed for a domain

Route 53 - Hosted Zones
    A container for records that define how to route traffic to a domain and its subdomains
    Public Hosted Zones - contains records that specify how to route traffic on the Internet (public domain names) 
                          application1.mypublicdomain.com
    Private Hosted Zones - contain records that specify how you route traffic within one or more VPCs (private domain names)
                           application1.company.internal
    
    $0.50 per month per hosted Zone


Route 53 - Creating Record
----------------------------
Route53 > Hosted Zones > select Domain > Create Record > Enter Details

Route 53 - Records TTL (Time to Live)
---------------------------------------
Client is accessing our DNS Route53 and a webserver
Client sends request myapp.example.com? to Route53 and we get answer from DNS 
which is saying "Hey, This is an A record, here is IP 12.34.56.78 and there is a TTL"
Maybe a TTL of 300 seconds, TTL is saying "Client please cache this result for duration of TTL"

Idea is that we don't want to query the DNS too often because we don't expect records to change a lot
So using the response client has it can access our web server and do HTTP Request and Responses

High TTL - e.g., 24 hr
    Less traffic on Route 53
    Possibly outdated records
Low TTL - e.g., 60 sec.
    More traffic on Route53 ($$)
    Records are outdated for less time 
    Easy to change records

Except for Alias records, TTL is mandatory for each DNS record

CNAME vs Alias
-----------------
AWS Resources (Load Balancer, CloudFront...) expose an AWS hostname:
    lbl-1234.us-east-2.elb.amazonaws.com and you want myapp.mydomain.com

    CNAME:
        Points a hostname to any other hostname. (app.mydomain.com => xyz.anything.com)
        ONLY FOR NON ROOT DOMAIN NAME (eg., something.mydomain.com)
    Alias:
        Points a hostname to an AWS Resource (app.mydomain.com => xyz.amazonaws.com)
        Works for ROOT DOMAIN and NON ROOT DOMAIN (xyz.mydomain.com)
        Free of charge
        Native health check

Route53 - Alias Records
---------------------------
Maps a hostname to an AWS resource
An extension to DNS functionality
Automatically recognizes changes in the resources IP addresses
Unlike CNAME, it can be used for the top node of a DNS namespace (Zone Apex), e.g.: example.com
Alias Record is aloways of type A/AAAA for AWS resources (IPv4/IPv6)
You can't set the TTL

Route53 - Alias Records Targets
---------------------------------
Elastic Load Balancers
CloudFront Distributions
API Gateway
Elastic Beanstalk environments
S3 Websites
VPC Interface Endpoints
Global Accelerator accelerator
Route 53 record in the same hosted Zone
You cannot set an ALIAS record for an EC2 DNS name  

For Apex domains like aadarshnaik.com or stephanetheteacher.com CNAME cannot be created
only Ailas can be created

Route53 - Routing Policies
----------------------------
Define how Route53 responds to DNS queries
DNS doesn't route any traffic, it only responds to the DNS queries
Route53 Supports the following Routing Policies
    Simple
    Weighted
    Failover
    Latency based
    Geolocation
    Multi-Value answer
    Geoproximity (using Route 53 Traffic Flow feature)

Routing Policies - Simple
    Typically, route traffic to a single resource
    Can specify multiple values in the same record
    If multiple values are returned, a random one is chosen by client
    When Alias enabled, specify only one AWS resource
    Can't be associated with Health Checks
Routing Policies - Weighted
    Control the % of requests that go to each specific resource
    Assign each record a relative weight:
        traffic(%) = weight for a specific record / sum of all the weights for all records
        Weights don't need to sum up to 100
    DNS records must have the same sname and type
    Can be associated with health Checks
    Use case: load balancing between regions, testing new application versions...
    Assign a weight of 0 to a record to stop sending traffic to a resource
    If all records have weight 0, then all records will be returned equally
Routing Policies - Latency-based
    Redirect to the resource that has the least latency close to us
    Super helpful when latency for users is a priority
    Latency is based on traffic between users and AWS
    Germany users may be directed to the US (if that's the lawest latency)
    Can be associated with Health Checks (has a failover capability)

    Route53 - Health Checks
        HTTP Health Checks are only for public resources
        Health Check => Automated DNS Failover:
            Health checks that monitor an endpoint (application, server, other AWS resource)
            Health checks that monitor other health checks (Calculated Health Checks)
            Health checks that monitor CloudWatch Alarms (full control !!) - e.g., throttles of DynamoDB, alarms on RDS, custom metrics.
        Health Checks are integrated with CloudWatch metrics

    Health Checks - Monitor an Endpoint
        About 15 global Health checkers will check the endpoint health
            Healthy/Unhealthy Threshold - 3 (default)
            Interval - 30 sec (can set to 10 sec - higher cost)
            Supported protocol: HTTP, HTTPS and TCP
            if >18% of health checkers report the endpoint is healthy, Route 53 considers it Healthy. Otherwise, it's Unhealthy
            Ability to choose which locations you want Route53 to use
        Health Checks can pass only when the endpoint responds with the 2xx and 3xx status codes
        Health checks can be setup to pass/fail based on the text in the first 5120 bytes of the response
        Configure your router/firewall to allow incomming requests from Route53 Health Checkers

    Route53 - Calculated Health Checks  
        Combine the results of multiple Health Checks into a single Health Check
        You use OR, AND or NOT
        Can monitor upto 256 Child Health Checks
        Specify how many of the health checks need to pass to make the parent pass
        Usage: perform maintenance to your website without causing all health checks to fail

    Health Checks - Private Hosted Zones
        Route53 health checkers are outside the VPC 
        They can't access private Endpoints
        You can create a CloudWatch Metric and associate a CloudWatch Alrm, then creata a Health Check that checks the alarm itself

Routing Policy - Failover (Active-Passive)
    There is a primary and secondary instance. In Route53 we have to specify which is primary and secondary and for
    primary which Health Check is to be considered. If primary health check goes Unhealthy then client are redirected 
    to secondary instance

Routing Policies - Geolocation
    Different from Latency-based!
    This routing is based on user location
    Specify location by Continent, Country or by US State (if there's overlapping, most precise location selected)
    Should create a "Default" record (in case there's no match on location)
    Use cases: website localization, restrict content distribution, load balancing, ...
    Can be associated with Health Checks

Geoproximity Routing Policy
    Route traffic to your resources based on the geographic location of users and resources
    Ability to shift more traffic to resources based on the defined bias
    To change the size of the geographic region, Specify bias values:
        To expand (1 to 99) - more traffic to the resources
        To shrink (-1 to -99) - less traffic to the resource
    Resources can be:
        AWS resources (specify AWS region)
        Non-AWS resources (specify Latitude and Longitude)
    You must use Route53 Traffic Flow (advanced) to use this feature

Routing Policy - Multi-Value
    Use when routing tarffic to multiple resources
    Route53 return multiple values/resources
    Can be associated with Health Checks (return only values for healthy resources)
    Up to 8 healthy records are returned for each Multi-Value query
    Multi-Value is not a substitute for having an ELB

    Multi Value might seem like simple routing policy with multiple IP's but simple Routing 
    doesn't have health checks

3rd Party Registrar with Amazon Route53
    If you buy your domain on a 3rd Party registrar, you can still use Route53 as the DNS Service provider

    Create a Hosted Zone in Route53
    Update NS Records on 3rd party website to use Route53 Name Servers

    Domain Registrar != DNS Service
    But every Domain Registrar usually comes with some DNS features
    






-------------------------------------------------------------------------------------------------------------------------------------------
S3 - SIMPLE STORAGE SERVICE
-------------------------------------------------------------------------------------------------------------------------------------------
S3
-----
Amazon S3 allows people to store objects (files) in "buckets" (directories)
Buckets must have a globally unique name (across all regions all accounts)
Buckets are defined at the region level
S3 looks like a global service but buckets are created in a region
Naming convention
    No uppercase, No underscore
    3-63 characters long
    Not an IP
    Must start with lowercase letter or number
    Must NOT start with an prefix xn--
    Must NOT end with the suffix -s3alias

Amazon S3 - objects
---------------------
Objects (files) have a Key
The key is the FULL path:
    - s3://my-bucket/my_file.txt
    - s3://my-bucket/my_folder1/another_folder/my_file.txt
The key is composed of prefix + object name
    - s3://my-bucket/my_folder1/another_folder/my_file.txt
There's no concept of "directories" within buckets (although UI will trick you to think otherwise)
Just keys with very long names that contain slashes ("/")
Object values are the content of the body:
    Max. Object Size is 5TB (5000GB)
    If uploading more than 5GB, must use "multi-part upload"
Metadata (list of text key / value pairs - system or user metadata)
Tags (Unicode key/value pair - up to 10) - useful for security / lifestyle
Version ID (if versioning is enabled)

Amazon S3 - Security
-----------------------

    User-Based
        IAM Policies - which API calls should be allowed for a specific user from IAM
    
    Resource-Based
        Bucket Policies - bucket wide rules from S3 console - allows cross account
        Object Access Control List (ACL) - finer grain (can be disabled)
        Bucket Access Control List (ACL) - less common (can be disabled)
    
    Note: an IAM principle can access an S3 object if 
        The user IAM permissions ALLOW it OR the resource policy ALLOWS it
        AND there's no explicit DENY
    
    Encryption: encrypt object in Amazon S3 using encryption keys

    S3 Bucket policy
    -----------------
        JSON based policies
            Resources: buckets and objects
            Effect: Allow / Deny
            Actions: Set of API to Allow or Deny
            Principal: The account or user to apply the policy to
        
        Use S3 bucket for policy to:
            Grant public access to the bucket
            Force objects to be encrypted at upload
            Grant access to another account (Cross Account)

    {
        "Version": "2012-10-17"
        "Statement": [
            {
                "Sid": "PublicRead",
                "Effect": "Allow",
                "Principal": "*".
                "Action": [
                    "s3:GetObject"
                ],
                "Resource": [
                    "arn:aws:s3:::examplebucket/*"
                ]
            }
        ]
    }
    Hands-On:
        How to make a bucket policy ?
        Permissions > Allow Public Access > edit > Untick Block all public access >
        Bucket Policy > Edit > Policy Generator > Select type > Principal: * > Actions: GetObject >  
        ARN: arn:aws:s3:::examplebucket/* > Add Statement > Generate Policy

Amazon S3 - Versioning
----------------------
We can version files in Amazon S3
It is enabled at the bucket level
Same key overwrite will increment the "version": 1,2,3....
It is best practice to version buckets
-  protection against uninteded deletes
-  Easy rollback to previous version
Any file that is not versioned prior to enabling versioning will have version "null"
Suspending versioning does not delete the previous versions.

Amazon S3 - Replication  (CRR & SRR)
--------------------------------------
Must enable Versioning in source and destination buckets
Cross-Region Replication (CRR)
Same-Region Replication (SRR)
Buckets can be in different AWS accounts
Copying is asynchronous
Must give proper IAM permissions to S3
Use cases:
    CRR - compliance, lower latency access, replication across accounts
    SRR - log aggregation, live replication between production and test accounts

We create a new bucket for replication of a bucket
For replication to happen and work we need to enable versioning on both source and target bucket.
main Bucket > Management > Replication rule

Amazon S3 - Replication (Notes)
--------------------------------
After you enable Replication, only new objects are replicated
Optionally, you can replicate existing Objects using S3 Batch Replication
    Replicates existing objects and objects that failed replication

For DELETE operations
    Can replicate delete markers from source to target (optional setting)
    Deletions with a version ID are not replicated (to avoid malicious deletes)

There is no "chaining" of replication
    If bucket 1 has replication into bucket 2, which has replication into bucket 3
    Then objects created in bucket 1 are not replicated to bucket 3

S3 Storage Classes
--------------------
S3 Durability and Availability
    Durability


S3 Standard - general purpose storage of frequently accessed data. Fast access & object replication in multi AZ.
    Low Latency and High throughput
    Content Distribution
    Big Data Analytics
    Dynamic websites 
    Gaming Applications
S3 IA-Infrequent Access - Long-lived, but less frequently accessed data. Slow access, object replication in multi AZ
    Backups
    Disaster Recovery files
    Long term storage
S3 One Zone-IA - is for data that is accessed less frequently, but requires rapid access when needed. Slow access, no object replication.
    Used to store data not frequently accessed in cost efficient way
    Non Critical and easily reproducable data.
S3 Glacier - Low cost Storage class for data Archiving
	Low cost object storage meant for archiving/backup
	price for storage + object retrieval cost
	
    
    S3 Glacier Instant Retrieval - 
        Millisecond retrieval, great for data accessed once a quarter
        Minimum storage duration is 90 days

    S3 Glacier Flexible Retrieval - 
        Expedited(1 to 5 mins), Standard(3 to 5 hours), Bulk (5 to 12 hours) - free
        Minimum storage duration is 90 days

    S3 Glacier Deep Archive - Lowest cost storage,Long time storage, retrieval time of 12Hrs to 48 Hrs
        public sectors, Financial Services, Healthcare
        which need to store data for 7-10 years for compliance requirements should use this class
        Minimum storage duration is 180 days

    S3 Intelligent Tiering - Automatically moves data to most cost effective tier.
        If confused between S3 Standard and S3 Standard IA this can be used.
        If data is not used for 30 days its is moved to IA.
        Small monthly monitoring and auto-tiering fee
        Moves objects automatically between access tiers based on usage
        There are no retrieval charges in S3 Intelligent-Tiering

        Frequent Access tier (automatic): default tier 
        Infrequent Access tier (automatic): objects not accessed for 30 days
        Archive Instant Access tier (automatic): objects not accessed for 90 days
        Archive Access tier (optional): configurable from 90 days to 700+ days
        Deep Archive Access tier (optional): 180 days to 700+ days

Amazon S3 - Moving between Storage Classes
--------------------------------------------
We can transition objects between storage Classes

For infrequently accessed object, move them to Standard IA
For archive objects that you don't need fast access to, move toGlacier or Glacier Deep Archive
Moving objects can be automated using a Lifecycle Rules

Amazon S3 - Lifecycle Rules
    Transition Actions - configure objects to transition to another storage class
        Move objects to Standard IA class 60 days after creation
        Move to Glacier for archiving after 6 months
    
    Expiration actions - configure objects to expire (delete) after some time
        Access log files can be set to delete after a 365 days
        Can be used to delete old versions of files (if versioning is enabled)
        Can be used to delete incomplete Multi-Part uploads

    Rules can be created for a certain prefix (example:s3://mybucket/mp3/*)
    Rules can be created for certain object Tags (example: Department: Finance)

    - Your application on EC2 creates images thumbnails after profile photos are uploaded
      to Amazon S3. These thumbnails can be easily recreated, and only need to be kept for 
      60 days. The source images should be able to be immediately retrieved for these 60 days, 
      and afterwards, the user can wait up to 6 hours. How to design this Scenario?

      S3 source images can be on Standardm with a lifecycle configuration to transition them to 
      Glacier after 60 days

      S3 thumbnails can be on One-Zone IA, with a lifecycle configuration 
      to expire them (delete them) after 60 days
    
    - A rule in your company states that you should be able to recover your 
      deleted S3 objects immediately for 30 days, although this may happen 
      rarely. After this time, and for up to 365 days, deleted objects should be 
      recoverable within 48 hours 

      Enable S3 Versioning in order to have object versions, so that 
      "deleted objects" are in fact hidden by a "delete marker" and can be recovered

      Transition the "noncurrent versions" of the object to Standard IA
      Transition afterwards the "noncurrent versions" to Glacier Deep Archive

Amazon S3 Analytics - Storage Class Analysis
----------------------------------------------
    Help you decide when to transition objects to the right storage class
    Recommendations for Standard and Standard IA
        Does not work for One-Zone IA or Glacier
    Report is updated daily
    24 to 48 hours to start seeing data Analysis

Hands-On:
    Bucket > Management > Lifecycle Rule 


S3 - Requester Pays
---------------------
    In general, bucket owners pay for all Amazon S3 storage and adata transfer costs associated with
    their bucket

    With Requester Pays buckets, the requester instead of the bucket owner Pays
    the cost of the request and the data download from the bucket

    Helpful when you want to share large datasets with other accounts

    The requester must be authenticated in AWS (cannot be anonymous)

S3 Event Notifications
------------------------
    S3:ObjectCreated, S3:ObjectRemoved, S3:ObjectRestore, S3:Replication...
    Object name filtering possible (*.jpg)
    Use case: generate thumbnails of images uploaded to S3
    Can create as many "S3 events" as desired
    S3 event notifications typically deliver events in seconds but can sometimes take a min or longer

    S3 Event Notification with Amazon EventBridge
        From EventBridge we can setup rules and with hep of rules we can send 
        these events to 18 AWS Services as destinations

        Advanced filtering options with JSON rules (metadata, object size, name...)
        Multiple Destinations - ex Step Functions, Kinesis Streams / Firehose...
        EventBridge Capabilities - Archive, Replay Events, Reliable delivery

Hands-On:
    Create a SQS Queue and enhance Access Policy to allow S3 bucket to write into SQS Queue

    Bucket > Properties > Event notifications > Create event Notification > Enter details > 
    select Event types > Destination > select SQL Queue > Save changes
     
S3 - Baseline Performance
---------------------------
Amazon S3 automatically scales to high request rates, latency 100-200 ms
Your application can achieve at least 3500 PUT/COPY/POST/DELETE and 5500 GET/HEAD request per second per prefix in a bucket.

There are no limits to the number of prefix in a bucket
If we spread reads across 4 prefixes evenly, we can achieve 22000 requests per second for GET and HEAD

S3 Performance
---------------
Multi-Part Upload:
    - recommended for files > 100MB
      must use for files > 5GB
    - Can help parallelize uploads (speed up transfers)

S3 Transfer Acceleration
    - Increase transfer speed by transferring file to an AWS edge location 
      which will forward the data to the S3 bucket in the target region
    - Compatible with multi-part upload

S3 Byte-Range Fetches
    Parallelize GETS by requesting specific byte ranges
    Better resilience in case of failures

    If a file is too large we can download it in parts at the same time
    We can also request first few bytes or last few bytes according to needs

S3 Select & Glacier Select 
----------------------------
    Retrieve less data using SQL by performing server-side filtering
    Can filter by rows & columns (simple SQL statements)
    Less network transfer, less CPU cost client-side

S3 Batch operations
--------------------
Perform bulk operations on existing S3 objects with a single requests
example:
    Modify object metadata & Properties
    Copy objects between S3 buckets
    Encrypt un-encrypted objects
    Modify ACLs, Tags
    Restore objects from S3 Glacier
    Invoke Lambda function to perform custom action on each object
A job consists of a list of objects, the action to perform, and optional parameters
S3 Batch Operations manages retries, tracks progress, sends completion notifications, generate reports...
You can use S3 Inventory to get object list and use S3 Select to filter your objects

Amazon S3 Security
--------------------
Amazon S3 - Object Encryption
    You can encrypt objects in S3 buckets using one of 4 methods
    Server-Side Encryption (SSE)
        Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) - Enabled by default
            Encrypts S3 objects using keys handled, managed, and owned by AWS
        Server-Side Encryption with KMS Keys stored in AWS KMS (SSE-KMS)
            Leverage AWS Key Management Service (AWS KMS) to manage encryption keys
        Server-Side Encryption with Customer-Provided Keys (SSE-C)
            When you want to manage your own encryption keys
    Client-Side Encryption
        Use client libraries such as Amazon S3 Client-Side Encryption Library
        Clients must encrypt data themselves before sending to Amazon S3
        Clients must decrypt data themselves when retrieving from Amazon S3
        Customer fully manages the keys and encryption cycle


    Amazon S3 Encryption - SSE-S3
        Encryption using keys handled, managed, and owned by AWS
        Object is encrypted server-side
        Encryption type is AES-256
        Must set header "x-amz-server-side-encryption":"AES256"
        Enabled by default for new buckets & new objects 
    Amazon S3 Encryption - SSE-KMS
        Encryption using keys handled and managed by AWS KMS (Key Management Service)
        KMS advantages: user control + audit key usage using CloudTrail
        Object is encrypted server side
        Must set header "x-amz-server-side-encryption":"aws:kms"
        Limitations
            In you use SSE-KMS, you may be impacted by the KMS limits
            When you upload, it calls the GenerateDataKey KMS API
            When you download, it calls the Decrypt KMS API
            Count towards the KMS quota per second (5500, 10000, 30000 req/s based on region)
            You can request a quota increase using the Service Quotas Console
    Amazon S3 Encryption - SSE-C
        Server-Side Encryption using keys fully managed by the customer outside of AWS
        Amazon S3 does NOT store the encryption key you provide
        HTTPS must be used
        Encryption keys must Provided in HTTP headers, for every HTTP request made

    Amazon S3  - Encryption in transit (SSL/TLS)
        Encryption in flight is also called SSL/TLS
        Amazon S3 exposes two endpoints:
            HTTP Endpoint - not encrypted
            HTTPS Endpoint - encryption in flight
        HTTPS is recommended
        HTTPS is mandatory for SSE-C
        Most clients would use the HTTPS endpoint by default
    
Amazon S3 - Default Encryption vs Bucket Policies
--------------------------------------------------
SSE-S3 encryption is automatically applied to new objects stored in S3 bucket
Optionally, you can "force encryption" using a bucket policy and refuse any API 
                            call to PUT an S3 Object without encryption headers
Eg.,

{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Deny",
            "Action": "s3:PutObject",
            "Principal": "*",
            "Resource": "arn:aws:s3:::my-bucket/*",
            "Condition": {
                "StringNotEquals": {
                    "s3:x-amz-server-side-encryption": "aws:kms"
                }
            }
            
        }
    ]
}

Bucket Policies are evaluated before "Default Encryption"

S3 - CORS
---------------
Cross-Origin Resource Sharing (CORS)
Origin = scheme (protocol) + host (domain) + port
    example: https://www.example.com (implied port is 443 for HTTPS, 80 for HTTP)
Web Browser based mechanism to allow requests to ther origins while visiting the main origin
Same origin: http://example.com/app1 & http://example.com/app2
Different origins: http://www.example.com & http://other.example.com
The requests won't be fulfilled unless the other origin allows for the requests, using CORS Headers

Hands-On:
    If we have a image or html-file in different s3 bucket and we have to access it from first page, 
    we have to enable CORS for object in second s3 bucket. Other bucket should also have public access and 
    should be enabled as website. 
    Second-Bucket > Permissions > Cross-origin resource sharing 
    [
        {
            "AllowedHeaders": [
                "Authorization"
            ],
            "AllowedMethods": [
                "GET"
            ],
            "AllowedOrigins": [
                "<url of the first bucket with http://..... without the slash at the end>"
            ],
            "ExposeHeaders": [],
            "MaxAgeSeconds": 3000
        }
    ]


Amazon S3 - MFA Delete
------------------------
    - MFA (Multi-Factor Authentication) - force users to generate a code on a 
      device (usually a mobile phone or hardware) before doing important operations on S3
    - MFA will be required to:
        Permanently delete an object version
        Suspend Versioning on the bucket
    - MFA won't be required to:
        Enable Versioning
        List deleted versions
    To use MFA Delete, Versioning must be enabled on the bucket
    Only the bucket owner (root account) can enable/disable MFA Delete

    Hands-On: 
        Select Bucket > Properties > Edit Bucket Versioning
        We can enable MFA Delete using aws-cli
        Pre-req:
            Under IAM we have setup MFA for root account

S3 Access Logs
---------------
For audit purpose, you may want to log all access to S3 buckets
Any request made to S3, from any account, authorized or denied, will be logged into another S3 bucket
That data can be analyzed using data analysis tools...
The target logging bucket must be in the same AWS region
Warning
    Do not set your logging bucket to be the monitored bucket
    It will create a logging loop, and your bucket will grow exponentially
Bucket > Properties > Server access logging > enable > choose target logs bucket 


Amazon S3 - Pre-Signed URLs
-----------------------------
Generate pre-signed URLs using the S3 Console, AWS CLI or SDK
URL Expiration
    S3 Console - 1 min up to 720 mins (12 hours)
    AWS CLI - configure expiration with -expires-in parameter in seconds (default 3600 secs, max. 604800 secs ~ 168 hours)
Users given a pre-signed URL inherit the permissions of the user that generated the URL for GET/PUT
Examples:
    Allow only logged-in users to download a premium video from S3 bucket
    Allow an ever-changing list of users to download files by generating URLs dynamically
    Allow temporarily a user to upload a file to a precise location in your S3 bucket

    Hands-On:
        Select Bucket > Select object > Object actions > Share a presigned URL > Specify time > Create presigned url


S3 Glacier Vault Lock
-----------------------
Adopt a WORM (Write Once Read Many) model
Create a Vault Lock Policy
Lock the policy for future edits (can no longer be changed or deleted)
Helpful for compliance and data retention

S3 Object Lock (versioning must be enabled)
    Adopt a WORM (Write Once Read Many) model
    Block an object version deletion for a specific ampount of time
    Retention mode - compliance:
        Object versions can't be overwritten or deleted by any user, including the root user
        Objects retention modes can't be changed, and retention periods can't be shortened
    Retention mode - Governance:
        Most users can't overwrite or delete an object version or alter its lock settings
        Some users have special permissions to change the retention or delete the object
    Retention periods  
        protect the object for a fixed period, it can be extended
    Legal Hold:
        protect the object indefinitely, independent from retention period
        can be freely placed and removed using s3:PutObjectLegalHold IAM permission


S3 - Access Point
-------------------
There are Finance, Sales and Analytics people who will access the same data
We can give access by bucket policy but if we have more user groups Bucket policy can get complicated
So we create an Access Point for Finance group connected to Finance Data
Access point for Sale group connected to sales data
Analytics point for analytics group connected to Finance and Sales data

We will attach a Policy to grant r/w access to a specific /finance or /sales prefix
For Analytics group Policy to grant read access to all the bucket


S3 Object Lambda
------------------
Use AWS Lambda Functions to change the object before it is retrieved by the caller application
Only one S3 bucket is needed, on top of which we create S3 Access Point and S3 Object Lambda Access Points.

An Analytics application may need only deleted data from object 
We create a S3 Access Point on top of S3 Bucket and its connected to
a Lambda function. Lambda Function will redact the data as it is being 
retrieved from the s3 bucket and on top of this Lambda function we will 
create a S3 Object Lambda Access point and from here our Analytics application will
access our S3 bucket.

Use Cases:
    Redacting personally identifiable information for analytics or non-production environments
    Coverting across data formats, such as converting XML to JSON.
    Resizing and watermarking images on the fly using caller-specific details, such as the user who requestsed the object.







-------------------------------------------------------------------------------------------------------------------------------------------
DECOUPLING APPLICATIONS: SQS, SNS, Kinesis, Active MQ
-------------------------------------------------------------------------------------------------------------------------------------------

Introduction
    When we start deploying multiple applications, they will inevitably need to communicate with one another
    There are two patterns of application communication 
         - Synchronous communications (application to application)
         - Asynchronous / Event based (application to queue to application)
    Synchronous between applications can be problematic if there are sudden spikes of traffic
    What if you need to suddenly encode 1000 videos but usually it's 10
    In that case its better to decouple your applications,
    	using SQS: queue model
    	using SNS: pub/sub model
    	using Kinesis: real-time streaming model
    These services can scale independently from our application!

Amazon SQS
What's a queue?
    Multiple/single Producer sends message to SQS Queue. The consumers will Poll the messages from the queue, 
    from the message it will get some information. Process it and delete it back from the queue.

Amazon SQS - Standard Queue
    Oldest offering (over 10 years old)
    Fully managed service, used to decouple applications
    Attributes:
        Unlimited throughput, unlimited number of messages in queue
        Default retention of messages: 4 days, maximum for 14 days
        Low latency (<10ms on publish and receive)
        Limitation of 256KB per message sent
    Can have duplicate messages (at least once delivery, occasionally)
    Can have out of order messages (best effort ordering)

SQS - Producing Messages
    Produced to SQS using the SDK (SendMessage API)
    The message is persisted in SQS untill a consumer deleted it 
    Message retention: default 4 days, up to 14 days

    Example: send an order to be processed
        Order id
        Customer id
        Any attributes we want
    
    SQS standard: unlimited throughput

SQS - Consuming Messages
    Consumers (running on EC2 instances, servers, or AWS Lambda)...
    Poll SQS for Messages (receive up to 10 messages at a time)
    Process the messages (example: insert the message into an RDS database)
    Delete the messages using the DeleteMessage API

    SQS - Multiple EC2 Instances Consumers
        SQS Queue can have multiple consumers that will receive and process these messages in parallel
        At least once delivery
        Best-effort message ordering
        Consumers delete messages after processing them
        We can scale consumers horizontally to improve throughput of processing

    SQS with Auto Scaling Group (ASG)
        Our consumers would be running on EC2 Instances inside of an Auto Scaling Group
        They will be polling for messages from SQS Queue
        The ASG has to be autoscaling on some kind of metric and the metric available to us is Queue Length
        Queue Length is called ApproximateNumberOfMessages
        We can set an set an CloudWatch Alarm such as if queuelength goes over certain Level
        it will increase capacity of my ASG by some amount.
    
    SQS to decouple between application tiers
        If we had one big application that would take the data, process it and store it in S3 bucket
        It will take a lot of time to process.
        The request to process a file and actual processing of file can happen in 2 different applications
        So from frontend when we get a request to process a file we will send it to SQS Queue
        Then we can write a backend that would be in its own ASG to receive the messages, process the data 
        and insert it to S3 bucket.
        With this architecture we can scale the frontend and backend accordingly but independently

Amazon SQS - Security
    Encryption:
        In-flight encryption using HTTPS API
        At-rest encryption using KMS keys
        Client-side encryption if the client wants to perform encryption/decryption itself
    Access Controls: IAM policies to regulate access to the SQS API
    SQS Access Policies (similar to S3 bucket policies)
        Useful for cross-account access to SQS queues
        Useful for allowing other services (SNS, S3...) to write to an SQS queue

SQS - Message Visibility Timeout
	After a message is polled by a consumer, it becomes invisible to other consumers
    By default, the "message visibility timeout" is 30 seconds
    That means the message has 30 seconds to be processed
    After the message visibility timeout is over, the message is "visible" in SQS
    If the message is not processed within the visibility timeout, it will be processed twice
    A consumer could call the ChangeMessageVisibility API to get more time
    If visibility timeout is high (hours), and consumer crashes, re-processing will take time
    If visibility timeout is too low (seconds), we may get duplicates

SQS - Long polling 
    When a consumer requests messages from the queue, it can optionally "wait" for messages to aarrive if there are none in the queue
    This is called Long Polling
    LongPolling decreases the number of API calls made to SQS while increasing the efficiency and latency of your application
    The wait time can be between 1 sec to 20 sec (20 sec preferable)
    Long Polling is preferable to Short Polling
    Long Polling can be enabled at the queue level or at the API level using WaitTimeSeconds
SQS - FIFO queues
    FIFO - First In First Out (Ordering of messages in the queue)
    Limited throughput: 300 msg/s without batching, 3000 msg/s with
    Exactly-once send capability
    Messages are processed in order by the consumer


Amazon - SNS
---------------
    If we want to send one message to many receivers
    The "event producer" only sends message to one SNS topic
    As many "event receivers" (subscriptions) as we want to listen to the SNS topic notification
    Each subscriber to the topic will get all the messages (note: new feature to filter messages)
    Up to 12,500,000 subscriptions per topic
    100,000 topic limit 
    Many AWS services can send data directly to SNS for notifications

AWS SNS - How to Publish
    Topic Publish (using the SDK)
        Create a topic
        Create a subscription(or many)
        Publish to the topic
    Direct Publish (for mobile apps SDK)
        Create a platform application
        Create a platform endpoint
        Publish to platform endpoint
        Works with Google GCM, Apple APNS, Amazon ADM...

Amazon SNS - Security
    Encryption:
        In-flight encryption using HTTPS API
        At-rest encryption using KMS Keys
        Client-side encryption if the client wants to perform encryption/decryption itself
    Access Contols: IAM policies to regulate access to the SNS API
    SNS Access Policies (similar to S3 bucket policies)
        Useful for cross-account access to SNS topics
        Useful for allowing other services (S3...) to write to an SNS topic

SQS + SNS: Fan Out
    Push once in SNS, receive in all SQS queues that are subscribers
    Fully decoupled, no data loss
    SQS allows for: data persistence, delayed processing and retries of work
    Ability to add more SQS subscribers over time
    Make sure your SQS queue access policy allows SNS to write
    Cross-Region Delivery: works with SQS Queues in other regions

Application: S3 Events to multiple queues
    For the same combination of: event type (eg. object create) and prefix (e.g. images/) you can only have one S3 Event rule
    If you want to send the same S3 event to many SQS queues, use fan out

                                                                    |-->SQS Queues
    S3 Object created ---events---> S3 -----> SNS Topic ---Fan-out----->SQS Queues
                                                                    |--> Lambda Function

Application: SNS to Amazon S3 through Kinesis Data Firehose
    SNS can send to Kinesis and therefore we can have the following solution architecture

    Buying service -----> SNS Topic-----> Kinesis Data Firehose ---------> Amazon S3
                                                                     |---> Any supported KDF Destination 

Amazon SNS -  FIFO Topic
    Similar features as SQS FIFO:
        Ordering by Messages Group ID (all messages in the same group are ordered)
        Deduplication using a Deduplication ID or Content Based Deduplication
    Can only have SQS FIFO queues as subscribers
    Limited throughput (same throughput as SQS FIFO)

SNS - Message filtering 
    JSON policy used to filter messages sent to SNS topic's subscriptions
    If a subscription doesn't have a filter policy it receives every message

Kinesis Overview
    Makes it easy to collect, process and analyze streaming data in real-time
    Ingest real-time data such as: Application logs, Metrics, Website clickstreams, IoT telemetry data...
    Kinesis Data Streams: capture, process, and store data streams
    Kinesis Data Firehose: load data streams into AWS data stores
    Kinesis Data Analytics: analyze data streams with SQL or Apache Flink
    Kinesis Video Streams: capture, process, and store video streams 

Kinesis Data Streams 
    Its a way to stream big data in your systems
    Kinesis data streams are made up of multiple shards
    Shards are numbered and we have to provision this ahed of time
    Data is going to be split across no of shards provisioned and 
    shards are going to be defining stream capacity in terms of ingestion and consumption rights
    
    Producers are going to send Record(data) to Kinesis Data Streams
    Records consists of Partition Key and Data Blob (upto 1 MB)
    Producers can send data at rate of 1Mb/s or 1000 msg/s to Kinesis Data Stream
    Once data is in Kinesis Data Streams it can be consumed by many consumers
    The consumer received Partition key, Sequence no, Data Blob
    Consumers can consume data at standard(2mb/s for all consumers) or enhanced(2mb/s per shard per consumer)

    Data Renention between 1 day to 365 days
    Ability to reprocess (replay) data
    Once data is inserted in Kinesis, it can't be deleted (immutability)
    Data that shares the same partition goes to the same shard (ordering)
    Producers: AWS SDK, Kinesis Producer Library (KPL), Kinesis Agent
    Consumers:
        Write your own: Kinesis Client Library (KCL), AWS SDK
        Managed: AWS Lambda, Kinesis Data Firehose, Kinesis Data Analytics
    
    Kinesis Data Streams - Capacity modes
        Provisioned mode:
            You can choose the number of shards provisioned, scale manually or using API
            Each shard gets 1MB/s in 
            Each shard gets 2MB/s out
            You pay per shard provisioned per hour
        On-demand mode:
            No need to provision or manage the capacity
            Default capacity provisioned (4MB/s in or 4000 records per second)
            Scales automatically based on observed throughput peak during the last 30 days
            Pay per stream per hour & data in/out per GB

    Kinesis Data Streams Security
        Control access/Authorization using IAM policies
        Encryption in flight using HTTPS endpoints
        Encryption at rest using KMS
        You can implement encryption/decryption of data on client side (harder)
        VPC Endpoint available for Kinesis to access within VPC
        Monitor API calls using CloudTrail

Kinesis Data Firehose
    Kinesis data firehose takes data from sources and writes this data into destinations in batches
    3 kinds of destinations in Kinesis Data Firehose
        Amazon S3
        Amazon Redshift (COPY through S3)
        Amazon OpenSearch
        Many 3rd party destinations
    Fully Managed Service, no administration, automatic scaling, Serverless
        AWS: Redshift/S3/OpenSearch
        3rd party partner: Splunk/MongoDB/DataDog/NewRelic/...
        Custom:send to any HTTP endpoint
    Pay for data going through Firehose
    Near Real Time 
        60 seconds latency minimum for non full batches
        Or minimum 1 MB of data at a time
    Supports many data formats, conversions, transformations, compression
    Supports custom data transformations using AWS Lambda
    Can send failed or all data to a backup S3 bucket


Kinesis Data Streams vs Firehose
    Kinesis Data Streams
        Streaming service for ingest at scale
        Write custom code (producer / consumer)
        Real-time (~200 ms)
        Manage scaling (shard splitting / merging)
        Data storage for 1 to 365 days
        Supports replay capability
    Kinesis Data Firehose
        Load streaming data into S3/Redshift/OpenSearch/3rd party/custom HTTP
        Fully managed
        Near real-time (buffer time 60 sec)
        Automatic scaling
        No data storage
        Doesn't support replay capability


Ordering data into Kinesis 
    Imagine you have 100 truckson the roadsending GPS positions regauly into AWS
    You want to consume the data in order for each truck, so that you can track their movement accurately
    How should you send the data into Kinesis ?
    Answer: send using a "Partition Key" value of the "truck_id"
    The same key will always go to the same shard

Ordering data into SQS
    For SQS standard, there is no ordering
    For SQS FIFO, if you don't use a Group ID, messages are consumed in the order they are sent, with only one consumer
    We want to scale the number of consumers, but you want messages to be "grouped" when they are related to each other
    Then you use a Group ID (similar to Partition Key in Kinesis)

Kinesis vs SQS ordering
    Lets assume 100 trucks, 5 kinesis shards, 1 SQS FIFO
    Kinesis Data Streams:
        On average you'll have 20 trucks per shard
        Trucks will have their data ordered within each shard
        The maximum amount of consumers in parallel we can have is 5
        Can receive up to 5 MB/s of data
    SQS FIFO
        You only have one SQS FIFO queue
        You have 100 Group ID
        You can have up to 100 Consumers (due to the 100 Group ID)
        You have upto 300 messages per second (or 3000 if using batching)

Amazon MQ
-----------
    SQS, SNS are cloud native services: proprietary protocols from AWS
    Traditional applications running from on-premises may use open protocols suck as MQTT,AMQP,STOMP,Openwire,WSS
    When migrating to the cloud, unstead of re-engineering the application to use SQS and SNS, we can use Amazon MQ
    Amazon MQ is managed message broker service for RabbitMQ, Active MQ
    Amazon MQ doesn't scale as much as SQS / SNS
    Amazon MQ runs on servers, can run in Multi-AZ with failover
    Amazon MQ has both queue feature (~SQS) and topic features (~SNS) 







-------------------------------------------------------------------------------------------------------------------------------------------
CONTAINERS ON AWS
-------------------------------------------------------------------------------------------------------------------------------------------
What is Docker?
Docker is a software development platform to deploy apps
Apps are packaged in containers that can be run on any OS
Apps run the same, regardless of where they're run
    Any machine
    No compatibility issues
    Predictable behaviour
    Less work
    Easier to maintain and deploy
    Works with any language, any OS, any technology
Use cases: microservices architecture, lift-and-shift apps from on-premises to the AWS cloud

Aamzon ECS - EC2 Launch Type
    ECS = Elastic Container Service
    Launch Docker containers on AWS = Launch ECS tasks on ECS clusters
    EC2 Launch Type: you must provision & maintain the infrastructure (the EC2 instances)
    Each EC2 Instance must run the ECS Agent to register in the ECS Cluster
    AWS takes care of starting/stopping containers

Amazon ECS - Fargate Launch Type
    Launch Docker containers on AWS 
    You don not provision the infrastructure (no EC2 instances to manage)
    Its all serverless!
    You just create task definitions
    AWS just runs ECS Tasks for you based on the CPU/RAM you need
    To scale, just increase the number of tasks. Simple - no more EC2 instances

Amazon ECS - IAM Roles for ECS 
    EC2 Instance Profile (EC2 Launch Type only)
        Used by the ECS agent
        Makes API calls to ECS service
        Send container logs to CloudWatch Logs
        Pull Docker image from ECR
        Reference sensitive data in Secrets Manager or SSM Parameter Store
    ECS Task Role:
        Allows each task to have a specific role
        Use different roles for the different ECS Services you run
        Task Role is defined in the task definition

Amazon ECS - Load Balancer Integrations
    Application Load Balancer supported and works for most use cases
    Network Load Balancer recommended only for high throughput/high performance use cases, or to pair it with AWS Private Link
    Elastic Load Balancer supported but not recommended

Amazon ECS - Data Volumes (EFS)
    Mount EFS file systems onto ECS tasks
    Works for both EC2 and Fargate launch types
    Tasks running in any AZ will share the same data in the EFS file system
    Fargate + EFS = Serverless
    Use cases: persistent multi-AZ shared storage for your containers
    Note:
        Amazon S3 cannot be mounted as a file system
    
ECS Service Auto Scaling
    Automatically increase/descrease the desired number of ECS tasks
    Amazon ECS Auto Scaling uses AWS Application Auto Scaling
        ECS Service Average CPU utilization
        ECS Service Average Memory Utilization
        ALB Request Count Per Target - metric coming from the ALB
    Target Tracking - scale based on target value for a specific CloudWatch metric
    Step Scaling - scale based on a specified CloudWatch Alarm
    Scheduled Scaling - scale based on a specified date/time
    ECS Service Auto Scaling (task level) != EC2 Auto Scaling
    Fargate Auto Scaling is much easier to setup (because Serverless)

EC2 Launch Type - Auto Scaling EC2 Instances
    Accomodate ECS Service Scaling by adding underlying EC2 Instances
    Auto Scaling Group Scaling
        Scale your ASG based on CPU Utilization
        Add EC2 instances over time 
    ECS Cluster Capacity provider
        Used to automatically provision and scale the infrastructure for your ECS Tasks
        Capacity Provider paired with an Auto Scaling Group
        Add EC2 Instances when you're missing capacity (CPU, RAM...)

Amazon ECS Solution Architecture
    We have an Amazon ECS Cluster backed by Fargate, Our users are going to
    upload objects into our S3 bucket. These S3 bucket can be integrated with 
    Amazon Event Bridge to send all the events to it. Amazon Eventbridge can have
    a rule to run ECS tasks on the go. ECS Tasks created will have ECS Task role 
    associated with which it can get the object from S3 bucket process it and send 
    results to Dynamo DB

    Amazon ECS Cluster backed by Fargate and Amazon EventBridge and we schedule a rule
    to be triggred every 1 hour. This rule is going to run ECS tasks for us in Fargate
    so every 1 hour a new task will be created in our Fargate cluster. we can create a 
    ECS task role with access to Amazon S3. So our Container can do some batch processing 
    from Amazon S3. Whole process is serverless

Amazon ECR - Elastic Container Registry
    Store and manage Docker images on AWS
    Private and Public repository 
    Fully integrated with ECS, backed by Amazon S3
    Access is controlled through IAM (permission errors => policy)
    Supports Image vulnerability scanning, versioning, image tags, image lifecycle ,..

Amazon EKS 
    Amazon Elastic Kubernetes Service 
    It is a way to launch managed Kubernetes clusters on AWS
    Kubernetes is an open-source system for automatic deployment, scaling 
    and management of containerized (usually Docker) application
    Its an alternative to ECS, similar goal but different API
    EKS supports EC2 if you want to deploy worker nodes or Fargate to deploy serverless containers
    Use case: If your company is already using Kubernetes on-premises or in another cloud, and wants to migrate to AWS using Kubernetes
    Kubernetes is cloud-agnostic (can be used in any cloud)

    EKS - Node Types
        Managed Node Groups
        Creates and managed Nodes (EC2 instances) for you
        Nodes are part of an ASG managed by EKS
        Supports On-Demand or Spot Instances
    Self-Managed Nodes
        Nodes are created by you and registered to the EKS cluster and managed by an ASG
        You can use prebuilt AMI - Amazon EKS Optimized AMI
    
    Amazon EKS - Data Volumes
        Need to specify StorageClass manifest on your EKS Cluster
        Leverages a Container Storage Interface (CSI) compliant driver
        Support for 
            Amazon EBS  
            Amazon EFS (works with Fargate)
            Amazon FSx for Lusture
            Amazon FSx for NetApp ONTAP

AWS App Runner
-----------------
Fully managed service that makes it easy to deploy web applications amd APIs at scale
No Infrastructure experience required
Start with your source code or container image
Automatically builds and deploy the web app
Automatic scaling, highly available, load balancer, encryption
VPC access support
Connect to database, cache, and message queue services
Use cases: web apps, APIs, microservices, rapid production deployments



-------------------------------------------------------------------------------------------------------------------------------------------
SERVERLESS
-------------------------------------------------------------------------------------------------------------------------------------------

What is serverless?
Serverless is a new paradigm in which the developers don't have to manage servers anymore
They just deploy code
Initially... Serverless == FaaS (Function as a Service)
Serverless was pioneered by AWS Lambda but now also includes anything that's managed "databases, messaging, storage, etc."
Serverless does not mean there are no servers..
It means we just don't manage / provision / see them

Serverless in AWS
    AWS Lambda
    Dynamo DB
    AWS Cognito
    AWS API Gateway
    Amazon S3
    AWS SNS & SQS
    AWS Kinesis Data Firehose
    Aurora Serverless
    Step Functions
    Fargate

Why AWS Lambda
    EC2
        Virtual Servers in the Cloud
        Limited by RAM and CPU
        Continuously running
        Scaling means intervention to add / remove servers
    Amazon Lambda
        Virtual functions - no servers to manage !
        Limited by time - short executions
        Run on-demand
        Scaling is automated
    
Benefits of AWS Lambda
    Easy Pricing:
        Pay per request and compute time
        Free tier of 1,000,000 AWS Lambda requests and 400,000 GBs of compute time
    Integrated with the whole AWS suite of services
    Integrated with many programming languages
    Easy monitoring through AWS CloudWatch
    Easy to get more resources per functions (up to 10GB of RAM!)
    Increasing RAM will also improve CPU and network!

AWS Lambda Limits to know - per region
    Execution:  
        Memory allocation: 128MB - 10GB (1 MB increments)
        Maximum execution time: 900 seconds (15 minutes)
        Environment variables (4 KB)
        Disk capacity in the "function container" (in/tmp):512 MB to 10GB
        Concurrency executions: 1000 (can be increased)
    Deployment:
        Lambda function deployment size (compressed.zip): 50 MB
        Size of uncompressed deployment (code + dependencies): 250 MB
        Can use the /tmp directory to load other files at startup
        Size of environment variables: 4 KB

Customization At the Edge
    Many modern applications execute some form of the logic at the edge
    Edge Function:
        A code that you write and attach to CloudFront Distributions
        Runs close to your users to minimize latency
    CloudFront provides two types: CloudFront Functions & Lambda@Edge
    You don't have to manage any servers, deployed globally
    Use case: customize the CDN content

CloudFront Functions
    Lightweight functions written in JavaScript
    For high-scale, latency-sensitive CDN customizations
    Sub-ms startup times, millions of requests/second
    Used to change Viewer requests and responses:
        Viewer Request: after CloudFront receives a request from a viewer
        Viewer Response: before CloudFront forwards the response to the viewer
    Native feature of CloudFront (manage code entirely within CloudFront)
    Use Cases:   
        Cache key normalization
            Transform request attributes (headers, cookies, query strings, URL) to create an optimal Cache Key
        Header manipulation
            Insert/modify/delete HTTP headers in the request or response
        URL rewrites or redirects
        Request authentication & Authorization
            Create and validate user-generated tokens to allow/deny requests

Lambda@Edge
    Lambda functions written in NodeJS or Python
    Scales to 1000s of requests/second
    Used to change CloudFront requests and responses:
        Viewer Request - after CloudFront receives a request from a viewer
        Origin Request - before CloudFront forwards the request to the origin
        Origin Response - after CloudFront receives the response from the origin
        Viewer Response - before CloudFront forwards the response to the viewer
    Author your functions in one AWS Region (us-east-1), then CloudFront replicates to its locations
    Use Cases:
        Longer execution time (several ms)
        Adjustable CPU or memory
        Your code depends on a 3rd libraries (e.g., AWS SDK to access other AWS services)
        Network access to use external services for processing
        File system access to access to the body of HTTP requests

Lambda with RDS Proxy   
    If Lambda functions directly access your database, they may open too many connections under high load
    RDS Proxy
        Improves scalability by pooling and sharing DB connections
        Improve availability by reducing by 66% the failover time and preserving connections
        Improve security by enforcing IAM authentication and strong credentials in Secrets Manager
    The Lambda function must be deployed in your VPC, because RDS Proxy is never publically accessible

Invoking Lambda from RDS & Aurora
    Invoke Lambda functions from within your DB instance
    Allows you to process data events from within a database
    Supported for RDS for PostgreSQL and Aurora MySQL
    Must allow outbound traffic to your Lambda function from within your DB instnace (Public, NAT GW, VPC Endpoints)
    DB instance must have the required permissions to invoke the Lambda function (Lambda Resource-based Policy & IAM Policy)

RDS Event Notifications 
    Notifications that tells information about the DB instance itself (created, stopped, start, ...)
    You don't have any information about the data itself
    Subscribe to the following event categories: DB instance, DB snapshot, DB Parameter Group, DB Security Group, RDS Proxy, Custom Engine Version
    Near real-time events (up to 5 minutes)
    Send notifications to SNS to subscribe to events using EventBridge

API Gateway
-------------
Its a serverless offering from AWS Which allows us to create REST APIs that are going to be public and accessible to client
AWS Lambda + API Gateway: No infrastructure to manage
Support for the WebSocket Protocol
Handle API versioining (v1,v2...)
Handle different environments (dev, test, prod,...)
Handle security (Authentication and Authorization)
Create API Keys, handle request throttling
Swagger / Open API import to quickly define APIs
Transform and validate requests and responses
Generate SDK and API specifications
Cache API responses

API Gateway - Integrations High Level
    Lambda Function
        Invoke Lambda function
        Easy way to expose REST API backed by AWS Lambda
    HTTP
        Expose HTTP endpoints in the backend
        Example: internal HTTP API on premise, Application Load Balancer
        Why? Add rate limiting, caching, user authentications, API keys, etc...
    AWS Service
        Expose any AWS API through the API Gateway?
        Example: start an AWS Step Function workflow, post a message to SQS
        Why? Add authentication, deploy publicly, rate control...

    API Gateway - Endpoint Types
        Edge-Optimized - For global clients
            Requests are routed through the CloudFront Edge locations (improves latency)
            The API Gateway still lives in only one region
        Regional:
            For clients within the same region
            Cloud manually combine with CloudFront (more control over the caching strategies and the distribution)
        Private:
            Can only be accessses from your VPC using an interface VPC endpoint (ENI)
        
    API Gateway - Security
        User authentication through
            IAM Roles (useful for internal applications)
            Cognito (identify for external users - example mobile users)
            Custom Authorizer (your own logic)
        Custom Domain Name HTTPS securoity through intergation with AWS Certificate Manager (ACM)
            If using Edge-Optimized endpoint, then the certificate must be in us-east-1
            If using Regional endpoint, the ertificate must be in the API Gateway region

AWS Step Functions
-------------------
Build serverless visual workflow to orchestrate your Lambda Functions
Feature: Sequence, parallel, conditions, timeouts, error handling,...
Can integrate with EC2, ECS, On-Premises servers, API Gateway, SQS Queues, etc...
Possible to implement human approval feature
Use cases: order fulfillment, data processing, web applications, any workflow

Amazon Cognito
---------------
    Give users an identity to interact with our web or mobile application 
    Cognito User Pools:
        Sign in functionality for app users
        Integrate with API Gateway & Application Load Balancer
    Cognito identity Pools (Federated Identity)
        Provide AWS credentials to users so they can access AWS resources directly
        Integrate with Cognito User Pools as an identity provider
    Cognito vs IAM : "hundreds of users","mobile users","autheticate with SAML"

    Cognito Users Pools (CUP):
        Create a Serverless database of user for your web & mobile apps
        Simple login: Username (or email) / password combination
        Password reset
        Email & Phone Number Verification
        Multi-factor authentication (MFA)
        Federated Identities: users from Facebook, Google, SAML...
        CUP integrates with API Gateway and Application Load Balancer
    
    Cognito Identity Pools (Federated Identities)
        Get identities for "users" so they obtain temporary AWS credentials 
        Users source can be Cognito User Pools, 3rd party logins, etc...
        Users can then access AWS services directly or through API Gateway
        The IAM policies applied to the credentials are defined in Cognito
        They can be customized based on the user_id for fine grained control
        Default IAM roles for authenticated and guest users



DATABASES
---------------------------------------------------------------------------------------------------------------------------------------------------
Database Types
    RDBMS (=SQL/OLTP): RDS, Aurora - greate for joins
    NoSQL databases - no joins, no SQL: DynamoDB(~JSON), ElastiCache (key/value pairs)
    Neptune (graphs)m DocumentDB (for MongoDB), Keyspaces (for Apache Cassandra)
    Object Store: S3 (for big objects) / Glacier (for backups / archives)
    Data Warehouse (=SQL Analytics / BI): Redshift (OLAP), Athena, EMR
    Search: OpenSearch (JSON) - free text, unstructured searches
    Graphs: Amazon Neptune - displays relationships between data
    Ledger: Amazon Quantum Ledger Databases
    Time series: Amazon Timestream

Amazon RDS - Summary
    Managed PostgreSQL / MySQL / Oracle / SQL Server / MariaDB / Custom
    Provisioned RDS Instance Size and EBS Volume Type and Size
    Auto-scaling capability for Storage
    Support for Read Replicas and Multi AZ
    Security through IAM, Security Groups, KMS, SSL in transit
    Automated Backup with Point in time restore feature (upto 35 days)
    Manual DB Snapshot for longer-term recovery
    Managed and Scheduled maintenance (with downtime)
    Support for IAM Authentication, integration with Secrets Manager
    RDS Custom for access to and customize the underlying instance (Oracle & SQL Server)

Amazon Aurora - Summary
    Compatible API for PostgreSQL / MySQL, seperation of storage and compute
    Storage: data is stored in 6 replicas, across 3 AZ, auto-scaling of Read Replicas
    Compute: Cluster of DB Instance across multiple AZ, auto-scaling of Read Replicas
    Cluster: Custom endpoints for writer and reader DB instances
    Same security / monitoring / maintenance features as RDS
    Aurora Serverless - for unpredictable / intermittent workloads, no capacity planning
    Aurora Multi-Master - for Continuous writes failover (hight write availability)
    Aurora Global - upto 16 DB Read Instances in each region < 1 second storage replication
    Aurora Machine Learning: perform ML using SageMaker & Comprehend on Aurora 
    Aurora Database Cloning: new cluster from existing one, faster than restoring a snapshot
    Use case: same as RDS, but with less maintenance / more flexibility / more performance / more features

Amazon ElastiCache - Summary
    Managed Redis / Memcached (similar offering as RDS, but for caches)
    In-memory data store, sub-millisecond latency
    Must provision an EC2 instance type
    Support for Clustering (Redis) and Multi AZ, Read Replicas (sharding)
    Security through IAM, Security Groups, KMS, Redis Auth
    Backup / Snapshot / Point in time restore feature
    Managed and Scheduled maintenance
    Requires some application code changes to be leveraged
    Use case: Key/Value store, Frquent reads, less writes, cache results for DIB queries, 
    store session data for websites, cannot use SQL

Amazon DynamoDB - Summary
    AWS proprietary technology, managed serverless NoSQL database, millisecond latency
    Capacity modes: provisioned capacity with optional auto-scaling or on-demand capacity
    Can replace ElastiCache as a key/value store 
    Highly available, Multi AZ by default, Read and Writes are decoupled, transaction capability
    DAX cluster for read cache, microsecond read latency
    Security, authentication and authorization is done through IAM
    Event Processing: DynamoDB Streams to integrate with AWS Labda, or Kinesis Data Streams
    Global Table feature: active-active setup
    Automated backups up to 35 days with PITR (Point in time recovery), or on demand backups
    Export to S3 without using RCU within the PITR window, import from S3 without using WCU
    Great to rapidly evolve schemas
    Use Cases: Serverless applications development, distributed serverless cache, doesn't have SQL query language available

Amazon S3 - Summary
    S3 is a key/value store for objects
    Great for bigger objects, not so great for many small objects
    Serverless, scales infinitely, max object size is 5 TB, versioning capability
    Tiers: S3 Standard, S3 Infrequent Access, S3 Intelligent, S3 Glacier + lifecycle policy
    Features: Versioning, Encryption, Replication, MFA-Delete, Access Logs
    Security: IAM, Bucket Policies, ACL, Access Ponts, Object Lambda, CORS, Object/Vault Lock
    Encryption: SSE-S3, SSE-KMS, SSE-C, client-side, TLS in transit, default encryption
    Batch operations on objects using S3 Batch, listing files using S3 Inventory
    Performance: Multi-part upload, S3 Transfer Acceleration, S3 Select
    Automation: S3 Evenet Notifications (SNS, SQS, Lambda, EventBridge)
    Use Cases: static files, key value store for big files, website hosting

Amazon DocumentDB - Summary
    Aurora is an "AWS-implementation" of PostgreSQL / MySQL
    DocumentDB is the same for MongoDB
    MongoDB is used to store, query, and index JSON data
    Similar "deployment concepts" as Aurora
    Fully Managed, highly available with replication across 3 AZ
    DocumentDB storage automatically grows in increments of 10GB upto 64TB
    Automatically scales to workloads with millions of requests per seconds

Amazon Neptune
    Fully managed graph database
    A popular graph dataset would be a social network
        Users have friends 
        Post have comments
        Comments have likes from users
        Users share and like posts...
    Highly available across 3 AZ, with upto 15 read replicas
    Build and run applications working with highly connected datasets - optimized for
    these complex and hard queries
    Can store upto billions of relations and query the graph with milliseconds latency
    Highly availabe with replications across multiple AZs
    Great for knowledge graphs (Wikipedia), fraud detection, recommendation engines, social networking

Amazon Keyspaces (for Apache Cassandra)
    Apache Cassandra is an open-source NoSQL distributed database
    A managed Apache Cassandra-compatible database service 
    Serverless, Scalable, highly available, fully managed by AWS
    Automatically scale tables up/down based on the application's traffic
    Tables are replicated 3 times across multiple AZ
    Using the Cassandra Query Language(CQL)
    Single-digit millisecond latencyat any scale, 1000s of requests per second
    Capacity: On-demand mode or provisioned mode with auto-scaling
    Encryption, backup, Point-In-Time Recovery up to 35 days
    Use cases: store IoT devices info, time-series data,...

Amazon QLDB
    QLDB stands for "Quantum Ledger Database"
    A ledger is a book recording financial transaction
    Fully managed, Serverless, High available, Replication across 3 AZ
    Used to review history of all the changes made to your application data over time
    Immutable system: no entry can be removed or modified, cryptographically verifiable
    2-3x better performance than common ledger blockchain frameworks, manipulate data using SQL
    Difference with Amazon Managed BlockchainL no decentralization component, in accordance with financial regulation rules

Amazon Timestream
    Fully managed, fast, scalable, serverless time series database
    Automatically scales up/down to adjust capacity
    Store and analyze trillions of events per day
    1000s times faster & 1/10th the cost of relational databases
    Scheduled queries, multi-measure records, SQL compatibaility
    Data storage tiering: recent data kept in memory and historical data kept in cost optimized storage
    Built in time series analytics functions (helps you identify pattens in your data in near real time)
    Encryption in transit and at rest
    Use cases: IoT apps, operational applications, real-time analytics,...

Amazon Athena
    Serverless query serive to analyze data stored in Amazon S3
    Uses stabdard SQL language to query the files (buit in Presto)
    Supports CSV, JSON, ORC Avro and Parquet
    Pricing: $5.00 per TB of data scanned
    Commonly used with Amazon Quicksight for reporting/dashboards
    Use cases: Business Intelligence / analytics / reporting, analyze and query VPC Flow Logs, ELB Logs, CloudTrail trails etc

    Athena - Performance improvement
        Use columnar data for cost-savings (less scan)
            Apache Parquet or ORC is recommended
            Huge performance improvement
            Use Glue to convert yout data for Parquet or ORC
        Compress data for smaller retrievals (bzip2, gzip, zlip...)
        Partition datasets in S3 for easy quering on virtual columns
        Use larger files (> 128 gb) to minimize overhead
    
    Amazon Athena - Federated Query
        Allows you to run SQL queries across data stored in relational, non-relational,
        object, and custom data sources (AWS or on-premises)

        Uses Data Source Connectors that run on AWS Lambda to run Federated Queries
        (e.g. CloudWatch Logs, DynamoDB, RDS etc.)

        Store the results baack in Amazon S3

Amazon Redshift
    Redshift is based on PostgreSQL
    Its OLAP - online analytical processing (analytics and data warehousing)
    10x better performance than other data warehouses, scale to PBs of data
    columnar storage of data (instead of row based) & parallel query engine
    Pay as you go based on the instances provisioned
    Has a SQL interface for performing the queries
    BI tools such as Amazon Quicksight or Tableau integrate with it
    Redshift has indexes which does faster queries/joins/aggregations than Athena

    Leader node: for query planning, results aggregation
    Compute node: for performing the queries, send results to Leader
    You provision the node size in advance
    You can use Reserved Instnaces for cost savings

    Redshift - Snapshots & DR
        Redshift has Multi-AZ mode for some clusters
        Snapshots are point-in-time backups of a cluster, stored internally in S3 
        Snapshots are incremental (only what changed is saved)
        You can restore a snapshot into a new cluster
        Automated: every 8 hours, every 5 GB, or on a schedule. Set retention
        Manual: snapshot is retained untill you delete it
        We can configure Redshift to automaticcally copy snapshots (automated or manual) of a cluster to another AWS region

    Redshift spectrum
        Query data that is already in S3 without loading it
        Must have a Redshift cluster available to start the query
        The query is then submitted to thousands of Redshift Spectrum nodes

Amazon OpenSearch Service
    Amazon OpenSearch is successor to Amazon ElasticSearch
    In DynamoDB, queries only exists by primary key or indexes...
    With OpenSearch, you can search any field, even partially matches
    It's common to use OpenSearch as a complement to another database
    Open Search requires a cluster of instances (not serverless)
    Does not support SQL (it has its own query language)
    Ingestion through Kinesis data Firehose, AWS IoT, and CloudWatch Logs
    Security through Cognito & IAM, KMS encryption, TLS
    Comes with OpenSearch Dashboards (visualization)  

Amazon EMR - Elastic MapReduce
    EMR stands for "Elastic MapReduce"
    EMR helps creating Hadoop clusters (Big Data) to analyze and process vast amount of data
    The clusters can be made of hundreds of EC2 instances
    EMR comes bundled with Apache Spark, HBase, Presto, Flink...
    EMR takes care of all the provisioning and cinfiguration
    Auto-scaling and integrated with Spot instances
    Use cases: data processing, machine learning, web indexing, big data...

    Amazon EMR - Node type and Purchasing
        Master Node: Manage the cluster, coordinate, manage health - long running
        Core Node: Run tasks and store data - long running
        Task Node (optional): Just to run tasks - usually Spot
        Purchasing options:
            On-demand: reliable, predictable, won't be terminated
            Reserved (min 1 year): cost savings (EMR will automatically use if available)
            Spot Instances: cheaper, can be terminated, less reliable
        Can have long-running cluster, or transient (temporary) cluster

Amazon Quicksight   
    Serverless Machine learning-powered business intelligence service to create interactive dashboards
    Fast, automatically scalable, embeddable, with per-session pricing
    Use cases:
        Business analytics
        Building visualizations
        Perform ad-hoc analysis
        Get business insights using data
    Integrated with RDS, Aurora, Athena, Redshift, S3...

    In-memory computation using SPICE engine if adata is imported into Quicksight
    Enterprise edition: Possibility to setup Column-Level security (CLS)

    QuickSight - Dashboard & Analysis
        Define Users (standard versions) and Groups (enterprise version)
            These users & groups only exist within QuickSight, not IAM !!
        A dashboard..
            is a read-only snapshot of an analysis that you can share
            preserves the configuration of the analysis (filtering, parameters, controls, sort)
        You can share the analysis or the dashboard with Users or Groups
        To share a dashboard, you must first publish it
        Users who see the dashboard can also see the underlying data
    
AWS Glue
    Manage extract, transform, and load (ETL) service
    Useful to prepare and transform data for analytics
    Fully serverless service

    Glue - things to know at a high-level
        Glue Job Bookmarks: prevent re-processing old data
        Glue Elastic Views:
            Combine and replicate data across multiple data stores using SQL
            No custom code, Glue monitors for changes in the source data, serverless
            Leverages a "virtual table" (materialized View)
        Glue DataBrew: clean and normalize data using pre-built transformation
        Glue Studio: new GUI to create, run and monitor ETL jobs in Glue
        Glue Streaming ETL (built on Apache Spark Structured Streaming):
            compatible with Kinesis Data Streaming, Kafka, MSK (managed Kafka)
    
AWS Lake Formation
    Data Lake = central place to have all your data for analytics purposes
    Fully managed service that makes it easy to setup a data lake in days
    Discover, cleanse, transform, and ingest data into your Data Lake
    It automates many complex manual steps (collecting, cleansing, moving, cataloging data,...) and de-duplicate (using ML Transforms)
    Combining structured and unstructured data in the data lake
    Out of the box source blueprints: S3, RDS, Relational & NoSQL DB...
    Fine-grained Access Control for your applications (row and column-level)
    Built on AWS Glue

Kinesis Data Analytics
    For SQL applications
        Real time analytics on Kinesis Data Streams & Firehose using SQL
        Add reference data from Amazon S3 to enrich streaming data
        Fully managed, no servers to provision
        Automatic scaling
        Pay for actual consumption rate
        Output:
            Kinesis Data Streams: create streams out of the real-time analytics queries
            Kinesis Data Firehose: send analytics query results to destinations
        Use cases:
            Time-series analytics
            Real-time dashboards
            Real-time metrics
    
    For Apache Flink
        Use Flink (Java, Scala or SQL) to process and analyze streaming data
        Run any Apache Flink application on a managed cluster on AWS
            provisioning compute resources, parallel computation, automatic scaling
            application backups (implemented as checkpoints and snapshots)
            Use any Apache Flink programming features
            Flink does not read from Firehose (use Kinesis Analytics for SQL instead)

Amazon Managed Streaming for Apache Kafka (Amazon MSK)
    Alternative to Amazon Kinesis 
    Fully managed Apache Kafka on AWS
        Allow you to create, update, delete clusters
        MSK creates and manages Kafka brokers nodes & Zookeeper nodes 
        Deploy the MSK cluster in you VPC, multi-AZ (up to 3 for HA)
        Automatic recovery from common Apache Kafka failures
        Data is stored on EBS volumes for as long as you want
    MSK Serverless
        Run Apache Kafka on MSK without managing the capacity
        MSK automatically provisions resources and scales compute and storage

Big Data Ingestion Pipeline
    We want the ingestion pipeline to be fully serverless
    We want to collect data in real time
    We want to transform the data
    We want to query the transformed data using SQL
    The reports created using the queries should be in S3
    We want to load the data into a warehouse and create dashboards

    Big Data Ingestion Pipeline discussion
        IoT Core allows you to harvest data from IoT devices
        Kinesis is great for real-time data collection
        Firehose helps with data delivery to S3 in near real time (1 minute)
        Lambda can help Firehose with data transformations
        Amazon S3 can trigger notifications to SQS
        Lambda can subscribe to SQS (we could have connector S3 to Lambda)
        Athena is a serverless SQL service and results are stored in S3
        The reporting bucket contains analyzed data and can be used by reporting tool such as QuickSight, Redshift, etc...














MACHINE LEARNING
----------------------------------------------------------------------------------------------------------------------------------------

Amazon Rekognition
    Find objects, people, text, scenes in images and videos using ML
    Facial Analysis and facial search to do user verification, people counting
    Creata a database of "familiar faces" or compare against celebrities
    Use cases:
        Labeling
        Content Moderation
        Text Detection
        Face Detection and Analysis
        Face search and Verification
        Celebrity Recognition
        Pathing (ex: for sports game analysis)
    Content Moderation
        Detect content that is inappropiate, unwantedm or offensive (image and videos)
        Used in social media, broadcast media, advertising, and e-commerce situations to create a safer user experience
        Set a Minimum Confidence Threshold for itemss that will be flagged
        Flag sensitive content for manual review in Amazon Augmented AI (A2I)
        Help Comply with regulations 

Amazon Transcribe
    Automatically convert speech to text
    Uses a deep learning process called automatic speech recognition (ASR) to 
    convery sppech to text quickly and accurately
    Automatically remove Personally Identifiable Information (PII) using Redaction
    Supports Automatic Language Identification for multi-lingual audio
    Use cases:
        transcribe customer service calls
        automate closed captioning and subtitling
        generate metadata for media assets to create a fully searchable archive

Amazon Polly
    Turn text into life like speech using deep learning
    Allowing you to create applications that talk

    Amazon Polly - Lexicon & SSML
        Customize the pronounciation of words with Pronounciation lexicons
            Stylized words: St3ph4ne => "Stephane"
            Acronyms: AWS => "Amazon Web Services"
        Upload the lexicons and use them in the SynthesizeSpeech operation
        Generate speech from plain text or from documents marked up with Speech Synthesis Markup Language (SSML) - enables more customization
            emphasizing specific words or phrases
            using phonetic pronounciation
            including breathing sounds, whispering
            using the Newscaster speaking style

Amazon Translate
    Natural and accurate language translation
    Amazon Translate allows you to localize content - such as websites
    and applications - for international users, and to easily translate large volumes of text efficiently
    
Amazon Lex & Connect
    Amazon Lex: (same technology that powers Alexa)
        Automatic Speech Recognition (ASR) to convert speech to text
        Natural Language Understanding to recognize the intent of text, callers
        Helps build chatbots, call center bots
    Amazon Connect:
        Receive calls, create contact flows, cloud-based virtual contact center
        Can integrate with other CRM systems or AWS
        No upfront payments, 80% cheaper than traditional contact center solutions \

Amazon Comprehend
    For Natural Language Processing - NLP
    Fully managed serverless service
    Uses machine learning to find insights and relationships in text
        Language of the text
        Extracts key phrases, places, people, brands, or events
        Understands how positive or negative the text is 
        Analyzes text using tokenization and parts of speech
        Automatically organizes a collection of text files by topic
    Sample use cases:
        analyze customer interactions (emails) to find what leads to a positive or negative experience
        Create and groups articles by topics that Comprehent will uncover

Amazon Comprehend Medical
    Amazon Comprehend Medical detects and returns useful information in unstructured clinical text
        Physician's Notes
        Dischange summaries
        Test results
        Case Notes
    Use NLP to detect Protected Health Information (PHI)
    Store your doccuments in Amazon S3, analyze real-time data with Kinesis Data Firehose, or
    use Amazon Transcribe to transcribe patient narratives into text that can be analyzed by
    Amazon Comprehend Medical.

Amazon SageMaker
    Fully managed service for developers / data scientists to build ML models
    Typically difficult to do all the processes in one place + provision servers
    Machine learning process (simplified): predicting your exam score

Amazon Forecast
    Fully managed service that uses ML to deliver highlt accurate forecasts
    Example: predict the future sales of a raincoat
    50% more accurate than looking at the data itself
    Reduce forecasting time from months to hours 
    Use cases: Product Demand Planning, Financial Planning, Resource Planning

Amazon Kendra
    Fully managed document search service powered by Machine Learning
    Extract answers from within a document (text, pdf, HTML, PowerPoint, MS Word, FAQs...)
    Natural language search Capabilities
    Learn from user interactions/feedback to promote preferred results (Incremental Learning)
    Ability to manually fine-tune search results (importance of data, freshness, custom,...)

Amazon Personalize
    Fully managed ML-service to build apps with real-time personalized recommendations
    Example: personalized product recommendations/re-ranking, customized direct marketing
        Example: User bought gardening tools, provide recommendations on the next one to buy
    Same technology used by Amazon.com
    Integrates into existing websites, applications, SMS, email marketing systems, ...
    Implement in days, not months (you don't need to build, train, and deploy ML solutions)
    Use cases: retail stores, media and entertainment...

Amazon Textract
    Automatically extracts text, handwriting, and data from any scanned documents using AI and ML
    Extract data from forms and tables
    Read and process any type of document (PDFs, images, ...)
    Use cases:
        Financial Services (e.g., invoices, financial reports)
        Healthcare (e.g., medical records, insurance claims)
        Public Sector (e.g., tax forms, ID documents, passports)

















AWS MONITORING & AUDIT: CLOUDWATCH, CLOUDTRAIL & CONFIG
-------------------------------------------------------------------------------------------------------------------

Amazon CloudWatch Metrics
    CloudWatch provides metrics for every services in AWS
    Metric is a variable to monitor (CPUUtilization, Networkln...)
    Metrics belong to Namespaces
    Dimention is an attribute of a metric (instance id, environment, etc...)
    Up to 10 dimentions per metric
    Metrics have timestamps
    Can create CloudWatch dashboards of metrics
    Can creata CloudWatch Custom Metrics (for the RAM for example)

    Cloudwatch Metric Streams
        Continually stream CloudWatch metrics to a destination of your choice, 
        with near-real-time delivery and low latency.
            Amazon Kinesis Data Firehose (and then its Destinations)
            3rd party service provider: Datadog, Dynatrace, New Relic, Splunk, Sumo Logic...
        Option to filter metrics to only stream a subset of them

CloudWatch Logs
    Log groups: arbitrary name, usually representing an application
    Log stream: instances within application / log files / containers
    Can define log expiration policies (never expire, 30 days, etc)
    CloudWatch Logs can send logs to:
        Amazon S3 (exports)
        Kinesis Data Streams
        Kinesis Data Firehose
        AWS Lambda
        OpenSearch
    Cloudwatch Logs - sources
        SDK, CLoudWatch Logs Agent, CloudWatch Unified Agent
        Elastic Beanstalk: collection of logs from application
        ECS: collection from containers
        AWS Lambda: collection from function logs
        VPC Flow Logs: VPC specific logs
        API Gateway
        CloudTrail based on filter
        Route53: Log DNS queries
    CloudWatch Logs Metric Filter & Insights
        CloudWatch Logs can use filter expressions
            For example, find a specific IP inside of a log
            Or count occurrences of "ERROR" in your logs
        Metric filters can be used to trigger CloudWatch alarms
        CloudWatch Logs Insights can be used to query logs and add queries to CloudWatch Dashboards
    CloudWatch Logs - S3 Export
        Log Data can take up to 12 hours to become available for export
        The APi call is CreateExportTask
        Not near-real time or real-time... use Logs Subscriptions instead

CloudWatch Logs for EC2
    By default, no logs from your EC2 machine will go to CloudWatch
    You need to run a CloudWatch agent on EC2 to push the log files you want
    Make sure IAM permissions are correct
    The CloudWatch log agent can be setup on-premises too

    CloudWatch Logs Agent & Unified Agent
        For virtual servers (EC2 instances, on-premises servers...)
        CloudWatch Logs Agent
            Old version of the agent
            Can only send to CloudWatch Logs
        
        CloudWatch Unified Agent
            Collect additional system-level metrics such as RAM, processes, etc...
            Collect logs to send to CloudWatch Logs
            Centralized configuration using SSM Parameter Store
        
        CloudWatch Unified Agent - Metrics
            Collected directly on your Linux server / EC2 instance
            CPU (active, guest, idle, system, user, steal)
            Disk Metrics (free, used, total), Disk IO (writes, read, bytes, iops)
            RAM (free, inactive, used, total, cached)
            Netstat (number of TCP and UDP connections, net packets, bytes)
            Processes (total, dead, bloqued, idle, running, sleep)
            Swap Space (free, used, used %)
            Reminder: out-of-the box metrics for EC2 - disk, CPU, network (high level)
        
CloudWatch Alarms
    Alarms are used to trigger notifications for any metric
    Various options (sampling, %, max, min, etc...)
    Alarm States:
        OK
        INSUFFICIENT_DATA
        ALARM
    Period:
        Length of time in seconds to evaluate the metric
        High resolution custom metrics: 10 sec, 30 sec or multiples of 60 sec
    
    CloudWatch Alarm Targets
        Stop, Terminate, Reboot or Recover an EC2 Instance
        Trigger Auto Scaling Action
        Send notification to SNS (from which you can do pretty much anything)
    
    CloudWatch Alarms - Composite Alarms
        CloudWatch Alarms are on a single metric
        Composite Alarms are monitoring the states of multiple other alarms
        AND and OR conditions
        Helpful to reduce "alarm noise" by creating complex composite alarms

    CloudWatch Alarm: good to knoow
        Alarms can be created on CloudWatch Logs Metrics Filter
        To test alarms and notification, set the alarm state to Alarm using CLI
        aws cloudwatch set-alarm-state --alarm-name "myalarm" --state-value ALARM --state-reason "testing purposes"

    Amazon EventBridge (Formerly CloudWatch Events)
        Schedule: Cron jobs (scheduled scripts)
        Ecent Pattern: Event rules to react to a service doing something
    
    CloudWatch Container Insights
        Collect, aggregate, summarize metrics and logs from containers
        Available for containers on ...
            Amazon Elastic Container Service (Amazon ECS)
            Amazon Elastic Kubernetes Services (Amazon EKS)
            Kubernetes platforms on EC2
            Fargate (both for ECS and EKS)
        In Amazon EKS and Kubernetes, CloudWatch Insights is using a containerized version 
        of the CloudWatch Agent to discover containers

    CloudWatch Lambda Insights
        Monitoring and troubleshooting solution for serverless applications running on AWS Lambda
        Collects, aggregates, and summarizes system-level metrics including CPU time, memory, disk and network
        Collects, aggregates and summarizes diagnostic information such as cold starts and Lambda worker shutdowns
        Lambda Insights is provided as a Lambda Layer
    
    CloudWatch Contributer Insights 
        Analyze log data and create time series that display contributer data.
            See metrics about the top-N contributers
            The total number of unique contributers, and their usage.
        This helps you find top talkers and understand who or what is impacting syatem performance
        Works for any AWS generated logs (VPC, DNS, etc..)
        For example, we can find bad hosts, identify the heaviest network users, or find the URLs that 
        generate the most errors
        We can build rules from scratch or use sample rules that AWS has created - leverages your CloudWatch Logs
        CloudWatch also provides built-in rules that you can use to analyze metrics from other AWS services
    
    CloudWatch Application Insights
        Provides automated dashboards that show potential problems with monitored applications, to help isolate ongoing issues
        Your application runs on Amazon EC2 Instances with select technologies only (java, .net, databases...)
        And we can use other AWS resources such as Amazon EBS, RDS, ELB, ASG, Lambda, SQS, DynamoDB, S3 bucket, ECS, EKS, SNI, API Gateway...
        Powered by SageMaker
        Enhanced visibility into your application health to reduce the time ot take you to troubleshoot and repair yor applications
        Findings and alerts are sent to Amazon EventBridge and SSM OpsCenter

    AWS CloudTrail
        Provides governance, compliance and audit for your AWS Account
        CloudTrail is enabled by default!
        Get an history of events / API calls made within your AWS Account by:
            Console
            SDK
            CLI
            AWS Services
        Can put logs from CloudTrail into CloudWatch Logs or S3
        A trail can be applied to All Regions (default) or a single Region
        If a resource is deleted in AWS, investigate CloudTrail first!

        CloudTrail Events
            Management Events:
                Operations that are performed on resources in your AWS account
                Examples:
                    Configuring security (IAM AttachRolePolicy)
                    Configuring rules for routing data (Amazon EC2 CreateSubnet)
                    Setting up logging (AWS CloudTrail CreateTrail)
                By default, trails are configured to log management events.
                Can seperate Read Events (that don't modify resources) from Write Events (that may modify resources)
            Data Events:
                By default, data events are not logged (because high volume operations)
                Amazon S3 object-level activity (ex: GetObject, DeleteObject, PutObject) can seperate Read and Write Events
                AWS Lambda function execution activity (the Invoke API)
        
        CloudTrail Insights 
            Enable CloudTrail Insights to detect unusual activity in your account:
                inaccurate resource provisioning
                hitting service limits
                Bursts of AWS IAM actions
                Gaps in periodic maintenance activity
            CloudTrail Insights analyzes normal management events to create a Baseline
            And then continuously analyzes write events to detect unusual patterns
                Anomalies appear in the CloudTrail console
                Event is sent to Amazon S3
                An EventBridge event is generated (for automation needs)
        
        CloudTrail Events Retention
            Events are stored for 90 days in CloudTrail
            To keep events beyond this period, log them to S3 and use Athena

        
    AWS Config
        Helps with auditing and recording compliance of your AWS resources
        Helps record configurations and changes over time
        Questions that can be solved by AWS Config
            Is there unrestricted SSH access to my security groups?
            Do my buckets have any public access?
            How has by ALB configurations changed over time?
        You can receive alerts (SNS notifications) for any changes
        AWS Config is a per-region service 
        Can be aggregated across regions and accounts
        Possibility of storing the configuration data into S3 (analyzed by Athena)

        Config Rules:
            Can use AWS managed config rules (over 75)
            Can make custom config rules (must be defined in AWS Lambda)
                Ex: evaluate if each EBS disk is of type gp2
                Ex: evaluate if each EC2 instance is t2.micro
            Rules can be evaluated / triggered
                For each config change
                And / or: at regular intervals
            AWS Config Rules does not prevent actions from happening (no deny)
            no free tier $0.003 per configuration item recorded per region, $0.001 per config rule evaluation per region

            Congig Rules - Remediations
                Automate remediations of non-compliant resources using SSM Automation Documents
                Use AWS Managed Automation Documents or create custom Automation Documents
                    You can create custom Automation Documents that invokes Lambda function
                You can set Remediation Retries if the resource is still non-compliant after auto-remediation
            Config Rules - Notifications
                Use EventBridge to trigger notifications when AWS resources are non-compliant
                Ability to send configuration changes and compliance state notifications to SNS (all events - use SNS Filtering or filter at client-side)

        
    CloudWatch vs CloudTrail vs Config
        CloudWatch
            Performance monitoring (metrics, CPU, network, etc...) & dashboards
            Events and Alerting
            Log Aggregation & Analysis
        CloudTrail
            Record API calls made within your Account by everyone
            Can define trails for specific resources
            Global Service
        Config
            Record configuration changes
            Evaluate resources against compliance rules
            Get timeline of changes and compliance

        For an Elastic Load Balancer
            CloudWatch
                Monitoring Incomming connections metric
                Visualize error codes as a % over time
                Make a dashboard to get an idea of your load balancer performance
            Config
                Track security group rules for the Load Balancer
                Track configuration changes for the Load Balancer
                Ensure an SSL certificate is always assigned to the Load Balancer (compliance)
            CloudTrail  
                Track who made any changes to the Load Balancer with API calls














IAM - ADVANCED
-----------------------------------------------------------------------------------------------
AWS Organizations
    Global service
    Allows to manage multiple multiple AWS accounts
    The main account is the management account
    Other accounts are member accounts
    Member accounts can be part of one organization
    Consolidated across all accounts - single payment method
    Pricing benefits from aggregated usage (volume discount from EC2, S3...)
    Shared reserved instances and Savings Plans discounts across accounts
    API is available to automate AWS account creation

    Advantages
        Multi Accounts vs One Account Multi VPC
        Use tagging standards for billing purposes
        Enable CloudTrail on all accounts, send logs to central S3 account
        Send CloudWatch Logs to central logging account
        Establish Cross Account Roles for Admin purposes
    Security: Service Control Policies (SCP)
        IAM policies applied to OU or Accounts to restrict Users and Roles
        They do not apply to the management account (full admin power)
        Must have an explicit allow (does not allow anything by default - like IAM)
 
    SCP Examples - Blocklist and Allowlist strategies
    BlockList
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Sid": "AllowAllActions",
                "Effect": "Allow",
                "Action": "*",
                "Resource": "*"
            },
            {
                "Sid": "DenyDynamoDB",
                "Effect": "Deny",
                "Action": "dynamodb:*",
                "Resource": "*"
            }
        ]
    }

    AllowList
    {
        "Version": "2012-10-17",
        "Statement": [
            {
                "Effect": "Allow".
                "Action": [
                    "ec2:*",
                    "cloudwatch:*"
                ],
                "Resources":"*"
            }
        ]
    }

    IAM Conditions
        AWS:SourceIP - restrict the client IP from which the API calls are being made
        aws:RequestedRegion - restrict the region the API calls are made to
        ec2:ResourceTag - restrict based on tags\
        aws:MultiFactorAuthPresent - to force MFA

        Resource Policies & aws:PrincipleOrgID
            aws:PrincipleOrgID can be used in any resource policies to restrict access to accounts that are 
            member of an AWS Organization

    IAM Roles vs Resource Based Policies
        Cross account:
            attaching a resource-based policy to a resource (example: S3 bucket policy)
            OR using a role as a proxy
        When you assume a role (user, application or service), you give up your 
        original permissions and take the permissions assigned to the role

        When using a resource-based policy, the principal doesn't have to give up his permissions.
        Example: User in account A needs to scan a DynamoDB table in Account A and dump it in S3 bucket in Account B.

        Supported by: Amazon S3 buckets, SNS topics, SQS queues, etc...
    
    Amazon EventBridge - Security
        When a rule runs, it needs permissions on the target
        Resource-based policy: Lambda, SNS, SQS, CloudWatch Logs, API Gateway...
        IAM role: Kinesis stream, Systems Manager Run Command, ECS task...


    IAM Permission Boundries
        IAM Permission Boundries are supported for users and roles (not groups)
        Advanced feature to use a managed policy to set maximum permissions an IAM entity can get.
    
        If we create a user and give him adminstrativeaccess but also set a permission boundry to S3 only. 
        Even if john has adminstrativeaccess. He won't be able to do administrative tasks

        Can be used in combinations of AWS Organizations SCP

        Use cases
            Delegate responsbilities to non administrators within their permission boundries, for example create new IAM users
            
            Allow developers to self-assign policies and manage their own permissions, while making sure they can't "escalate" their 
            privileges (= make themselves admin)

            Useful to restrict one specific user (instead of a whole account using Organizations & SCP)
        
    AWS IAM Identity Center (successor to AWS Single Sign-On)
        One login (single sign-on) for all your
            AWS accounts in AWS Organizations
            Business cloud applications (e.g., Salesforce, Box, Microsoft 365,...)
            SAML2.0-enabled applications
            EC2 Windows Instances
        
        Identity providers
            Built-in identity store in IAM Identity Center
            3rd party: Active Directory (AD), OneLogin, Okta...

        AWS IAM Identity Center Fine-grained Permissions and Assignments
            Multi-Account Permissions
                Manage access across AWS accounts in yout AWS Organization
                Permission Sets - a collection of one or more IAM Policies assigned
                                  to users and groups to define AWS access.
                Application Assignments
                    SSO access to many SAML 2.0 business applications (Salesforce, Box, Microsoft 365...)
                    Provide required URLs, certificates, and metadata 

What is Microsoft Active Directory (AD) ?

    Found on any Windows Server with AD Domain Services
    Database of objects: User Accounts, Computers, Printers, File Shares, Security Groups
    Centralized security management, create account, assign permissions
    Objects are organized in trees
    A group of trees is a forest

AWS Directory Services
    AWS Managed Microsoft AD
        Create your own AD in AWS, manage users locally, supports MFA
        Establish "trust" connections with your on-premise AD
    AD Connector
        Directory Gateway (proxy) to redirect to on-premise AD, supports MFA
        Users are managed on the on-premise AD
    Simple AD
        AD-compatible managed directory on AWS
        Cannot be joined with on-premise AD

    IAM Identity Center - Active Directory Setup
        Connect to an AWS Managed Microsoft AD (Directory Service)
            Integration is out of the box
        Connect to a Self Managed Directory
            Create Two-way Trust Relationship using AWS Managed Microsoft AD
            Create an AD Connector 

AWS Control Tower
    Easy to set up and govern a secure and compliant multi-account AWS environment based on best Practices
    Benefits:
        Automate the set up of your environment in a few clicks
        Automate ongoing policy management using guardrails
        Detect policy violations and remediate them
        Monitor compliance through an interactive dashboard
    
    AWS Control Tower - Guardrails
        provides ongoing governance for your Control Tower environment (AWS Accounts)
        Preventive Guardrail - using SCPs (e.g., Restrict Regions acorss all your accounts)
        Detective Guardrail - using AWS Config (e.g., indentify untagged resources)










AWS SECURITY & ENCRYPTION: KMS, SSM PARAMETER STORE, CLOUDHSM, SHIELD, WAF
------------------------------------------------------------------------------------------------------
Why encryption?
Server side encryption at rest
    Data is encrypted after being received by the server
    Data is decrypted before being sent
    It is stored in an encrypted form thanks to a key (usually a data key)
    The encruption / decryption keys must be managed somewhere and the server must have access to it

Client side encryption
    Data is encrypted by the client and never decrypted by the server
    Data will be decrypted by a receiving client
    The server should not be able to decrypt the data
    Could leverage Envelope Encryption

AWS KMS (Key Managenent Service)
    Anytime you hear "encryption" for an AWS service, it's most likely KMS
    AWS manages encryption keys for us
    Fully integrated with IAM for authorization
    Easy way to control access to your data 
    Able to audit KMS Key usage using CloudTrail
    Seamlessly integrated into most AWS services (EBS, S3, RDS, SSM...)
    Never ever store your secrets in plaintext, especially in your code
        KMS Key Encryption also available through API calls (SDK, CLI)
        Encrypted secrets can be stored in the code / environment variables

KMS Keys Types
    KMS Keys is the new name of KMS Customer Master Key
    Symmetric (AES-256 keys)
        Single encryption key that is used to Encrypt and Decrypt
        AWS services that are integrated with KMS use Symmetric CMKs
        You never get access to the KMS Key unencrypted (must call KMS API to use )
    Asymmentric (RSA & ECC key pairs)
        Public (Encrypt) and Private Key (Decrypt) pair
        Used for Encrypt/Decrypt, or Sign/Verify operations
        The public key is downloadable, but you can't access the Private Key enencrypted
        Use case: encryption outside of AWS by users who can't call the KMS API

Types of KMS Keys:
    AWS Owned Keys (free): SSE-S3, SSE-SQS, SSE-DDB (default key)
    AWS Managed Key: free (aws/service-name, example: aws/rds or aws/ebs)
    Customer managed keys created in KMS: $1 / month
    Customer managed keys imported (must be symmentric key): $1 / month
    + pay for API call to KMS ($0.03 / 10000 calls)

    Automatic Key rotation:
        AWS-managed KMS Key: automatic every 1 year
        Customer-managed KMS Key: (must be enabled) automatic every 1 year
        Imported KMS key: only manual rotation possible using alias

KMS Key Policies
    Control access to KMS keys, "similar" to S3 buckey policies
    Difference: you cannot control access without them

    Default KMS Key Policy
        Created if you don't provide a specific KMS Key Policy
        Complete access to the key to the root user = entire AWS account
    Custom KMS Key Policy:
        Define users, roles that can access the KMS key
        Define who can administer the key
        Useful for cross-account access of your KMS key

Copying Snapshots across accounts
    Create a Snapshot, encrypted with your own KMS Key 
    Attack a KMS Key Policy to authorize cross-account access
    Share the encrypted snapshot
    (in target) Create a copy of the Snapshot, encrypt it with a CMK in your account
    Create a volume from the snapshot

KMS Multi-Region Keys
    Identical KMS keys in deiffernet Regions that can be used interchangeably
    Multi-Region keys have the smae key ID, key material, automatic rotation
    Encrypt in one Region and decrypt in other region
    No need to re-encrypt or making cross-Region API calls
    KMS Multi-Region are NOT global (Primary + Replicas)
    Each Multi-Region key is managed independently
    Use cases: global client-side encryption, encryption on Global DynamoDB, Global Aurora

    DynamoDB Global Tables and KMS Multi-Region Keys Client-Side encryption
        We can encrypt specific attributes client-side in our DynamoDB table using the Amazon DynamoDB Encryption Client
        Combined with Global Tables, the client-side encrypted data is replicated to other regions
        If we use a multi-region key, replicated in the same region as the DynamoDB table, then the clients 
        in these regions can use low latency API calls to KMS in their region to decrypt the data client-side
        Using Client-side encryption we can protect specific fields and gurantee the decryption if the client has access to an API key
    Global Aurora and KMS Multi-Region Keys Client-Side encryption
        We can encrypt specific attributes client-side in our Aurora table using the AWS Encryption SDK
        Combined with Aurora Global Tables, the client-side encrypted data is replicated to other regions
        If we use a multi-region key, replicated in the same region as the Global Aurora DB, then client in these regions can use low-latency
        API calls to KMS in their region to decrypt the data client-side
        Using client-side encryption we can protect specific fields and gurantee only decryption if the client has access to an API key, we can 
        protect specific fields even from database admins 

S3 Replication Encryption Considerations
    Unencrypted objects and objects encrypted with SSE-S3 are replicated by default
    Objects encrypted with SSE-C (customer provided key) are never replicated
    For objects encrypted with SSE-KMS, you need to enable the option
        Specify which KMS key to encrypt the objects within the target bucket
        Adpat the KMS Key Policy for the target key
        An IAM Role with kms:Decrypt the source KMS Key and kms:Encrypt for the target KMS Key
        You might get KMS throttling errors, in which case you can ask for a Service Quotas increase
    You can use multi-region AWS KMS Keys, but they are currently treated as independent keys by 
    Amazon S3 (the object will still be decrypted and then encrypted)

AMI Sharing Process Encrypted via KMS
    1. AMI in Source Account is encrypted with KMS Key from Source Account
    2. Must modify the image attribute to add a Launch Permission which corresponds to the specified target AWS account
    3. Must share the KMS Keys used to encrypted the snapshot the AMI references with the target account / IAM Role
    4. The IAM Role/User in the target account must have the permissions to DescribeKey, ReEncrypted, CreateGrant, Decrypt.
    5. When launching an EC2 instance from the AMI, optionally the target account can specify a new KMS key in its own account to re-encrypt the volumes


SSM Parameter Store
    Secure storage for configuration and secrets
    Optionally Seamless Encryption using KMS
    Serverless, scalable, durable, easy SDK
    Version tracking of configurations / secrets
    Security through IAM
    Notifications with Amazon EventBridge
    Integration with CloudFormation

    We can access secrets of secrets manager from Parameter store by
        /aws/reference/secretsmanager/secret_ID_in_Secrets_Manager

AWS Secrets Manager 
    Newer service, meant for storing secrets
    Capability to force rotation of secrets every X days
    Automate generation of secrets on rotation (uses Lambda)
    Integration with Amazon RDS (MySQL, PostgreSQL, Aurora)
    Secrets are encrypted using KMS 
    Mostly meant for RDS integration

    AWS Secrets Manager - Multi Region Secrets
        Replicate Secrets across multiple AWS Regions
        Secrets Manager keeps read replicas in sync with the primary Secret
        Ability to promote a read replica Secret to a standalone Secret
        Use cases: multi-region apps, disaster recovery strategies, multi-region DB
    
AWS Certificate Manager
    Easily provision, manage, and deploy TLS Certificates
    Provide in-flight encryption for websites (HTTPS)
    Supports both public and private TLS certificates
    Free of charge for public TLS certificates
    Automatic TLS certificate renewal
    Integrations with (load TLS certificates on)
        Elastic Load Balancer (CLB, ALB, NLB)
        CloudFront Distributions
        APIs on API Gateway
    Cannot use ACM with EC2 (can't be extracted)

    ACM - Requesting Public Certificates
        1. List domain names to be included in the certificate
            Fully qualified Domain name (FQDN): corp.example.com
            Wildcard Domain: *.example.com
        2. Select Validation Method: DNS Validation or Email Validation
            DNS Validation is preferred for automation purposes
            Email validation will send emails to contact addresses in the WHOIS database
            DNS Validation will leverage a CNAME record to DNS config (ex: Route 53)
        3. It will take a few hours to get verified
        4. The Public Certificate will be enrolled for automatic renewal
            ACM automatically renews ACM-generated certificates 60 days before expiry
        
    ACM - Importing Public Certificates
        Option to generate the certificate outside of ACM and then import it
        No automatic renewal, must import a new certificate before expiry
        ACM sends daily expiration events starting 45 days prior to expiration
            The # of days can be configured
            Events are appearing in EventBridge
        AWS Config has a managed rule names acm-certificate-expiration-check to check for expiring certificates

API Gateway - Endpoint Types
    Edge-Optimized (default): For global clients
        Requests are routed through the CloudFront Edge locations (Improves latency)
        The API Gateway still lives in only one region
    Regional:
        For clients within the same region
        Cloud manually combine with CloudFront (more control over the caching strategies and the distribution)
    Private:
        Can only be accesses from your VPC using an interface VPC endpoint (ENI)
        Use a resource policy to define access

    ACM - Integration with API Gateway
        Create a Custom Domain Name in API Gateway
        Edge-Optimized (default): for global clients
            Requests are routed through the CloudFront Edge locations (improves latency)
            The API Gateway still lives in only one region 
            The TLS Certificate must be in the same region as CloudFront, in us-east-1
            Then setup CNAME or (better) A-Alias record in Route 53
        Regional:
            For clients within the same region
            The TLS Certificates must be imported on API Gateway, in the same region as the API Stage
            Then setup CNAME or (better) A-Alieas record in Route 53
        
AWS WAF - Web Application Firewall
    Protects your web applications from common web exploits (Layer 7)
    Layer 7 is HTTP (vs Layer 4 is TCP/UDP)

    Deploy on
        Application Load Balancer
        API Gateway
        CloudFront
        AppSync GraphQL API
        Cognito User Pool
    
    Define Web ACL (Web Access Control List) Rules:
        IP Set: up to 10,000 IP addresses - use multiple Rules for more IPs
        HTTP headers, HTTP body, or URI strings Protects from common attack - SQL injection and Cross-Site Scripting (XSS)
        Size constraints, geo-match (block countries)
        Rate-based rules (to count occurrences of events) - for DDoS protection
    Web ACL are Regional except for CloudFront
    A rule group is a reusable set of rules that you can add to a web ACL

    WAF - Fixed IP while using WAF with a Load Balancer
        WAF does not support the Network Load Balancer (Layer 4)
        We can use Global Accelerator for fixed IP and WAF on the ALB

AWS Shield: protect from DDoS attack
    DDoS: Distributed Denial of Service - many requests at the same time 
    AWS Shield Standard:
        Free service that is activated for every AWS customer
        Provides protection from attacks such as SYN/UDP Floods, Reflection attacks and other layer 3/layer 4 attacks
    AWS Shield Advanced:
        Optional DDoS mitigation service ($3,000 per month per organization)
        Protect against more sophisticated attack on Amazon EC2, ELastic Load Balncing (ELB), CloudFront, Global Accelerator and Route 53
        24/7 access to AWS DDoS response team (DRP)
        Protect againt highrt fees during spikes due to DDoS
        Shield Advanced automatic application layer DDoS mitigation automatically creates, evaluates and deploys AWS WAF rules to mitigate layer 7 attacks

AWS Firewall Manager
    Manage rules in all accounts of an AWS Organization
    Security policy: common set of security rules
        WAF rules (Application Load Balancer, API Gateways, CloudFront)
        AWS Shield Advanced (ALB, CLB, NLB, Elastic IP, CloudFront)
        Security Groups for EC2, Application Load Balancer and ENI resources in VPC
        AWS Network Firewall (VPC Level)
        Amazon Route 53 Resolver DNS Firewall
        Policies are created at the region level
    Rules are applied to new resources as they are created (good for compliance) across all and future accounts in your Organization

WAF vs Firewall Manager vs Shield
    WAF, Shield and Firewall Manager are used together for comprehensive protection
    Define your Web ACL rules in WAF
    For granular protection of your resources, WAF alone is the correct choice
    If you want to use AWS WAF across accounts, accelerate WAF configuration, automate the protection of new resources, use Firewall Manager with AWS WAF
    Shield Advanced adds additional features on top of AWS WAF, such as dedicated support from the Shield Response Team and advanced reporting
    If you are prone to frequent DDoS attacks, consider purchasing Shield Advanced

AWS Best Practices for DDoS Resiliency Edge Location Mitigation (BPI, BP3)

BPI - CloudFront
    Web Application delivery at the edge
    Protect from DDoS Common Attacks (SYN floods, UDP Reflection...)
BPI - Global Accelerator
    Access your application from the edge
    Integration with Shield for DDoS protection
    Helpful if your backend is not compatible with CloudFront
BP3 - Route 53
    Domain Name Resolution at the edge
    DDoS Protection mechanism

Best practices for DDoS mitigation
    Infrastructure layer defence (BP1, BP3, BP6)
        Protect Amazon EC2 against high traffic
        That includes using Global Accelerator, Route 53, CloudFront, Elastic Load Balancing
    Amazon EC2 with Auto Scaling (BP7)
        Helps scale in case of sudden traffic surges including a flash croud or a DDoS attack
    Elastic Load Balancing (BP6)
        Elastic Load Balancing scales with the traffic increases and will distribute the traffic to many EC2 instances

Application Layer defence
    Detect and filter malicious web requests (BP1, BP2)
        CloudFront cache static content and serve it from edge locations, protecting your backend
        AWS WAF is used on top of CloudFront and Application Load Balancer to filter and block requests based on request signatures
        WAF rate-based rules can automatically block the IPs of bad actors
        Use managed rules on WAF to block attacks based on IP reputation, or block anonymous IPs
        CloudFront can block specific geographies
    Shield Advanced (BP1, BP2, BP6)
        Shield Advanced automatic application layer DDoS mitigation automatically creates, evaluates and deploys AWS WAF rules to mitigate layer 7 attacks

Attack surface reduction
    Obfuscating AWS resources (BP1, BP4, BP6)
        Using CloudFront, API Gateway, Elastic Load Balancing to hide your backend resources (Lambda Functions, EC2 Instances)
    Security Groups and Network ACLs (BP5)
        Use security groups and NACLs to filter traffic based on specific IP at the subnet or ENI level
        Elastic IP are protected by AWS Shield Advanced
    Protecting API endpoints (BP4)
        Hide EC2, Lambda, elsewhere
        Edge-optimized mode, or CloudFront + regional mode (more control for DDoS)
        WAF + API Gateway: burst limits, headers filtering, use API keys

Amazon GuardDuty
    Intelligent Threat discovery to protect your AWS Account
    Uses Machine Learning algorithms, anomaly detection, 3rd party data
    One click to enable (30 days trial), no need to install software
    Input data includes:
        CloudTrail Events Logs - unusual API calls, unauthorized deployments
            CloudTrail Management Events - create VPC subnet, create trail,...
            CloudTrail S3 Data Events - get object, list object, delete object,...
        VPC Flow Logs - unusual internal traffic, unusual IP address
        DNS Logs - compromised EC2 instances sending encoded data within DNS queries
        Kubernetes Audit Logs - suspicious activities and potential EKS cluster compromises
    Can setup EventBridge rules to be notified in case of findings
    EventBridge rules can target AWS Lambda or SNS 
    Can protect against CryptoCurrency attacks (has a dedicated "finding" for it)

Amazon Inspector
    Automated Security Assessments
    For EC2 instances
        Leveraging the AWS System Manager (SSM) agent
        Analyze against unintended network accessibility
        Analyze the running OS against known vulnerabilities
    For Container Images push to Amazon ECR
        Assessment of Container Images as they are pushed
    For Lambda Functions
        Identifies software vulnerabilities in function code and package dependencies
        Assessment of functions as they are deployed
    Reporting & integration with AWS Security Hub
    Send findings to Amazon Event Bridge

    What does Amazon Inspector evaluate?
        Remember: only for EC2 instances, Container Images & Lambda Functions
        Continuous scanning of the infrastructure, only when needed
        Package vulnerabilities (EC2, ECR & Lambda) - database of CVE
        Network reachability (EC2)
        A risk score is associated with all vulnerabilities for prioritization

Amazon Macie
    Amazon Macie is a fully managed data security and data privacy service 
    that uses machine learning and pattern matching to discover and protect your sensitive data in AWS
    Macie helps identify and alert you to sensitive data, such as personally identifiable information (PII)






AWS NETWORKING
------------------------------------------------------------------------------------------------------------
Understanding CIDR - IPv4

    Classless Inter-Domain Routing - a method for allocating IP addresses
    Used in Security Group rules and AWS networking in general
    They help to define an IP address range:
        We've seen WW.XX.YY.ZZ/32 => One IP
        We've seen 0.0.0.0/0 => all IPs
        But we can define: 192.168.0.0/26 => 192.168.0.0 - 192.168.0.63 (64 IP addresses)

    A CIDR consists of two components
    Base IP
        Represents an IP contaained in the range (XX.XX.XX.XX)
        Example: 10.0.0.0, 192.168.0.0, ...
    Subnet Mask
        Defines how many bits can change in the IP
        Example: /0, /24, /32
        Can take two forms:
            /8 <=> 255.0.0.0
            /16 <=> 255.255.0.0
            /24 <=> 255.255.255.0
            /32 <=> 255.255.255.255
        
    The Subnet Mask basically allows part of the underlying IP to get additional nect values from the base IP

    192.168.0.0/32 => aalows for 1 IP (2^0) -------------> 192.168.0.0
    192.168.0.0/31 => aalows for 2 IP (2^1) -------------> 192.168.0.0 -> 192.168.0.1
    192.168.0.0/30 => aalows for 4 IP (2^2) -------------> 192.168.0.0 -> 192.168.0.3
    192.168.0.0/29 => aalows for 8 IP (2^3) -------------> 192.168.0.0 -> 192.168.0.7
    192.168.0.0/28 => aalows for 16 IP (2^4) -------------> 192.168.0.0 -> 192.168.0.15
    192.168.0.0/27 => aalows for 32 IP (2^5) -------------> 192.168.0.0 -> 192.168.0.31
    192.168.0.0/26 => aalows for 64 IP (2^6) -------------> 192.168.0.0 -> 192.168.0.63
    192.168.0.0/25 => aalows for 128 IP (2^7) -------------> 192.168.0.0 -> 192.168.0.127
    192.168.0.0/24 => aalows for 256 IP (2^8) -------------> 192.168.0.0 -> 192.168.0.255
    
    192.168.0.0/16 => allows for 65536 IP (2^16) -------------> 192.168.0.0 -> 192.168.255.255
    
    192.168.0.0/0 => allows for ALL IP (2^16) -------------> 0.0.0.0 -> 255.255.255.255


    The Internet Assigned Numbers Authority (IANA) eastablished certain blocks of 
    IPv4 addresses for the use of private (LAN) and public (Internet) addresses

    Private IP can allow certain values:
        10.0.0.0 - 10.255.255.255 (10.0.0.0/8) <= in big networks
        172.16.0.0 - 172.31.255.255 (172.16.0.0/12) <= AWS default VPC in that range
        192.168.0.0 - 192.168.255.255 (192.168.0.0/16) <= home networks
    
    All the rest of the IP addresses on the Internet are Public 


Default VPC Walkthrough
    All new AWS accounts have a default VPC
    New EC2 instances are launched into the default VPC if no subnet is specified
    Default VPC has Internet connectivity and all EC2 instances inside it have public IPv4 addresses
    We also get a public and a private IPv4 DNS names